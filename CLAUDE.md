# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LinkML schema project for representing "Datasheets for Datasets" (D4D) - a standardized way to document datasets inspired by electronic component datasheets. The project creates structured schemas for the 50+ questions and topics outlined in the original Datasheets for Datasets paper by Gebru et al.

**Related work**:
- Original paper: [Datasheets for Datasets](https://m-cacm.acm.org/magazines/2021/12/256932-datasheets-for-datasets/fulltext)
- Example: [Structured dataset documentation: a datasheet for CheXpert](https://arxiv.org/abs/2105.03020)
- Google's alternative: [Data Cards](https://arxiv.org/abs/2204.01075)
- Augmented model: [Augmented Datasheets for Speech Datasets and Ethical Decision-Making](https://dl.acm.org/doi/10.1145/3593013.3594049)

## Development Commands

This project uses Poetry for dependency management and Make for build automation:

### Setup and Installation
```bash
make setup          # Initial setup (run this first)
make install        # Install dependencies only
poetry install      # Alternative direct poetry install
```

### Testing and Validation
```bash
make test           # Run all tests (schema validation, Python tests, examples)
make test-schema    # Test schema validation only (full merged schema)
make test-modules   # Validate all individual D4D module schemas
make test-python    # Run Python unit tests only
make test-examples  # Test example data validation
make lint           # Run LinkML schema linting (main schema only)
make lint-modules   # Lint all individual D4D module schemas
```

### Building and Generation
```bash
make all            # Generate all project artifacts
make gen-project    # Generate Python datamodel, JSON schema, OWL, etc.
make gen-examples   # Copy example data to examples/ directory
make gendoc         # Generate documentation
```

### Documentation and Site
```bash
make site           # Build complete site (gen-project + gendoc)
make serve          # Serve documentation locally
make testdoc        # Build docs and serve locally
make deploy         # Deploy site to GitHub Pages
```

## Architecture and Structure

### Core Schema Files
- `src/data_sheets_schema/schema/data_sheets_schema.yaml` - Main LinkML schema that imports all modules
- `src/data_sheets_schema/schema/D4D_Base_import.yaml` - Base schema with shared classes (NamedThing, Organization, DatasetProperty, Person, Software, Information, FormatDialect), slots, and enums used across all modules
- D4D Module files (directly in schema/ directory, NOT in modules/):
  - `D4D_Motivation.yaml` - Dataset motivation questions
  - `D4D_Composition.yaml` - Dataset composition questions
  - `D4D_Collection.yaml` - Data collection process questions
  - `D4D_Preprocessing.yaml` - Data preprocessing questions
  - `D4D_Uses.yaml` - Recommended uses questions
  - `D4D_Distribution.yaml` - Distribution questions
  - `D4D_Maintenance.yaml` - Maintenance questions
  - `D4D_Human.yaml` - Human subjects questions
  - `D4D_Ethics.yaml` - Ethics and data protection questions
  - `D4D_Data_Governance.yaml` - Data governance and licensing questions
  - `D4D_Metadata.yaml` - Metadata-specific definitions
  - `D4D_Minimal.yaml` - Minimal schema subset

### Generated Artifacts (DO NOT EDIT)
- `src/data_sheets_schema/datamodel/` - Auto-generated Python datamodel classes
- `project/` - Generated schemas in multiple formats:
  - `project/jsonschema/` - JSON Schema representations
  - `project/owl/` - OWL ontology representations
  - `project/shacl/` - SHACL validation shapes
  - `project/jsonld/` - JSON-LD context files
  - `project/graphql/` - GraphQL schema
  - `project/excel/` - Excel templates
  - `project/docs/` - Generated markdown documentation
- `docs/` - Generated documentation site
- `src/data_sheets_schema/schema/data_sheets_schema_all.yaml` - Fully merged schema with all imports resolved (generated by `make full-schema`)

### Key Configuration
- `about.yaml` - Project metadata and configuration (used by Makefile)
- `pyproject.toml` - Poetry dependencies and build configuration
- `Makefile` - Build automation and commands
- `config.env` - Environment variables for the build process

## Schema Development Workflow

1. Edit schema files in `src/data_sheets_schema/schema/`
2. Run `make lint-modules` to lint individual module changes (faster for module-only edits)
3. Run `make test-modules` to validate individual modules
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate Python datamodel and other artifacts
6. Run `make test` to validate everything works
7. Run `make gendoc` to update documentation

**Quick validation during module development:**
- `make lint-modules && make test-modules` - Fast validation of D4D modules only
- `make test-schema` - Full schema validation (includes all modules merged)

## Working with Modules

The schema is modularized by D4D sections. Key architectural patterns:

### Module Structure
- Each module is a standalone LinkML schema file that imports `D4D_Base_import.yaml`
- Modules define classes that inherit from base classes (especially `DatasetProperty`)
- The main `data_sheets_schema.yaml` imports all modules to create the complete schema
- Each module uses its own namespace prefix (e.g., `d4dmotivation:`, `d4dcomposition:`)

### Full Schema Generation
- Run `make full-schema` to generate `data_sheets_schema_all.yaml` - a fully merged version with all imports resolved
- This full schema is used for testing and validation
- The full schema should have NO import statements (it's fully materialized)
- Tests verify the full schema includes all expected prefixes and has no imports

## Testing Strategy

The project has three types of tests (all run with `make test`):

### Schema Validation (`make test-schema`)
- Validates LinkML schema syntax and structure
- Tests the fully merged schema (`data_sheets_schema_all.yaml`)
- Run with: `poetry run gen-project -d tmp <schema_file>`

### Python Unit Tests (`make test-python`)
- Located in `tests/` directory
- Tests generated datamodel classes
- `tests/test_d4d_full_schema.py` - Tests full schema generation and validation
- `tests/test_data.py` - Basic data tests
- `tests/test_renderer.py` - Tests for rendering functionality
- `tests/extract_docx.py` - Document extraction tests
- Run with: `poetry run python -m unittest discover`

### Example Validation (`make test-examples`)
- Validates example data against the schema
- Valid examples in `src/data/examples/valid/`
- Invalid examples (should fail) in `src/data/examples/invalid/`
- Output goes to `examples/output/`

## LinkML-Specific Commands

Beyond the Makefile commands, these LinkML CLI tools are useful:

```bash
# Validate a schema
poetry run linkml-lint src/data_sheets_schema/schema/data_sheets_schema.yaml

# Convert data between formats
poetry run linkml-convert -s <schema.yaml> -C <ClassName> <input.yaml> -o <output.json>

# Run examples and validate
poetry run linkml-run-examples --schema <schema.yaml> --input-directory <dir> --output-directory <out>

# Generate merged schema
poetry run gen-linkml -o <output.yaml> -f yaml <input.yaml>

# Render documentation
poetry run gen-doc -d docs <schema.yaml>
```

## Common Workflows

### Adding a New Module
1. Create new module file in `src/data_sheets_schema/schema/` (e.g., `D4D_NewModule.yaml`)
2. Define imports: `imports: [D4D_Base_import]`
3. Set up prefixes and default_prefix
4. Create classes inheriting from `DatasetProperty` or other base classes
5. Add import to main `data_sheets_schema.yaml`
6. Add attributes to `Dataset` class in main schema to reference new module classes
7. Run `make gen-project` and `make test`

### Modifying Existing Schema
1. Edit the relevant module file or main schema
2. Run `make lint-modules` to validate module syntax (if editing D4D modules)
3. Run `make test-modules` to validate module schemas (if editing D4D modules)
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate artifacts
6. Run `make test` to ensure all tests pass
7. Check generated Python in `src/data_sheets_schema/datamodel/` to verify changes

### Working with Example Data
1. Add valid examples to `src/data/examples/valid/`
2. Add invalid examples (for negative testing) to `src/data/examples/invalid/`
3. Run `make test-examples` to validate
4. Check output in `examples/output/` for validation results

## D4D Agent Scripts

This repository includes AI-powered scripts to extract D4D metadata from dataset documentation.

### Running D4D Extraction

#### 1. Process Individual Downloaded Files

**Validated D4D Wrapper (Recommended)**
```bash
python src/download/validated_d4d_wrapper.py -i downloads_by_column -o data/extracted_by_column
```
Features:
- Validates download success
- Checks content relevance to project categories (AI_READI, CHORUS, CM4AI, VOICE)
- Generates D4D YAML metadata
- Creates detailed validation reports

**Basic D4D Wrapper**
```bash
python src/download/d4d_agent_wrapper.py -i downloads_by_column -o data/extracted_by_column
```
Simpler version without validation steps.

#### 2. Process Concatenated Documents (New!)

Process concatenated documents created by `make concat-extracted`:

```bash
# Process a single concatenated file
make process-concat INPUT_FILE=data/sheets_concatenated/AI_READI_d4d_concatenated.txt

# Process with custom output
make process-concat INPUT_FILE=data/sheets_concatenated/AI_READI_d4d_concatenated.txt OUTPUT_FILE=output.yaml

# Process all concatenated files in data/sheets_concatenated/
make process-all-concat

# Direct script usage with more options
cd aurelian
uv run python ../src/download/process_concatenated_d4d.py -i ../data/sheets_concatenated/AI_READI_d4d_concatenated.txt

# Use a different model
uv run python ../src/download/process_concatenated_d4d.py -i ../data/sheets_concatenated/AI_READI_d4d_concatenated.txt \
  -m "anthropic:claude-3-opus-20240229"

# Process all files in a directory
uv run python ../src/download/process_concatenated_d4d.py -d ../data/sheets_concatenated --output-dir ../data/sheets_concatenated
```

**Features of concatenated processing:**
- Synthesizes multiple D4D YAML files into a single comprehensive document
- Merges complementary information from all concatenated sources
- Prefers more detailed/specific information over generic
- Keeps the most comprehensive descriptions
- Combines all relevant metadata sections
- Uses aurelian's D4D agent for intelligent synthesis

**Typical workflow:**
1. `make concat-extracted` - Concatenate all D4D files per project column
2. `make process-all-concat` - Synthesize concatenated files into comprehensive D4D YAML
3. Output: `data/sheets_concatenated/{PROJECT}_d4d_alldocs.yaml`

#### 3. Test Script (Single URLs)
```bash
cd aurelian
python test_d4d.py
```

### D4D Agent Requirements
- Set `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variable
- Validated wrapper uses GPT-5 by default
- Concatenated processor uses GPT-5 by default
- Run from `aurelian/` directory using `uv run` for dependency management
- Expects column-organized input directories (by project category)
- Outputs YAML files conforming to the D4D schema

### D4D Agent Architecture
The D4D agents use the `aurelian` framework:
- Located in `aurelian/src/aurelian/agents/d4d/`
- Uses pydantic-ai for agent orchestration
- Loads full schema from GitHub or local file
- Processes HTML, PDF, JSON, and text documents
- Can synthesize multiple documents into comprehensive metadata
- Can be run via CLI: `aurelian datasheets <URL>` or `aurelian datasheets --ui`

**Note**: The `aurelian/` directory is a git submodule. Initialize it with:
```bash
git submodule update --init --recursive
```

## Document Concatenation

This project includes tools to concatenate multiple documents from a directory into a single document in reproducible order.

### Concatenation Commands

```bash
# Concatenate documents from a specific directory
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=path/to/output.txt

# Optional parameters:
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=output.txt EXTENSIONS=".txt .md" RECURSIVE=true

# Concatenate extracted D4D documents by column (from data/extracted_by_column)
make concat-extracted

# Concatenate raw downloads by column (from downloads_by_column)
make concat-downloads

# Direct script usage with more options:
python src/download/concatenate_documents.py -i input_dir -o output.txt [OPTIONS]

# Script options:
#   -e, --extensions .txt .md    # Filter by file extensions
#   -r, --recursive             # Search subdirectories
#   --no-headers                # Exclude file headers
#   --no-summary                # Exclude table of contents
#   -s "separator"              # Custom separator between files
```

### Features

- **Reproducible ordering**: Files are sorted alphabetically for consistent results
- **Multiple formats**: Handles text, HTML, YAML, and other text-based formats
- **File metadata**: Includes headers with filename, path, and size
- **Table of contents**: Summary section lists all concatenated files
- **Error handling**: Gracefully handles encoding issues and read errors

### Use Cases

- Combine all downloaded dataset documentation for a project
- Create single input documents for LLM processing
- Merge documentation fragments into complete documents
- Aggregate logs or reports from multiple files

## Custom Makefile Targets

Beyond standard LinkML targets, this project adds:

```bash
make gen-minimal-examples  # Generate minimal example files for all classes
make gen-html             # Generate HTML from D4D YAML files using human_readable_renderer.py
make full-schema          # Generate data_sheets_schema_all.yaml (merged schema)
make test-modules         # Validate all individual D4D module schemas
make lint-modules         # Lint all individual D4D module schemas
make concat-docs          # Concatenate documents from a directory
make concat-extracted     # Concatenate extracted D4D documents by column
make concat-downloads     # Concatenate raw downloads by column
make process-concat       # Process concatenated doc with D4D agent
make process-all-concat   # Process all concatenated docs with D4D agent
```

## Null/Empty Value Handling

The codebase follows a consistent pattern for handling empty/missing values:

### Schema and Python Code
- **Default for empty values**: `null`/`None`
- Python datamodel uses `Optional[type] = None` for all optional fields
- Schema uses `default_range: string` but does NOT use `ifabsent` rules that would force empty strings
- YAML data files should use `null` or omit fields entirely for missing values

### HTML Rendering
- **HTML output**: All `None`/`null` values are converted to empty strings `""`
- This applies to:
  - `src/html/human_readable_renderer.py` - Human-readable HTML output
  - `src/renderer/yaml_renderer.py` - YAML to HTML/PDF rendering
- Empty strings in HTML provide cleaner display without "Not specified" or placeholder text
- Tables display empty cells rather than "-" or "N/A" for null values

**Example:**
```yaml
# In YAML data file
field1: "value"     # Has value
field2: null        # No value (or omit entirely)
```

```html
<!-- In HTML rendering -->
<td>value</td>      <!-- field1 displays value -->
<td></td>           <!-- field2 displays as empty -->
```

## Running Single Tests

To run a specific test file:
```bash
poetry run python -m unittest tests.test_d4d_full_schema
poetry run python -m unittest tests.test_data
poetry run python -m unittest tests.test_renderer
```

To run a specific test class or method:
```bash
poetry run python -m unittest tests.test_d4d_full_schema.TestD4DFullSchema
poetry run python -m unittest tests.test_d4d_full_schema.TestD4DFullSchema.test_full_schema_generation
```

## Important Notes

- **DO NOT EDIT** files in `project/` or `src/data_sheets_schema/datamodel/` - these are auto-generated
- **DO NOT EDIT** `data_sheets_schema_all.yaml` - it's generated by `make full-schema`
- The main schema imports all module schemas for reusable components
- Poetry manages all dependencies - use `poetry add` rather than pip
- LinkML generates multiple output formats (JSON Schema, OWL, SHACL, etc.) from single YAML schema source
- Always run `make gen-project` after schema changes to regenerate artifacts
- Module files are in `src/data_sheets_schema/schema/` (NOT in a `modules/` subdirectory)
- When adding new classes, prefer inheriting from existing base classes in `D4D_Base_import.yaml`
- The `aurelian/` directory is a git submodule - initialize with `git submodule update --init --recursive`