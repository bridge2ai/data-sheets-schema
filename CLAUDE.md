# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LinkML schema project for representing "Datasheets for Datasets" - a standardized way to document datasets inspired by electronic component datasheets. The project creates structured schemas for the 50+ questions and topics outlined in the original Datasheets for Datasets paper by Gebru et al.

## Development Commands

This project uses Poetry for dependency management and Make for build automation:

### Setup and Installation
```bash
make setup          # Initial setup (run this first)
make install        # Install dependencies only
poetry install      # Alternative direct poetry install
```

### Testing and Validation
```bash
make test           # Run all tests (schema validation, Python tests, examples)
make test-schema    # Test schema validation only (full merged schema)
make test-modules   # Validate all individual D4D module schemas
make test-python    # Run Python unit tests only
make test-examples  # Test example data validation
make lint           # Run LinkML schema linting (main schema only)
make lint-modules   # Lint all individual D4D module schemas
```

### Building and Generation
```bash
make all            # Generate all project artifacts
make gen-project    # Generate Python datamodel, JSON schema, OWL, etc.
make gen-examples   # Copy example data to examples/ directory
make gendoc         # Generate documentation
```

### Documentation and Site
```bash
make site           # Build complete site (gen-project + gendoc)
make serve          # Serve documentation locally
make testdoc        # Build docs and serve locally
make deploy         # Deploy site to GitHub Pages
```

## Architecture and Structure

### Core Schema Files
- `src/data_sheets_schema/schema/data_sheets_schema.yaml` - Main LinkML schema that imports all modules
- `src/data_sheets_schema/schema/D4D_Base_import.yaml` - Base schema with shared classes (NamedThing, Organization, DatasetProperty, Person, Software, Information, FormatDialect), slots, and enums used across all modules
- D4D Module files (directly in schema/ directory, NOT in modules/):
  - `D4D_Motivation.yaml` - Dataset motivation questions
  - `D4D_Composition.yaml` - Dataset composition questions
  - `D4D_Collection.yaml` - Data collection process questions
  - `D4D_Preprocessing.yaml` - Data preprocessing questions
  - `D4D_Uses.yaml` - Recommended uses questions
  - `D4D_Distribution.yaml` - Distribution questions
  - `D4D_Maintenance.yaml` - Maintenance questions
  - `D4D_Human.yaml` - Human subjects questions
  - `D4D_Ethics.yaml` - Ethics and data protection questions
  - `D4D_Data_Governance.yaml` - Data governance and licensing questions
  - `D4D_Metadata.yaml` - Metadata-specific definitions
  - `D4D_Minimal.yaml` - Minimal schema subset

### Generated Artifacts (DO NOT EDIT)
- `src/data_sheets_schema/datamodel/` - Auto-generated Python datamodel classes
- `project/` - Generated schemas in multiple formats:
  - `project/jsonschema/` - JSON Schema representations
  - `project/owl/` - OWL ontology representations
  - `project/shacl/` - SHACL validation shapes
  - `project/jsonld/` - JSON-LD context files
  - `project/graphql/` - GraphQL schema
  - `project/excel/` - Excel templates
  - `project/docs/` - Generated markdown documentation
- `docs/` - Generated documentation site
- `src/data_sheets_schema/schema/data_sheets_schema_all.yaml` - Fully merged schema with all imports resolved (generated by `make full-schema`)

### Key Configuration
- `about.yaml` - Project metadata and configuration (used by Makefile)
- `pyproject.toml` - Poetry dependencies and build configuration
- `Makefile` - Build automation and commands
- `config.env` - Environment variables for the build process

## Schema Development Workflow

1. Edit schema files in `src/data_sheets_schema/schema/`
2. Run `make lint-modules` to lint individual module changes (faster for module-only edits)
3. Run `make test-modules` to validate individual modules
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate Python datamodel and other artifacts
6. Run `make test` to validate everything works
7. Run `make gendoc` to update documentation

**Quick validation during module development:**
- `make lint-modules && make test-modules` - Fast validation of D4D modules only
- `make test-schema` - Full schema validation (includes all modules merged)

## Working with Modules

The schema is modularized by D4D sections. Key architectural patterns:

### Module Structure
- Each module is a standalone LinkML schema file that imports `D4D_Base_import.yaml`
- Modules define classes that inherit from base classes (especially `DatasetProperty`)
- The main `data_sheets_schema.yaml` imports all modules to create the complete schema
- Each module uses its own namespace prefix (e.g., `d4dmotivation:`, `d4dcomposition:`)

### Full Schema Generation
- Run `make full-schema` to generate `data_sheets_schema_all.yaml` - a fully merged version with all imports resolved
- This full schema is used for testing and validation
- The full schema should have NO import statements (it's fully materialized)
- Tests verify the full schema includes all expected prefixes and has no imports

## Testing Strategy

The project has three types of tests (all run with `make test`):

### Schema Validation (`make test-schema`)
- Validates LinkML schema syntax and structure
- Tests the fully merged schema (`data_sheets_schema_all.yaml`)
- Run with: `poetry run gen-project -d tmp <schema_file>`

### Python Unit Tests (`make test-python`)
- Located in `tests/` directory
- Tests generated datamodel classes
- `tests/test_d4d_full_schema.py` - Tests full schema generation and validation
- `tests/test_data.py` - Basic data tests
- `tests/test_renderer.py` - Tests for rendering functionality
- `tests/extract_docx.py` - Document extraction tests
- Run with: `poetry run python -m unittest discover`

### Example Validation (`make test-examples`)
- Validates example data against the schema
- Valid examples in `src/data/examples/valid/`
- Invalid examples (should fail) in `src/data/examples/invalid/`
- Output goes to `examples/output/`

## LinkML-Specific Commands

Beyond the Makefile commands, these LinkML CLI tools are useful:

```bash
# Validate a schema
poetry run linkml-lint src/data_sheets_schema/schema/data_sheets_schema.yaml

# Convert data between formats
poetry run linkml-convert -s <schema.yaml> -C <ClassName> <input.yaml> -o <output.json>

# Run examples and validate
poetry run linkml-run-examples --schema <schema.yaml> --input-directory <dir> --output-directory <out>

# Generate merged schema
poetry run gen-linkml -o <output.yaml> -f yaml <input.yaml>

# Render documentation
poetry run gen-doc -d docs <schema.yaml>
```

## Common Workflows

### Adding a New Module
1. Create new module file in `src/data_sheets_schema/schema/` (e.g., `D4D_NewModule.yaml`)
2. Define imports: `imports: [D4D_Base_import]`
3. Set up prefixes and default_prefix
4. Create classes inheriting from `DatasetProperty` or other base classes
5. Add import to main `data_sheets_schema.yaml`
6. Add attributes to `Dataset` class in main schema to reference new module classes
7. Run `make gen-project` and `make test`

### Modifying Existing Schema
1. Edit the relevant module file or main schema
2. Run `make lint-modules` to validate module syntax (if editing D4D modules)
3. Run `make test-modules` to validate module schemas (if editing D4D modules)
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate artifacts
6. Run `make test` to ensure all tests pass
7. Check generated Python in `src/data_sheets_schema/datamodel/` to verify changes

### Working with Example Data
1. Add valid examples to `src/data/examples/valid/`
2. Add invalid examples (for negative testing) to `src/data/examples/invalid/`
3. Run `make test-examples` to validate
4. Check output in `examples/output/` for validation results

## D4D Pipeline and Data Organization

This repository includes AI-powered scripts and make targets to extract D4D metadata from dataset documentation.

### Data Organization Structure

All data is organized under the `data/` directory:

```
data/
  raw/                            # Raw downloads (moved from downloads_by_column/)
    AI_READI/, CHORUS/, CM4AI/, VOICE/

  preprocessed/
    individual/                   # Transformed source files (PDF→text, HTML→text)
      AI_READI/, CHORUS/, CM4AI/, VOICE/
    concatenated/                 # All docs per project concatenated
      {PROJECT}_concatenated.txt

  d4d_individual/                 # D4D YAMLs from individual docs
    gpt5/AI_READI/, CHORUS/, CM4AI/, VOICE/
    claudecode/AI_READI/, CHORUS/, CM4AI/, VOICE/

  d4d_concatenated/               # D4D YAMLs from concatenated docs
    gpt5/{PROJECT}_d4d.yaml
    claudecode/{PROJECT}_d4d.yaml
    curated/{PROJECT}_curated.yaml

  d4d_html/                       # HTML renderings
    individual/gpt5/, individual/claudecode/
    concatenated/gpt5/, concatenated/claudecode/, concatenated/curated/

  ATTIC/                          # Archived old data
```

### Complete D4D Pipeline (Recommended)

Run the complete end-to-end pipeline for all projects using GPT-5:

```bash
# Full pipeline: extract → validate → concatenate → synthesize → HTML
make d4d-pipeline-full-gpt5

# Or run individual pipeline stages:
make d4d-pipeline-individual-gpt5     # Extract + validate individual files
make d4d-pipeline-concatenated-gpt5   # Concatenate + synthesize + HTML
```

### Step-by-Step Pipeline Targets

#### Step 1: Extract D4D from Individual Files

```bash
# Extract D4D metadata for all projects using GPT-5 (with validation)
make extract-d4d-individual-all-gpt5

# Or extract for a single project
make extract-d4d-individual-gpt5 PROJECT=AI_READI

# Direct script usage (advanced)
cd aurelian
uv run python ../src/download/validated_d4d_wrapper.py -i ../data/raw/AI_READI -o ../data/d4d_individual/gpt5/AI_READI
```

**Features:**
- Validates download success
- Checks content relevance to project categories
- Generates D4D YAML metadata
- Creates detailed validation reports
- Output: `data/d4d_individual/gpt5/{PROJECT}/`

#### Step 2: Validate D4D YAMLs

```bash
# Validate all D4D YAMLs across all projects
make validate-d4d-all GENERATOR=gpt5

# Validate all YAMLs for a specific project
make validate-d4d-project PROJECT=AI_READI GENERATOR=gpt5

# Validate a single D4D YAML file
make validate-d4d FILE=data/d4d_individual/gpt5/AI_READI/file_d4d.yaml
```

**Features:**
- Validates against LinkML D4D schema
- Reports validation errors with details
- Provides summary statistics

#### Step 3: Concatenate Individual D4D YAMLs

```bash
# Concatenate individual D4D YAMLs by project (for synthesis)
make concat-extracted

# Concatenate preprocessed individual files
make concat-preprocessed

# Concatenate raw downloads
make concat-raw

# Custom concatenation from any directory
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=output.txt
```

**Features:**
- Reproducible alphabetical ordering
- Includes file metadata headers
- Generates table of contents
- Output: `data/preprocessed/concatenated/{PROJECT}_concatenated.txt`

#### Step 4: Extract D4D from Concatenated Files

```bash
# Extract D4D from all concatenated files using GPT-5
make extract-d4d-concat-all-gpt5

# Or extract from a single project's concatenated file
make extract-d4d-concat-gpt5 PROJECT=AI_READI

# Process with custom parameters
make process-concat INPUT_FILE=data/preprocessed/concatenated/AI_READI_concatenated.txt

# Direct script usage (advanced)
cd aurelian
uv run python ../src/download/process_concatenated_d4d.py \
  -i ../data/preprocessed/concatenated/AI_READI_concatenated.txt \
  -o ../data/d4d_concatenated/gpt5/AI_READI_d4d.yaml
```

**Features:**
- Synthesizes multiple D4D YAML files into comprehensive document
- Merges complementary information from all sources
- Prefers more detailed/specific information over generic
- Uses aurelian's D4D agent for intelligent synthesis
- Output: `data/d4d_concatenated/gpt5/{PROJECT}_d4d.yaml`

#### Step 5: Generate HTML from D4D YAMLs

```bash
# Generate human-readable HTML from all D4D YAMLs
make gen-d4d-html
```

**Features:**
- Generates HTML for individual and concatenated D4D YAMLs
- Creates both GPT-5 and Claude Code versions
- Preserves curated HTML files
- Output: `data/d4d_html/`

### Data Status and Monitoring

Check the current state of your data pipeline:

```bash
# Full detailed status report
make data-status

# Quick compact overview
make data-status-quick

# Detailed D4D YAML size report
make data-d4d-sizes
```

**Features:**
- Shows file counts for all directories in the pipeline
- Flags empty directories with ⚠️ warnings
- Identifies missing directories with ❌ markers
- Displays file sizes and line counts for key files
- Provides summary statistics across all projects
- Reports D4D YAML sizes with individual and concatenated breakdowns

### Quick Reference: Common Workflows

```bash
# Check current pipeline status
make data-status-quick

# Extract D4D from new raw downloads
make extract-d4d-individual-all-gpt5

# Validate all extracted YAMLs
make validate-d4d-all

# Create comprehensive D4D from all individual YAMLs
make concat-extracted
make extract-d4d-concat-all-gpt5

# Generate HTML renderings
make gen-d4d-html

# Run complete pipeline
make d4d-pipeline-full-gpt5
```

### D4D Agent Requirements
- Set `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variable
- Validated wrapper uses GPT-5 by default
- Concatenated processor uses GPT-5 by default
- Run from `aurelian/` directory using `uv run` for dependency management
- Expects column-organized input directories (by project category)
- Outputs YAML files conforming to the D4D schema

### D4D Agent Architecture
The D4D agents use the `aurelian` framework:
- Located in `aurelian/src/aurelian/agents/d4d/`
- Uses pydantic-ai for agent orchestration
- Loads full schema from GitHub or local file
- Processes HTML, PDF, JSON, and text documents
- Can synthesize multiple documents into comprehensive metadata
- Can be run via CLI: `aurelian datasheets <URL>` or `aurelian datasheets --ui`

## D4D Assistant Instructions (GitHub Actions)

**IMPORTANT**: This section is for the D4D Assistant running in GitHub Actions.

### When You Are the D4D Assistant

If you are invoked as the D4D Assistant (via GitHub Actions, issue mentions, or labeled requests), you MUST:

1. **Read the appropriate instruction file FIRST** before doing anything:
   - **For creating new datasheets**: Read `.github/workflows/d4d_assistant_create.md`
   - **For editing existing datasheets**: Read `.github/workflows/d4d_assistant_edit.md`
   - **For modifying an existing PR**: Both files contain "Modifying an Existing PR" sections

2. **Follow the instructions exactly** as specified in the file

3. **Do not proceed** without reading the instruction file - it contains critical information about:
   - Scope limitations (D4D tasks only - redirect non-D4D questions)
   - Available MCP tools (GitHub, ARTL, WebSearch, WebFetch) and how to use them
   - Step-by-step workflows for metadata extraction and datasheet generation
   - Pull request creation and modification procedures
   - Validation requirements (MUST validate before creating/updating PRs)
   - User communication templates
   - Error handling guidance

### Instruction File Locations

- **`.github/workflows/d4d_assistant_create.md`** - Creating new D4D datasheets from URLs
  - Includes: How to modify existing PRs with updates
- **`.github/workflows/d4d_assistant_edit.md`** - Editing existing D4D YAML files
  - Includes: How to apply additional edits to existing PRs
- **`.github/workflows/README.md`** - MCP server setup and troubleshooting guide

### Quick Reference: D4D Assistant Workflow

When a user requests D4D assistance:

```
1. Identify task type: create new, edit existing, or modify PR
2. READ appropriate instruction file from .github/workflows/
3. Follow step-by-step instructions exactly
4. Use MCP tools (GitHub, ARTL, WebSearch, WebFetch)
5. Validate YAML against schema (REQUIRED before PR creation)
6. Create PR with changes OR update existing PR
7. Comment on PR with what changed
8. Notify user in GitHub issue with PR link
```

**Critical Notes:**
- Always validate YAML before creating or updating PRs
- Use `gh pr checkout <number>` to modify existing PRs
- Comment on both the PR and the issue to keep users informed
- Only handle D4D-related tasks; politely redirect others

**Note**: These workflows are specific to the GitHub Actions environment and differ from interactive Claude Code usage.

## Document Concatenation

This project includes tools to concatenate multiple documents from a directory into a single document in reproducible order.

### Concatenation Commands

```bash
# Concatenate documents from a specific directory
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=path/to/output.txt

# Optional parameters:
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=output.txt EXTENSIONS=".txt .md" RECURSIVE=true

# Concatenate individual D4D YAMLs by project (from data/d4d_individual/gpt5/)
make concat-extracted

# Concatenate preprocessed files by project (from data/preprocessed/individual/)
make concat-preprocessed

# Concatenate raw downloads by project (from data/raw/)
make concat-raw

# Direct script usage with more options:
poetry run python src/download/concatenate_documents.py -i input_dir -o output.txt [OPTIONS]

# Script options:
#   -e, --extensions .txt .md    # Filter by file extensions
#   -r, --recursive             # Search subdirectories
#   --no-headers                # Exclude file headers
#   --no-summary                # Exclude table of contents
#   -s "separator"              # Custom separator between files
```

**Output location:** All concatenated files are saved to `data/preprocessed/concatenated/`

### Features

- **Reproducible ordering**: Files are sorted alphabetically for consistent results
- **Multiple formats**: Handles text, HTML, YAML, JSON, and other text-based formats
- **File metadata**: Includes headers with filename, path, and size
- **Table of contents**: Summary section lists all concatenated files
- **Error handling**: Gracefully handles encoding issues and read errors
- **Project organization**: Automatically processes all projects (AI_READI, CHORUS, CM4AI, VOICE)

### Use Cases

- Combine all D4D YAMLs for synthesis into comprehensive metadata
- Concatenate preprocessed files for batch D4D extraction
- Combine raw downloads for comprehensive processing
- Create single input documents for LLM processing
- Merge documentation fragments into complete documents

## Custom Makefile Targets

Beyond standard LinkML targets, this project adds comprehensive D4D pipeline targets:

### Status and Monitoring Targets
```bash
make data-status                         # Full data status report with counts
make data-status-quick                   # Compact status overview
make data-d4d-sizes                      # Detailed D4D YAML size report
```

### Concatenation Targets
```bash
make concat-extracted      # Concatenate individual D4D YAMLs by project
make concat-preprocessed   # Concatenate preprocessed files by project
make concat-raw            # Concatenate raw downloads by project
make concat-docs           # Concatenate documents from directory (INPUT_DIR=, OUTPUT_FILE=)
```

### D4D Extraction Targets
```bash
# Individual file extraction
make extract-d4d-individual-gpt5         # Extract for one project (PROJECT=AI_READI)
make extract-d4d-individual-all-gpt5     # Extract for all projects

# Concatenated file extraction
make extract-d4d-concat-gpt5             # Extract from one concatenated (PROJECT=AI_READI)
make extract-d4d-concat-all-gpt5         # Extract from all concatenated files
make process-concat                      # Process single file (INPUT_FILE=)
make process-all-concat                  # Process all files in directory
```

### Validation Targets
```bash
make validate-d4d                        # Validate single file (FILE=path/to/file.yaml)
make validate-d4d-project                # Validate project (PROJECT=, GENERATOR=gpt5)
make validate-d4d-all                    # Validate all D4D YAMLs (GENERATOR=gpt5)
```

### HTML Generation Targets
```bash
make gen-d4d-html                        # Generate HTML from D4D YAMLs
make gen-html                            # Alias for gen-d4d-html
```

### Complete Pipeline Workflows
```bash
make d4d-pipeline-individual-gpt5        # Extract + validate individual files
make d4d-pipeline-concatenated-gpt5      # Concatenate + synthesize + HTML
make d4d-pipeline-full-gpt5              # Complete end-to-end pipeline
```

### Schema and Example Targets
```bash
make gen-minimal-examples  # Generate minimal example files for all classes
make full-schema          # Generate data_sheets_schema_all.yaml (merged schema)
make test-modules         # Validate all individual D4D module schemas
make lint-modules         # Lint all individual D4D module schemas
```

## Null/Empty Value Handling

The codebase follows a consistent pattern for handling empty/missing values:

### Schema and Python Code
- **Default for empty values**: `null`/`None`
- Python datamodel uses `Optional[type] = None` for all optional fields
- Schema uses `default_range: string` but does NOT use `ifabsent` rules that would force empty strings
- YAML data files should use `null` or omit fields entirely for missing values

### HTML Rendering
- **HTML output**: All `None`/`null` values are converted to empty strings `""`
- This applies to:
  - `src/html/human_readable_renderer.py` - Human-readable HTML output
  - `src/renderer/yaml_renderer.py` - YAML to HTML/PDF rendering
- Empty strings in HTML provide cleaner display without "Not specified" or placeholder text
- Tables display empty cells rather than "-" or "N/A" for null values

**Example:**
```yaml
# In YAML data file
field1: "value"     # Has value
field2: null        # No value (or omit entirely)
```

```html
<!-- In HTML rendering -->
<td>value</td>      <!-- field1 displays value -->
<td></td>           <!-- field2 displays as empty -->
```

## Important Notes

- **DO NOT EDIT** files in `project/` or `src/data_sheets_schema/datamodel/` - these are auto-generated
- **DO NOT EDIT** `data_sheets_schema_all.yaml` - it's generated by `make full-schema`
- The main schema imports all module schemas for reusable components
- Poetry manages all dependencies - use `poetry add` rather than pip
- LinkML generates multiple output formats (JSON Schema, OWL, SHACL, etc.) from single YAML schema source
- Always run `make gen-project` after schema changes to regenerate artifacts
- Module files are in `src/data_sheets_schema/schema/` (NOT in a `modules/` subdirectory)
- When adding new classes, prefer inheriting from existing base classes in `D4D_Base_import.yaml`