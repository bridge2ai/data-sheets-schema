# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LinkML schema project for representing "Datasheets for Datasets" - a standardized way to document datasets inspired by electronic component datasheets. The project creates structured schemas for the 50+ questions and topics outlined in the original Datasheets for Datasets paper by Gebru et al.

## Development Commands

This project uses Poetry for dependency management and Make for build automation:

### Setup and Installation
```bash
make setup          # Initial setup (run this first)
make install        # Install dependencies only
poetry install      # Alternative direct poetry install
```

### Testing and Validation
```bash
make test           # Run all tests (schema validation, Python tests, examples)
make test-schema    # Test schema validation only
make test-python    # Run Python unit tests only
make test-examples  # Test example data validation
make lint           # Run LinkML schema linting
```

### Building and Generation
```bash
make all            # Generate all project artifacts
make gen-project    # Generate Python datamodel, JSON schema, OWL, etc.
make gen-examples   # Copy example data to examples/ directory
make gendoc         # Generate documentation
```

### Documentation and Site
```bash
make site           # Build complete site (gen-project + gendoc)
make serve          # Serve documentation locally
make testdoc        # Build docs and serve locally
make deploy         # Deploy site to GitHub Pages
```

## Architecture and Structure

### Core Schema Files
- `src/data_sheets_schema/schema/data_sheets_schema.yaml` - Main LinkML schema that imports all modules
- `src/data_sheets_schema/schema/D4D_Base_import.yaml` - Base schema with shared classes (NamedThing, Organization, DatasetProperty, Person, Software, Information, FormatDialect), slots, and enums used across all modules
- D4D Module files (directly in schema/ directory, NOT in modules/):
  - `D4D_Motivation.yaml` - Dataset motivation questions
  - `D4D_Composition.yaml` - Dataset composition questions
  - `D4D_Collection.yaml` - Data collection process questions
  - `D4D_Preprocessing.yaml` - Data preprocessing questions
  - `D4D_Uses.yaml` - Recommended uses questions
  - `D4D_Distribution.yaml` - Distribution questions
  - `D4D_Maintenance.yaml` - Maintenance questions
  - `D4D_Human.yaml` - Human subjects questions
  - `D4D_Ethics.yaml` - Ethics and data protection questions
  - `D4D_Data_Governance.yaml` - Data governance and licensing questions
  - `D4D_Metadata.yaml` - Metadata-specific definitions
  - `D4D_Minimal.yaml` - Minimal schema subset

### Generated Artifacts (DO NOT EDIT)
- `src/data_sheets_schema/datamodel/` - Auto-generated Python datamodel classes
- `project/` - Generated schemas in multiple formats:
  - `project/jsonschema/` - JSON Schema representations
  - `project/owl/` - OWL ontology representations
  - `project/shacl/` - SHACL validation shapes
  - `project/jsonld/` - JSON-LD context files
  - `project/graphql/` - GraphQL schema
  - `project/excel/` - Excel templates
  - `project/docs/` - Generated markdown documentation
- `docs/` - Generated documentation site
- `src/data_sheets_schema/schema/data_sheets_schema_all.yaml` - Fully merged schema with all imports resolved (generated by `make full-schema`)

### Key Configuration
- `about.yaml` - Project metadata and configuration (used by Makefile)
- `pyproject.toml` - Poetry dependencies and build configuration
- `Makefile` - Build automation and commands
- `config.env` - Environment variables for the build process

## Schema Development Workflow

1. Edit schema files in `src/data_sheets_schema/schema/`
2. Run `make test-schema` to validate changes
3. Run `make gen-project` to regenerate Python datamodel and other artifacts
4. Run `make test` to validate everything works
5. Run `make gendoc` to update documentation

## Working with Modules

The schema is modularized by D4D sections. Key architectural patterns:

### Module Structure
- Each module is a standalone LinkML schema file that imports `D4D_Base_import.yaml`
- Modules define classes that inherit from base classes (especially `DatasetProperty`)
- The main `data_sheets_schema.yaml` imports all modules to create the complete schema
- Each module uses its own namespace prefix (e.g., `d4dmotivation:`, `d4dcomposition:`)

### Full Schema Generation
- Run `make full-schema` to generate `data_sheets_schema_all.yaml` - a fully merged version with all imports resolved
- This full schema is used for testing and validation
- The full schema should have NO import statements (it's fully materialized)
- Tests verify the full schema includes all expected prefixes and has no imports

## Testing Strategy

The project has three types of tests (all run with `make test`):

### Schema Validation (`make test-schema`)
- Validates LinkML schema syntax and structure
- Tests the fully merged schema (`data_sheets_schema_all.yaml`)
- Run with: `poetry run gen-project -d tmp <schema_file>`

### Python Unit Tests (`make test-python`)
- Located in `tests/` directory
- Tests generated datamodel classes
- `tests/test_d4d_full_schema.py` - Tests full schema generation and validation
- `tests/test_data.py` - Basic data tests
- `tests/test_renderer.py` - Tests for rendering functionality
- `tests/extract_docx.py` - Document extraction tests
- Run with: `poetry run python -m unittest discover`

### Example Validation (`make test-examples`)
- Validates example data against the schema
- Valid examples in `src/data/examples/valid/`
- Invalid examples (should fail) in `src/data/examples/invalid/`
- Output goes to `examples/output/`

## LinkML-Specific Commands

Beyond the Makefile commands, these LinkML CLI tools are useful:

```bash
# Validate a schema
poetry run linkml-lint src/data_sheets_schema/schema/data_sheets_schema.yaml

# Convert data between formats
poetry run linkml-convert -s <schema.yaml> -C <ClassName> <input.yaml> -o <output.json>

# Run examples and validate
poetry run linkml-run-examples --schema <schema.yaml> --input-directory <dir> --output-directory <out>

# Generate merged schema
poetry run gen-linkml -o <output.yaml> -f yaml <input.yaml>

# Render documentation
poetry run gen-doc -d docs <schema.yaml>
```

## Common Workflows

### Adding a New Module
1. Create new module file in `src/data_sheets_schema/schema/` (e.g., `D4D_NewModule.yaml`)
2. Define imports: `imports: [D4D_Base_import]`
3. Set up prefixes and default_prefix
4. Create classes inheriting from `DatasetProperty` or other base classes
5. Add import to main `data_sheets_schema.yaml`
6. Add attributes to `Dataset` class in main schema to reference new module classes
7. Run `make gen-project` and `make test`

### Modifying Existing Schema
1. Edit the relevant module file or main schema
2. Run `make test-schema` to validate syntax
3. Run `make gen-project` to regenerate artifacts
4. Run `make test` to ensure all tests pass
5. Check generated Python in `src/data_sheets_schema/datamodel/` to verify changes

### Working with Example Data
1. Add valid examples to `src/data/examples/valid/`
2. Add invalid examples (for negative testing) to `src/data/examples/invalid/`
3. Run `make test-examples` to validate
4. Check output in `examples/output/` for validation results

## Important Notes

- **DO NOT EDIT** files in `project/` or `src/data_sheets_schema/datamodel/` - these are auto-generated
- **DO NOT EDIT** `data_sheets_schema_all.yaml` - it's generated by `make full-schema`
- The main schema imports all module schemas for reusable components
- Poetry manages all dependencies - use `poetry add` rather than pip
- LinkML generates multiple output formats (JSON Schema, OWL, SHACL, etc.) from single YAML schema source
- Always run `make gen-project` after schema changes to regenerate artifacts
- Module files are in `src/data_sheets_schema/schema/` (NOT in a `modules/` subdirectory)
- When adding new classes, prefer inheriting from existing base classes in `D4D_Base_import.yaml`