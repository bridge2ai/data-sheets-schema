# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LinkML schema project for representing "Datasheets for Datasets" (D4D) - a standardized way to document datasets inspired by electronic component datasheets. The project creates structured schemas for the 50+ questions and topics outlined in the original Datasheets for Datasets paper by Gebru et al.

**Related work**:
- Original paper: [Datasheets for Datasets](https://m-cacm.acm.org/magazines/2021/12/256932-datasheets-for-datasets/fulltext)
- Example: [Structured dataset documentation: a datasheet for CheXpert](https://arxiv.org/abs/2105.03020)
- Google's alternative: [Data Cards](https://arxiv.org/abs/2204.01075)
- Augmented model: [Augmented Datasheets for Speech Datasets and Ethical Decision-Making](https://dl.acm.org/doi/10.1145/3593013.3594049)

## Development Commands

This project uses Poetry for dependency management and Make for build automation:

### Setup and Installation
```bash
make setup          # Initial setup (run this first)
make install        # Install dependencies only
poetry install      # Alternative direct poetry install
```

### Testing and Validation
```bash
make test           # Run all tests (schema validation, Python tests, examples)
make test-schema    # Test schema validation only (full merged schema)
make test-modules   # Validate all individual D4D module schemas
make test-python    # Run Python unit tests only
make test-examples  # Test example data validation
make lint           # Run LinkML schema linting (main schema only)
make lint-modules   # Lint all individual D4D module schemas
```

### Building and Generation
```bash
make all            # Generate all project artifacts
make gen-project    # Generate Python datamodel, JSON schema, OWL, etc.
make gen-examples   # Copy example data to examples/ directory
make gendoc         # Generate documentation
```

### Documentation and Site
```bash
make site           # Build complete site (gen-project + gendoc)
make serve          # Serve documentation locally
make testdoc        # Build docs and serve locally
make deploy         # Deploy site to GitHub Pages
```

## Architecture and Structure

### Core Schema Files
- `src/data_sheets_schema/schema/data_sheets_schema.yaml` - Main LinkML schema that imports all modules
- `src/data_sheets_schema/schema/D4D_Base_import.yaml` - Base schema with shared classes (NamedThing, Organization, DatasetProperty, Person, Software, Information, FormatDialect), slots, and enums used across all modules
- D4D Module files (directly in schema/ directory, NOT in modules/):
  - `D4D_Motivation.yaml` - Dataset motivation questions
  - `D4D_Composition.yaml` - Dataset composition questions
  - `D4D_Collection.yaml` - Data collection process questions
  - `D4D_Preprocessing.yaml` - Data preprocessing questions
  - `D4D_Uses.yaml` - Recommended uses questions
  - `D4D_Distribution.yaml` - Distribution questions
  - `D4D_Maintenance.yaml` - Maintenance questions
  - `D4D_Human.yaml` - Human subjects questions
  - `D4D_Ethics.yaml` - Ethics and data protection questions
  - `D4D_Data_Governance.yaml` - Data governance and licensing questions
  - `D4D_Metadata.yaml` - Metadata-specific definitions
  - `D4D_Minimal.yaml` - Minimal schema subset

### Generated Artifacts (DO NOT EDIT)
- `src/data_sheets_schema/datamodel/` - Auto-generated Python datamodel classes
- `project/` - Generated schemas in multiple formats:
  - `project/jsonschema/` - JSON Schema representations
  - `project/owl/` - OWL ontology representations
  - `project/shacl/` - SHACL validation shapes
  - `project/jsonld/` - JSON-LD context files
  - `project/graphql/` - GraphQL schema
  - `project/excel/` - Excel templates
  - `project/docs/` - Generated markdown documentation
- `docs/` - Generated documentation site
- `src/data_sheets_schema/schema/data_sheets_schema_all.yaml` - Fully merged schema with all imports resolved (generated by `make full-schema`)

### Key Configuration
- `about.yaml` - Project metadata and configuration (used by Makefile)
- `pyproject.toml` - Poetry dependencies and build configuration
- `Makefile` - Build automation and commands
- `config.env` - Environment variables for the build process

## Schema Development Workflow

1. Edit schema files in `src/data_sheets_schema/schema/`
2. Run `make lint-modules` to lint individual module changes (faster for module-only edits)
3. Run `make test-modules` to validate individual modules
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate Python datamodel and other artifacts
6. Run `make test` to validate everything works
7. Run `make gendoc` to update documentation

**Quick validation during module development:**
- `make lint-modules && make test-modules` - Fast validation of D4D modules only
- `make test-schema` - Full schema validation (includes all modules merged)

## Keeping Schema Files in Sync

The project maintains **three representations** of the schema that must stay synchronized:

1. **`data_sheets_schema.yaml`** (12KB) - Source schema with module imports
2. **`data_sheets_schema_all.yaml`** (671KB) - Fully merged schema (all imports resolved)
3. **`data_sheets_schema.py`** (224KB) - Auto-generated Python datamodel

### Automated Sync Checking

```bash
# Check if all three files are in sync
make check-sync

# Force regenerate everything if out of sync
make regen-all
```

The `check-sync` target:
- Compares modification times of source schema, module files, merged schema, and Python model
- Reports which files are out of date
- Provides commands to fix sync issues
- Exit code 0 = in sync, 1 = out of sync

### When Files Get Out of Sync

Files become out of sync when:
- You edit source schema or module files but don't regenerate artifacts
- You pull changes that only update source files
- Make's timestamp-based dependencies fail to trigger regeneration

**Best practice after editing schemas:**
```bash
# 1. Edit source schema or modules
vim src/data_sheets_schema/schema/D4D_*.yaml

# 2. Regenerate everything
make regen-all

# 3. Verify sync
make check-sync

# 4. Test
make test
```

### What Each Command Does

| Command | What It Regenerates |
|---------|---------------------|
| `make full-schema` | Only `data_sheets_schema_all.yaml` (merged schema) |
| `make gen-project` | Python model + JSON Schema + OWL + JSON-LD + etc. |
| `make regen-all` | **Everything** (merged schema + all generated artifacts) |
| `make check-sync` | Nothing - just checks if files are in sync |

## Working with Modules

The schema is modularized by D4D sections. Key architectural patterns:

### Module Structure
- Each module is a standalone LinkML schema file that imports `D4D_Base_import.yaml`
- Modules define classes that inherit from base classes (especially `DatasetProperty`)
- The main `data_sheets_schema.yaml` imports all modules to create the complete schema
- Each module uses its own namespace prefix (e.g., `d4dmotivation:`, `d4dcomposition:`)

### Full Schema Generation
- Run `make full-schema` to generate `data_sheets_schema_all.yaml` - a fully merged version with all imports resolved
- This full schema is used for testing and validation
- The full schema should have NO import statements (it's fully materialized)
- Tests verify the full schema includes all expected prefixes and has no imports

## Testing Strategy

The project has three types of tests (all run with `make test`):

### Schema Validation (`make test-schema`)
- Validates LinkML schema syntax and structure
- Tests the fully merged schema (`data_sheets_schema_all.yaml`)
- Run with: `poetry run gen-project -d tmp <schema_file>`

### Python Unit Tests (`make test-python`)
- Located in `tests/` directory
- Tests generated datamodel classes
- `tests/test_d4d_full_schema.py` - Tests full schema generation and validation
- `tests/test_data.py` - Basic data tests
- `tests/test_renderer.py` - Tests for rendering functionality
- `tests/extract_docx.py` - Document extraction tests
- Run with: `poetry run python -m unittest discover`

### Example Validation (`make test-examples`)
- Validates example data against the schema
- Valid examples in `src/data/examples/valid/`
- Invalid examples (should fail) in `src/data/examples/invalid/`
- Output goes to `examples/output/`

## LinkML-Specific Commands

Beyond the Makefile commands, these LinkML CLI tools are useful:

```bash
# Validate a schema
poetry run linkml-lint src/data_sheets_schema/schema/data_sheets_schema.yaml

# Convert data between formats
poetry run linkml-convert -s <schema.yaml> -C <ClassName> <input.yaml> -o <output.json>

# Run examples and validate
poetry run linkml-run-examples --schema <schema.yaml> --input-directory <dir> --output-directory <out>

# Generate merged schema
poetry run gen-linkml -o <output.yaml> -f yaml <input.yaml>

# Render documentation
poetry run gen-doc -d docs <schema.yaml>
```

## Common Workflows

### Adding a New Module
1. Create new module file in `src/data_sheets_schema/schema/` (e.g., `D4D_NewModule.yaml`)
2. Define imports: `imports: [D4D_Base_import]`
3. Set up prefixes and default_prefix
4. Create classes inheriting from `DatasetProperty` or other base classes
5. Add import to main `data_sheets_schema.yaml`
6. Add attributes to `Dataset` class in main schema to reference new module classes
7. Run `make gen-project` and `make test`

### Modifying Existing Schema
1. Edit the relevant module file or main schema
2. Run `make lint-modules` to validate module syntax (if editing D4D modules)
3. Run `make test-modules` to validate module schemas (if editing D4D modules)
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate artifacts
6. Run `make test` to ensure all tests pass
7. Check generated Python in `src/data_sheets_schema/datamodel/` to verify changes

### Working with Example Data
1. Add valid examples to `src/data/examples/valid/`
2. Add invalid examples (for negative testing) to `src/data/examples/invalid/`
3. Run `make test-examples` to validate
4. Check output in `examples/output/` for validation results

## D4D Pipeline and Data Organization

This repository includes AI-powered scripts and make targets to extract D4D metadata from dataset documentation.

### Data Organization Structure

All data is organized under the `data/` directory:

```
data/
  raw/                            # Raw downloads (moved from downloads_by_column/)
    AI_READI/, CHORUS/, CM4AI/, VOICE/

  preprocessed/
    individual/                   # Transformed source files (PDF→text, HTML→text)
      AI_READI/, CHORUS/, CM4AI/, VOICE/
    concatenated/                 # All docs per project concatenated
      {PROJECT}_concatenated.txt

  d4d_individual/                 # D4D YAMLs from individual docs
    gpt5/AI_READI/, CHORUS/, CM4AI/, VOICE/
    claudecode/AI_READI/, CHORUS/, CM4AI/, VOICE/

  d4d_concatenated/               # D4D YAMLs from concatenated docs
    gpt5/{PROJECT}_d4d.yaml
    claudecode/{PROJECT}_d4d.yaml
    curated/{PROJECT}_curated.yaml

  d4d_html/                       # HTML renderings
    individual/gpt5/, individual/claudecode/
    concatenated/gpt5/, concatenated/claudecode/, concatenated/curated/

  ATTIC/                          # Archived old data
```

### Complete D4D Pipeline (Recommended)

Run the complete end-to-end pipeline for all projects using GPT-5:

```bash
# Full pipeline: extract → validate → concatenate → synthesize → HTML
make d4d-pipeline-full-gpt5

# Or run individual pipeline stages:
make d4d-pipeline-individual-gpt5     # Extract + validate individual files
make d4d-pipeline-concatenated-gpt5   # Concatenate + synthesize + HTML
```

### Step-by-Step Pipeline Targets

#### Step 1: Extract D4D from Individual Files

```bash
# Extract D4D metadata for all projects using GPT-5 (with validation)
make extract-d4d-individual-all-gpt5

# Or extract for a single project
make extract-d4d-individual-gpt5 PROJECT=AI_READI

# Direct script usage (advanced)
cd aurelian
uv run python ../src/download/validated_d4d_wrapper.py -i ../data/raw/AI_READI -o ../data/d4d_individual/gpt5/AI_READI
```

**Features:**
- Validates download success
- Checks content relevance to project categories
- Generates D4D YAML metadata
- Creates detailed validation reports
- Output: `data/d4d_individual/gpt5/{PROJECT}/`

#### Step 2: Validate D4D YAMLs

```bash
# Validate all D4D YAMLs across all projects
make validate-d4d-all GENERATOR=gpt5

# Validate all YAMLs for a specific project
make validate-d4d-project PROJECT=AI_READI GENERATOR=gpt5

# Validate a single D4D YAML file
make validate-d4d FILE=data/d4d_individual/gpt5/AI_READI/file_d4d.yaml
```

**Features:**
- Validates against LinkML D4D schema
- Reports validation errors with details
- Provides summary statistics

#### Step 3: Concatenate Individual D4D YAMLs

```bash
# Concatenate individual D4D YAMLs by project (for synthesis)
make concat-extracted

# Concatenate preprocessed individual files
make concat-preprocessed

# Concatenate raw downloads
make concat-raw

# Custom concatenation from any directory
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=output.txt
```

**Features:**
- Reproducible alphabetical ordering
- Includes file metadata headers
- Generates table of contents
- Output: `data/preprocessed/concatenated/{PROJECT}_concatenated.txt`

#### Step 4: Extract D4D from Concatenated Files

```bash
# Extract D4D from all concatenated files using GPT-5
make extract-d4d-concat-all-gpt5

# Or extract from a single project's concatenated file
make extract-d4d-concat-gpt5 PROJECT=AI_READI

# Process with custom parameters
make process-concat INPUT_FILE=data/preprocessed/concatenated/AI_READI_concatenated.txt

# Direct script usage (advanced)
cd aurelian
uv run python ../src/download/process_concatenated_d4d.py \
  -i ../data/preprocessed/concatenated/AI_READI_concatenated.txt \
  -o ../data/d4d_concatenated/gpt5/AI_READI_d4d.yaml
```

**Features:**
- Synthesizes multiple D4D YAML files into comprehensive document
- Merges complementary information from all sources
- Prefers more detailed/specific information over generic
- Uses aurelian's D4D agent for intelligent synthesis
- Output: `data/d4d_concatenated/gpt5/{PROJECT}_d4d.yaml`

#### Step 5: Generate HTML from D4D YAMLs

```bash
# Generate human-readable HTML from all D4D YAMLs
make gen-d4d-html
```

**Features:**
- Generates HTML for individual and concatenated D4D YAMLs
- Creates both GPT-5 and Claude Code versions
- Preserves curated HTML files
- Output: `data/d4d_html/`

### Data Status and Monitoring

Check the current state of your data pipeline:

```bash
# Full detailed status report
make data-status

# Quick compact overview
make data-status-quick

# Detailed D4D YAML size report
make data-d4d-sizes
```

**Features:**
- Shows file counts for all directories in the pipeline
- Flags empty directories with ⚠️ warnings
- Identifies missing directories with ❌ markers
- Displays file sizes and line counts for key files
- Provides summary statistics across all projects
- Reports D4D YAML sizes with individual and concatenated breakdowns

### Quick Reference: Common Workflows

```bash
# Check current pipeline status
make data-status-quick

# Extract D4D from new raw downloads
make extract-d4d-individual-all-gpt5

# Validate all extracted YAMLs
make validate-d4d-all

# Create comprehensive D4D from all individual YAMLs
make concat-extracted
make extract-d4d-concat-all-gpt5

# Generate HTML renderings
make gen-d4d-html

# Run complete pipeline
make d4d-pipeline-full-gpt5
```

### D4D Agent Requirements
- Set `OPENAI_API_KEY` or `ANTHROPIC_API_KEY` environment variable
- Validated wrapper uses GPT-5 by default
- Concatenated processor uses GPT-5 by default
- Run from `aurelian/` directory using `uv run` for dependency management
- Expects column-organized input directories (by project category)
- Outputs YAML files conforming to the D4D schema

### D4D Agent Architecture
The D4D agents use the `aurelian` framework:
- Located in `aurelian/src/aurelian/agents/d4d/`
- Uses pydantic-ai for agent orchestration
- Loads full schema from GitHub or local file
- Processes HTML, PDF, JSON, and text documents
- Can synthesize multiple documents into comprehensive metadata
- Can be run via CLI: `aurelian datasheets <URL>` or `aurelian datasheets --ui`

**Note**: The `aurelian/` directory is a git submodule. Initialize it with:
```bash
git submodule update --init --recursive
```

## D4D Assistant Instructions (GitHub Actions)

**IMPORTANT**: This section is for the D4D Assistant running in GitHub Actions.

### When You Are the D4D Assistant

If you are invoked as the D4D Assistant (via GitHub Actions, issue mentions, or labeled requests), you MUST:

1. **Read the appropriate instruction file FIRST** before doing anything:
   - **For creating new datasheets**: Read `.github/workflows/d4d_assistant_create.md`
   - **For editing existing datasheets**: Read `.github/workflows/d4d_assistant_edit.md`
   - **For modifying an existing PR**: Both files contain "Modifying an Existing PR" sections

2. **Follow the instructions exactly** as specified in the file

3. **Do not proceed** without reading the instruction file - it contains critical information about:
   - Scope limitations (D4D tasks only - redirect non-D4D questions)
   - Available MCP tools (GitHub, ARTL, WebSearch, WebFetch) and how to use them
   - Step-by-step workflows for metadata extraction and datasheet generation
   - Pull request creation and modification procedures
   - Validation requirements (MUST validate before creating/updating PRs)
   - User communication templates
   - Error handling guidance

### Instruction File Locations

- **`.github/workflows/d4d_assistant_create.md`** - Creating new D4D datasheets from URLs
  - Includes: How to modify existing PRs with updates
- **`.github/workflows/d4d_assistant_edit.md`** - Editing existing D4D YAML files
  - Includes: How to apply additional edits to existing PRs
- **`.github/workflows/README.md`** - MCP server setup and troubleshooting guide

### Quick Reference: D4D Assistant Workflow

When a user requests D4D assistance:

```
1. Identify task type: create new, edit existing, or modify PR
2. READ appropriate instruction file from .github/workflows/
3. Follow step-by-step instructions exactly
4. Use MCP tools (GitHub, ARTL, WebSearch, WebFetch)
5. Validate YAML against schema (REQUIRED before PR creation)
6. Create PR with changes OR update existing PR
7. Comment on PR with what changed
8. Notify user in GitHub issue with PR link
```

**Critical Notes:**
- Always validate YAML before creating or updating PRs
- Use `gh pr checkout <number>` to modify existing PRs
- Comment on both the PR and the issue to keep users informed
- Only handle D4D-related tasks; politely redirect others

**Note**: These workflows are specific to the GitHub Actions environment and differ from interactive Claude Code usage.
## Document Concatenation

This project includes tools to concatenate multiple documents from a directory into a single document in reproducible order.

### Concatenation Commands

```bash
# Concatenate documents from a specific directory
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=path/to/output.txt

# Optional parameters:
make concat-docs INPUT_DIR=path/to/dir OUTPUT_FILE=output.txt EXTENSIONS=".txt .md" RECURSIVE=true

# Concatenate individual D4D YAMLs by project (from data/d4d_individual/gpt5/)
make concat-extracted

# Concatenate preprocessed files by project (from data/preprocessed/individual/)
make concat-preprocessed

# Concatenate raw downloads by project (from data/raw/)
make concat-raw

# Direct script usage with more options:
poetry run python src/download/concatenate_documents.py -i input_dir -o output.txt [OPTIONS]

# Script options:
#   -e, --extensions .txt .md    # Filter by file extensions
#   -r, --recursive             # Search subdirectories
#   --no-headers                # Exclude file headers
#   --no-summary                # Exclude table of contents
#   -s "separator"              # Custom separator between files
```

**Output location:** All concatenated files are saved to `data/preprocessed/concatenated/`

### Features

- **Reproducible ordering**: Files are sorted alphabetically for consistent results
- **Multiple formats**: Handles text, HTML, YAML, JSON, and other text-based formats
- **File metadata**: Includes headers with filename, path, and size
- **Table of contents**: Summary section lists all concatenated files
- **Error handling**: Gracefully handles encoding issues and read errors
- **Project organization**: Automatically processes all projects (AI_READI, CHORUS, CM4AI, VOICE)

### Use Cases

- Combine all D4D YAMLs for synthesis into comprehensive metadata
- Concatenate preprocessed files for batch D4D extraction
- Combine raw downloads for comprehensive processing
- Create single input documents for LLM processing
- Merge documentation fragments into complete documents

## Custom Makefile Targets

Beyond standard LinkML targets, this project adds comprehensive D4D pipeline targets:

### Status and Monitoring Targets
```bash
make data-status                         # Full data status report with counts
make data-status-quick                   # Compact status overview
make data-d4d-sizes                      # Detailed D4D YAML size report
```

### Concatenation Targets
```bash
make concat-extracted      # Concatenate individual D4D YAMLs by project
make concat-preprocessed   # Concatenate preprocessed files by project
make concat-raw            # Concatenate raw downloads by project
make concat-docs           # Concatenate documents from directory (INPUT_DIR=, OUTPUT_FILE=)
```

### D4D Extraction Targets

#### GPT-5 Extraction
```bash
# Individual file extraction
make extract-d4d-individual-gpt5         # Extract for one project (PROJECT=AI_READI)
make extract-d4d-individual-all-gpt5     # Extract for all projects

# Concatenated file extraction
make extract-d4d-concat-gpt5             # Extract from one concatenated (PROJECT=AI_READI)
make extract-d4d-concat-all-gpt5         # Extract from all concatenated files
make process-concat                      # Process single file (INPUT_FILE=)
make process-all-concat                  # Process all files in directory
```

#### Claude Code Deterministic Extraction
```bash
# Concatenated file extraction (API-based, deterministic)
make extract-d4d-concat-claude           # Extract for one project (PROJECT=AI_READI)
make extract-d4d-concat-all-claude       # Extract for all projects

# Direct script usage
python3 src/download/process_d4d_deterministic.py --all
python3 src/download/process_d4d_deterministic.py -i INPUT -o OUTPUT -p PROJECT
```

**Requirements:**
- `ANTHROPIC_API_KEY` environment variable must be set
- Python packages: `anthropic`, `pyyaml` (install with `pip install anthropic pyyaml`)

**Limitations:**
- Requires ANTHROPIC_API_KEY and incurs API costs
- Requires network connectivity
- Rate limits may apply for batch processing

**Alternative Approach:**
For scenarios where API access is not available, use Claude Code assistant direct synthesis:
1. Read concatenated input files from `data/preprocessed/concatenated/`
2. Follow prompts from `src/download/prompts/d4d_concatenated_*.txt`
3. Reference schema from `src/data_sheets_schema/schema/data_sheets_schema_all.yaml`
4. Generate D4D YAML following the same deterministic principles
5. See `notes/DETERMINISM.md` for complete details on the direct synthesis approach

**Deterministic Settings:**
- Temperature: 0.0 (maximum determinism)
- Model: claude-sonnet-4-5-20250929 (date-pinned)
- Schema: Local version-controlled file
- Prompts: External version-controlled files
- Metadata: Comprehensive provenance tracking with SHA-256 hashes

### Validation Targets
```bash
make validate-d4d                        # Validate single file (FILE=path/to/file.yaml)
make validate-d4d-project                # Validate project (PROJECT=, GENERATOR=gpt5)
make validate-d4d-all                    # Validate all D4D YAMLs (GENERATOR=gpt5)
```

### HTML Generation Targets
```bash
make gen-d4d-html                        # Generate HTML from D4D YAMLs
make gen-html                            # Alias for gen-d4d-html
```

### Complete Pipeline Workflows
```bash
make d4d-pipeline-individual-gpt5        # Extract + validate individual files
make d4d-pipeline-concatenated-gpt5      # Concatenate + synthesize + HTML
make d4d-pipeline-full-gpt5              # Complete end-to-end pipeline
```

### Schema and Example Targets
```bash
make gen-minimal-examples  # Generate minimal example files for all classes
make full-schema          # Generate data_sheets_schema_all.yaml (merged schema)
make test-modules         # Validate all individual D4D module schemas
make lint-modules         # Lint all individual D4D module schemas
```

## Null/Empty Value Handling

The codebase follows a consistent pattern for handling empty/missing values:

### Schema and Python Code
- **Default for empty values**: `null`/`None`
- Python datamodel uses `Optional[type] = None` for all optional fields
- Schema uses `default_range: string` but does NOT use `ifabsent` rules that would force empty strings
- YAML data files should use `null` or omit fields entirely for missing values

### HTML Rendering
- **HTML output**: All `None`/`null` values are converted to empty strings `""`
- This applies to:
  - `src/html/human_readable_renderer.py` - Human-readable HTML output
  - `src/renderer/yaml_renderer.py` - YAML to HTML/PDF rendering
- Empty strings in HTML provide cleaner display without "Not specified" or placeholder text
- Tables display empty cells rather than "-" or "N/A" for null values

**Example:**
```yaml
# In YAML data file
field1: "value"     # Has value
field2: null        # No value (or omit entirely)
```

```html
<!-- In HTML rendering -->
<td>value</td>      <!-- field1 displays value -->
<td></td>           <!-- field2 displays as empty -->
```

## D4D Evaluation Framework

This repository includes a comprehensive evaluation system to compare D4D generation methods using two rubric systems.

### Evaluation Commands

```bash
# Evaluate concatenated D4D files (GPT-5, Claude Code, Curated)
make evaluate-d4d                            # Evaluate all projects
make evaluate-d4d-project PROJECT=VOICE      # Evaluate single project

# Evaluate individual D4D files (GPT-5, Claude Code)
make evaluate-d4d-individual                 # Evaluate all individual files

# View results
make eval-summary                            # View concatenated summary
make eval-summary-individual                 # View individual summary
make eval-details PROJECT=VOICE METHOD=claudecode  # View detailed report

# Clean results
make clean-eval                              # Remove concatenated results
make clean-eval-individual                   # Remove individual results
```

### Evaluation Architecture

The evaluation framework (`src/evaluation/evaluate_d4d.py`) compares three D4D generation methods:

1. **Curated Comprehensive** - Manually curated datasheets (DatasetCollection format)
2. **GPT-5** - Generated using GPT-5 API (flat D4D schema)
3. **Claude Code Deterministic** - Direct synthesis at temperature=0.0 (flat D4D schema)

### Two Rubric Systems

**Rubric10** (`data/rubric/rubric10.txt`):
- 10 hierarchical elements with 5 sub-elements each
- Binary scoring (0/1) per sub-element
- Maximum: 50 points total
- Elements: Discovery, Access, Reuse, Ethics, Composition, Provenance, Motivation, Technical Transparency, Limitations, Integration

**Rubric20** (`data/rubric/rubric20.txt`):
- 20 questions in 4 categories
- Quality-based scoring (0-5 scale) or pass/fail
- Maximum: 84 points total
- Categories: Structural Completeness, Metadata Quality, Technical Documentation, FAIRness & Accessibility

### Evaluation Modes

**Concatenated Mode** (default):
- Evaluates comprehensive D4D files synthesized from multiple sources
- Located in `data/d4d_concatenated/{curated,gpt5,claudecode}/`
- Results in `data/evaluation/`

**Individual Mode** (`--individual` flag):
- Evaluates D4D files extracted from single source documents
- Located in `data/d4d_individual/{gpt5,claudecode}/`
- Results in `data/evaluation_individual/`

### Key Evaluation Findings

**Concatenated files** (synthesis required):
- Claude Code: 37.5% (Rubric10), 52.4% (Rubric20) - **Best**
- Curated: 21.3% (Rubric10), 41.7% (Rubric20)
- GPT-5: 11.5% (Rubric10), 17.3% (Rubric20)
- Claude Code outperforms GPT-5 by **3.26×**

**Individual files** (single-source extraction):
- Claude Code: 18.8% (Rubric10), 26.3% (Rubric20)
- GPT-5: 18.8% (Rubric10), 26.3% (Rubric20)
- **Identical performance** - synthesis is Claude Code's advantage

### Evaluation Output

All evaluation runs generate:
- `summary_report.md` - Executive summary with comparison tables
- `detailed_analysis/{PROJECT}_{METHOD}_evaluation.md` - Per-file breakdowns
- `scores.csv` - Raw scoring data in CSV format
- `scores.json` - Detailed scores with full metadata in JSON

### Direct Script Usage

```bash
# Evaluate concatenated files
poetry run python src/evaluation/evaluate_d4d.py \
  --base-dir data \
  --projects AI_READI CM4AI VOICE CHORUS \
  --methods curated gpt5 claudecode \
  --output-dir data/evaluation

# Evaluate individual files
poetry run python src/evaluation/evaluate_d4d.py \
  --base-dir data \
  --methods gpt5 claudecode \
  --output-dir data/evaluation_individual \
  --individual

# Evaluate single project
poetry run python src/evaluation/evaluate_d4d.py \
  --project VOICE \
  --output-dir data/evaluation
```

### Evaluation Documentation

Complete methodology, rubric details, and findings documented in:
- `notes/D4D_EVALUATION.md` - Full evaluation methodology and results
- `data/rubric/rubric10.txt` - 10-element hierarchical rubric specification
- `data/rubric/rubric20.txt` - 20-question detailed rubric specification

## D4D LLM-based Evaluation (Quality Assessment)

The repository includes LLM-as-judge evaluation agents that provide quality-based assessment of D4D datasheets, complementing the existing field-presence detection.

### LLM Evaluation Agents

Two specialized agents for quality assessment:

**d4d-rubric10** (`.claude/agents/d4d-rubric10.md`):
- Evaluates using the 10-element hierarchical rubric
- Binary scoring (0/1) with quality notes and evidence
- Maximum: 50 points (10 elements × 5 sub-elements)
- Focus: Discovery, access, reuse, ethics, composition, provenance, motivation, transparency, limitations, integration

**d4d-rubric20** (`.claude/agents/d4d-rubric20.md`):
- Evaluates using the 20-question detailed rubric
- Mixed scoring: 0-5 numeric or pass/fail
- Maximum: 84 points across 4 categories
- Focus: Structural completeness, metadata quality, technical documentation, FAIR compliance

### Usage Examples

**Interactive Evaluation (via agents):**
```
User: Evaluate data/d4d_concatenated/claudecode/VOICE_d4d.yaml with d4d-rubric10

Agent returns:
✅ Rubric10 Evaluation Complete
Overall Score: 38.5/50 (77%)

Strengths:
- Comprehensive ethical documentation with IRB approval
- Clear access mechanisms and licensing
- Detailed preprocessing pipeline

Weaknesses:
- Missing funding agency and award details
- Limited version history documentation

Recommendations:
- Add funding_and_acknowledgements section
- Include GitHub links for preprocessing code
```

**More Usage Examples:** See `notes/RUBRIC_AGENT_USAGE.md` for comprehensive examples including:
- Comparing multiple methods
- Pre-publication quality checks
- Tracking improvements over time
- Understanding quality vs presence gaps
- Actionable improvement recommendations

**Batch Evaluation (via Makefile):**
```bash
# RECOMMENDED: Reproducible batch evaluation of all concatenated files
make evaluate-d4d-llm-batch-concatenated
# Evaluates: AI_READI, CHORUS, CM4AI, VOICE
# Methods: curated, gpt5, claudecode_agent, claudecode_assistant
# Time: ~25 minutes, Cost: ~$6

# Dry run (preview files without evaluating)
make evaluate-d4d-llm-batch-dry-run

# Evaluate all individual D4D files (~85 files)
make evaluate-d4d-llm-batch-individual
# Time: ~2 hours, Cost: ~$34

# Evaluate individual files for specific project/method
make evaluate-d4d-llm-batch-individual-filtered PROJECT=VOICE
make evaluate-d4d-llm-batch-individual-filtered METHOD=claudecode_agent

# Complete evaluation (concatenated + individual)
make evaluate-d4d-llm-batch-all
# Time: ~2.5 hours, Cost: ~$40

# ─────────────────────────────────────────────────
# Legacy single-file evaluation targets:
# ─────────────────────────────────────────────────

# Evaluate with rubric10
make evaluate-d4d-llm-rubric10

# Evaluate with rubric20
make evaluate-d4d-llm-rubric20

# Evaluate with both rubrics
make evaluate-d4d-llm-both

# Evaluate single file
make evaluate-d4d-llm FILE=data/d4d_concatenated/claudecode/VOICE_d4d.yaml \\
  PROJECT=VOICE METHOD=claudecode RUBRIC=both

# Compare LLM vs presence-based evaluation
make compare-evaluations

# View summaries
make eval-llm-summary

# Clean results
make clean-eval-llm
```

**Reproducibility:**
- Temperature: 0.0 (fully deterministic)
- Model: claude-sonnet-4-5-20250929 (date-pinned)
- Same D4D file → Same quality score every time
- Rubrics: Version-controlled in `data/rubric/`
- Prompts: Version-controlled in `src/download/prompts/`
- Scripts: `src/evaluation/batch_evaluate_*.sh`

### LLM Evaluation Features

**Quality over Presence:**
- Assesses content quality, not just field existence
- Evaluates completeness, actionability, and usefulness
- Provides evidence quotes from D4D files
- Identifies strengths, weaknesses, and recommendations

**Scoring Approach:**
- Temperature: 0.0 (fully deterministic evaluation)
- Model: claude-sonnet-4-5-20250929 (date-pinned)
- Returns: JSON with scores, evidence, and assessment
- Exports: Compatible CSV, JSON, and Markdown reports

**Comparison with Presence Detection:**

| Metric | Presence Detection | LLM Quality Assessment |
|--------|-------------------|----------------------|
| Speed | ~1 second | ~30-60 seconds |
| Cost | Free | ~$0.10-0.30 per file |
| Insight | "Field missing" | "Field present but generic/incomplete/excellent" |
| Evidence | None | Quotes, reasoning, context |
| Use Case | CI/CD, quick checks | Deep analysis, comparison |

### LLM Evaluation Commands

```bash
# Direct script usage
poetry run python src/evaluation/evaluate_d4d_llm.py \\
  --file data/d4d_concatenated/claudecode/VOICE_d4d.yaml \\
  --project VOICE --method claudecode --rubric both

# Batch evaluation
poetry run python src/evaluation/evaluate_d4d_llm.py \\
  --all --rubric both --output-dir data/evaluation_llm

# Compare methods
poetry run python src/evaluation/compare_evaluation_methods.py \\
  --llm-dir data/evaluation_llm \\
  --presence-dir data/evaluation
```

### LLM Evaluation Output

All LLM evaluation runs generate:
- `data/evaluation_llm/rubric10/summary_report.md` - Rubric10 summary
- `data/evaluation_llm/rubric20/summary_report.md` - Rubric20 summary
- `data/evaluation_llm/scores.csv` - Compatible scoring data
- `data/evaluation_llm/scores.json` - Full results with metadata
- `data/evaluation_comparison/comparison_report.md` - LLM vs presence comparison

### Dependencies

**Environment variables:**
```bash
export ANTHROPIC_API_KEY=sk-ant-...  # Required for LLM evaluation
```

**Python packages:**
```bash
poetry add anthropic  # Claude API client
```

### LLM Evaluation Documentation

Complete LLM evaluation methodology documented in:
- `notes/LLM_EVALUATION.md` - Full LLM evaluation methodology
- `src/download/prompts/rubric10_system_prompt.md` - Rubric10 evaluation prompt
- `src/download/prompts/rubric20_system_prompt.md` - Rubric20 evaluation prompt

## Running Single Tests

To run a specific test file:
```bash
poetry run python -m unittest tests.test_d4d_full_schema
poetry run python -m unittest tests.test_data
poetry run python -m unittest tests.test_renderer
```

To run a specific test class or method:
```bash
poetry run python -m unittest tests.test_d4d_full_schema.TestD4DFullSchema
poetry run python -m unittest tests.test_d4d_full_schema.TestD4DFullSchema.test_full_schema_generation
```

## Important Notes

- **DO NOT EDIT** files in `project/` or `src/data_sheets_schema/datamodel/` - these are auto-generated
- **DO NOT EDIT** `data_sheets_schema_all.yaml` - it's generated by `make full-schema`
- The main schema imports all module schemas for reusable components
- Poetry manages all dependencies - use `poetry add` rather than pip
- LinkML generates multiple output formats (JSON Schema, OWL, SHACL, etc.) from single YAML schema source
- Always run `make gen-project` after schema changes to regenerate artifacts
- Module files are in `src/data_sheets_schema/schema/` (NOT in a `modules/` subdirectory)
- When adding new classes, prefer inheriting from existing base classes in `D4D_Base_import.yaml`
- The `aurelian/` directory is a git submodule - initialize with `git submodule update --init --recursive`