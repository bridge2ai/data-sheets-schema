# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LinkML schema project for representing "Datasheets for Datasets" - a standardized way to document datasets inspired by electronic component datasheets. The project creates structured schemas for the 50+ questions and topics outlined in the original Datasheets for Datasets paper by Gebru et al.

## Development Commands

This project uses Poetry for dependency management and Make for build automation:

### Setup and Installation
```bash
make setup          # Initial setup (run this first)
make install        # Install dependencies only
poetry install      # Alternative direct poetry install
```

### Testing and Validation
```bash
make test           # Run all tests (schema validation, Python tests, examples)
make test-schema    # Test schema validation only (full merged schema)
make test-modules   # Validate all individual D4D module schemas
make test-python    # Run Python unit tests only
make test-examples  # Test example data validation
make lint           # Run LinkML schema linting (main schema only)
make lint-modules   # Lint all individual D4D module schemas
```

### Building and Generation
```bash
make all            # Generate all project artifacts
make gen-project    # Generate Python datamodel, JSON schema, OWL, etc.
make gen-examples   # Copy example data to examples/ directory
make gendoc         # Generate documentation
```

### Documentation and Site
```bash
make site           # Build complete site (gen-project + gendoc)
make serve          # Serve documentation locally
make testdoc        # Build docs and serve locally
make deploy         # Deploy site to GitHub Pages
```

## Architecture and Structure

### Core Schema Files
- `src/data_sheets_schema/schema/data_sheets_schema.yaml` - Main LinkML schema that imports all modules
- `src/data_sheets_schema/schema/D4D_Base_import.yaml` - Base schema with shared classes (NamedThing, Organization, DatasetProperty, Person, Software, Information, FormatDialect), slots, and enums used across all modules
- D4D Module files (directly in schema/ directory, NOT in modules/):
  - `D4D_Motivation.yaml` - Dataset motivation questions
  - `D4D_Composition.yaml` - Dataset composition questions
  - `D4D_Collection.yaml` - Data collection process questions
  - `D4D_Preprocessing.yaml` - Data preprocessing questions
  - `D4D_Uses.yaml` - Recommended uses questions
  - `D4D_Distribution.yaml` - Distribution questions
  - `D4D_Maintenance.yaml` - Maintenance questions
  - `D4D_Human.yaml` - Human subjects questions
  - `D4D_Ethics.yaml` - Ethics and data protection questions
  - `D4D_Data_Governance.yaml` - Data governance and licensing questions
  - `D4D_Metadata.yaml` - Metadata-specific definitions
  - `D4D_Minimal.yaml` - Minimal schema subset

### Generated Artifacts (DO NOT EDIT)
- `src/data_sheets_schema/datamodel/` - Auto-generated Python datamodel classes
- `project/` - Generated schemas in multiple formats:
  - `project/jsonschema/` - JSON Schema representations
  - `project/owl/` - OWL ontology representations
  - `project/shacl/` - SHACL validation shapes
  - `project/jsonld/` - JSON-LD context files
  - `project/graphql/` - GraphQL schema
  - `project/excel/` - Excel templates
  - `project/docs/` - Generated markdown documentation
- `docs/` - Generated documentation site
- `src/data_sheets_schema/schema/data_sheets_schema_all.yaml` - Fully merged schema with all imports resolved (generated by `make full-schema`)

### Key Configuration
- `about.yaml` - Project metadata and configuration (used by Makefile)
- `pyproject.toml` - Poetry dependencies and build configuration
- `Makefile` - Build automation and commands
- `config.env` - Environment variables for the build process

## Schema Development Workflow

1. Edit schema files in `src/data_sheets_schema/schema/`
2. Run `make lint-modules` to lint individual module changes (faster for module-only edits)
3. Run `make test-modules` to validate individual modules
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate Python datamodel and other artifacts
6. Run `make test` to validate everything works
7. Run `make gendoc` to update documentation

**Quick validation during module development:**
- `make lint-modules && make test-modules` - Fast validation of D4D modules only
- `make test-schema` - Full schema validation (includes all modules merged)

## Working with Modules

The schema is modularized by D4D sections. Key architectural patterns:

### Module Structure
- Each module is a standalone LinkML schema file that imports `D4D_Base_import.yaml`
- Modules define classes that inherit from base classes (especially `DatasetProperty`)
- The main `data_sheets_schema.yaml` imports all modules to create the complete schema
- Each module uses its own namespace prefix (e.g., `d4dmotivation:`, `d4dcomposition:`)

### Full Schema Generation
- Run `make full-schema` to generate `data_sheets_schema_all.yaml` - a fully merged version with all imports resolved
- This full schema is used for testing and validation
- The full schema should have NO import statements (it's fully materialized)
- Tests verify the full schema includes all expected prefixes and has no imports

## Testing Strategy

The project has three types of tests (all run with `make test`):

### Schema Validation (`make test-schema`)
- Validates LinkML schema syntax and structure
- Tests the fully merged schema (`data_sheets_schema_all.yaml`)
- Run with: `poetry run gen-project -d tmp <schema_file>`

### Python Unit Tests (`make test-python`)
- Located in `tests/` directory
- Tests generated datamodel classes
- `tests/test_d4d_full_schema.py` - Tests full schema generation and validation
- `tests/test_data.py` - Basic data tests
- `tests/test_renderer.py` - Tests for rendering functionality
- `tests/extract_docx.py` - Document extraction tests
- Run with: `poetry run python -m unittest discover`

### Example Validation (`make test-examples`)
- Validates example data against the schema
- Valid examples in `src/data/examples/valid/`
- Invalid examples (should fail) in `src/data/examples/invalid/`
- Output goes to `examples/output/`

## LinkML-Specific Commands

Beyond the Makefile commands, these LinkML CLI tools are useful:

```bash
# Validate a schema
poetry run linkml-lint src/data_sheets_schema/schema/data_sheets_schema.yaml

# Convert data between formats
poetry run linkml-convert -s <schema.yaml> -C <ClassName> <input.yaml> -o <output.json>

# Run examples and validate
poetry run linkml-run-examples --schema <schema.yaml> --input-directory <dir> --output-directory <out>

# Generate merged schema
poetry run gen-linkml -o <output.yaml> -f yaml <input.yaml>

# Render documentation
poetry run gen-doc -d docs <schema.yaml>
```

## Common Workflows

### Adding a New Module
1. Create new module file in `src/data_sheets_schema/schema/` (e.g., `D4D_NewModule.yaml`)
2. Define imports: `imports: [D4D_Base_import]`
3. Set up prefixes and default_prefix
4. Create classes inheriting from `DatasetProperty` or other base classes
5. Add import to main `data_sheets_schema.yaml`
6. Add attributes to `Dataset` class in main schema to reference new module classes
7. Run `make gen-project` and `make test`

### Modifying Existing Schema
1. Edit the relevant module file or main schema
2. Run `make lint-modules` to validate module syntax (if editing D4D modules)
3. Run `make test-modules` to validate module schemas (if editing D4D modules)
4. Run `make test-schema` to validate the full merged schema
5. Run `make gen-project` to regenerate artifacts
6. Run `make test` to ensure all tests pass
7. Check generated Python in `src/data_sheets_schema/datamodel/` to verify changes

### Working with Example Data
1. Add valid examples to `src/data/examples/valid/`
2. Add invalid examples (for negative testing) to `src/data/examples/invalid/`
3. Run `make test-examples` to validate
4. Check output in `examples/output/` for validation results

## D4D Agent Scripts

This repository includes AI-powered scripts to extract D4D metadata from dataset documentation.

### Running D4D Extraction

#### Validated D4D Wrapper (Recommended)
```bash
python src/download/validated_d4d_wrapper.py -i downloads_by_column -o data/extracted_by_column
```
Features:
- Validates download success
- Checks content relevance to project categories (AI_READI, CHORUS, CM4AI, VOICE)
- Generates D4D YAML metadata
- Creates detailed validation reports

#### Basic D4D Wrapper
```bash
python src/download/d4d_agent_wrapper.py -i downloads_by_column -o data/extracted_by_column
```
Simpler version without validation steps.

#### Test Script (Single URLs)
```bash
cd aurelian
python test_d4d.py
```

### D4D Agent Requirements
- Set `ANTHROPIC_API_KEY` or `OPENAI_API_KEY` environment variable
- Validated wrapper uses GPT-5 by default
- Expects column-organized input directories (by project category)
- Outputs YAML files conforming to the D4D schema

### D4D Agent Architecture
The D4D agents use the `aurelian` framework:
- Located in `aurelian/src/aurelian/agents/d4d/`
- Uses pydantic-ai for agent orchestration
- Loads full schema from GitHub or local file
- Processes HTML, PDF, JSON, and text documents
- Can be run via CLI: `aurelian datasheets <URL>` or `aurelian datasheets --ui`

## Custom Makefile Targets

Beyond standard LinkML targets, this project adds:

```bash
make gen-minimal-examples  # Generate minimal example files for all classes
make gen-html             # Generate HTML from D4D YAML files using human_readable_renderer.py
make full-schema          # Generate data_sheets_schema_all.yaml (merged schema)
make test-modules         # Validate all individual D4D module schemas
make lint-modules         # Lint all individual D4D module schemas
```

## Important Notes

- **DO NOT EDIT** files in `project/` or `src/data_sheets_schema/datamodel/` - these are auto-generated
- **DO NOT EDIT** `data_sheets_schema_all.yaml` - it's generated by `make full-schema`
- The main schema imports all module schemas for reusable components
- Poetry manages all dependencies - use `poetry add` rather than pip
- LinkML generates multiple output formats (JSON Schema, OWL, SHACL, etc.) from single YAML schema source
- Always run `make gen-project` after schema changes to regenerate artifacts
- Module files are in `src/data_sheets_schema/schema/` (NOT in a `modules/` subdirectory)
- When adding new classes, prefer inheriting from existing base classes in `D4D_Base_import.yaml`