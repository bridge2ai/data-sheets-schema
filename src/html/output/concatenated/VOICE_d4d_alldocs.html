
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>VOICE d4d (Synthesized) - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>VOICE d4d (Synthesized)</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. Recent advances in artificial intelligence have provided techniques to extract previously unknown prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. Bridge2AI-Voice provides a comprehensive collection of derived data from voice recordings with corresponding clinical information, demographic data, and validated questionnaires collected across multiple North American sites. Initial releases focus on low-risk, de-identified derived data (e.g., spectrograms, acoustic features) and phenotype data to reduce re-identification risk while enabling broad research utility.</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice</li><li>bridge2ai</li><li>PhysioNet</li><li>Health Data Nexus</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Created By
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>Alistair Johnson</li><li>Jean-Christophe B√©lisle-Pipon</li><li>David Dorr</li><li>Satrajit Ghosh</li><li>Philip Payne</li><li>Maria Powell</li><li>Anais Rameau</li><li>Vardit Ravitsky</li><li>Alexandros Sigaras</li><li>Olivier Elemento</li><li>Yael Bensoussan</li><li>Bridge2AI-Voice Consortium</li><li>PhysioNet</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Resources
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>physionet-b2ai-voice-1.1</dd><dt>Name</dt><dd>Bridge2AI-Voice v1.1 (PhysioNet)</dd><dt>Title</dt><dd>Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1)</dd><dt>Description</dt><dd><div class="long-description">Derived audio representations (e.g., spectrograms, MFCCs, acoustic and phonetic/prosodic features) and associated phenotype and questionnaire data from adult participants recruited at specialty clinics across five North American sites. Raw audio is not included in this release to reduce re-identification risk. Common identifiers include participant_id, session_id, and task_name.</div></dd><dt>DOI</dt><dd><a href="https://doi.org/10.13026/249v-w155" target="_blank">doi:10.13026/249v-w155</a></dd><dt>Issued</dt><dd>2025-01-17</dd><dt>Version</dt><dd>1.1</dd><dt>Keywords</dt><dd><ul class='formatted-list'><li>voice</li><li>bridge2ai</li><li>PhysioNet</li><li>RRID:SCR_007345</li></ul></dd><dt>License</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Created By</dt><dd><ul class='formatted-list'><li>PhysioNet</li><li>Bridge2AI-Voice Consortium</li></ul></dd><dt>Purposes</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>primary-purpose</dd><dt>Response</dt><dd>Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice representations to demographic, clinical, and questionnaire data.</dd></dl></li></ul></dd><dt>Tasks</dt><dd><table class="data-table"><thead><tr><th>Name</th><th>Response</th></tr></thead><tbody><tr><td>example-task-1</td><td>Development and benchmarking of models associating voice-derived features with health conditions.</td></tr><tr><td>example-task-2</td><td>Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived da...</td></tr></tbody></table></dd><dt>Addressing Gaps</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>gap-addressed</dd><dt>Response</dt><dd>Lack of an ethically sourced, clinically linked, multi-site voice dataset with robust de-identification suitable for AI research.</dd></dl></li></ul></dd><dt>Funders</dt><dd><table class="data-table"><thead><tr><th>Grantor</th><th>Grant Name</th><th>Grant Number</th></tr></thead><tbody><tr><td>National Institutes of Health</td><td>Bridge2AI: Voice as a Biomarker of Health</td><td>3OT2OD032720-01S1</td></tr><tr><td>National Institute of Biomedical Imaging and Bioengineering (NIBIB), NIH</td><td>PhysioNet infrastructure support</td><td>R01EB030362</td></tr></tbody></table></dd><dt>Instances</dt><dd><ol class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>dataset-instances</dd><dt>Representation</dt><dd>Derived representations of voice recordings linked to participant-level phenotype and questionnaire data.</dd><dt>Instance Type</dt><dd>Participants and their recordings; per-recording static feature rows and per-participant phenotype rows.</dd><dt>Data Type</dt><dd>De-identified derived features (spectrograms, MFCCs, acoustic and phonetic/prosodic features) and structured phenotype/questionnaire data.</dd><dt>Counts</dt><dd>12,523</dd><dt>Label</dt><dd>Clinical and demographic attributes (e.g., condition groups, validated questionnaires) are available as metadata; no explicit machine-learning labels are defined in this release.</dd><dt>Sampling Strategies</dt><dd><ol class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>purposive-sampling</dd><dt>Is Sample</dt><dd><ul class='formatted-list'><li>True</li></ul></dd><dt>Is Random</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Source Data</dt><dd><ul class='formatted-list'><li>Adult patients recruited at specialty clinics across five North American sites</li></ul></dd><dt>Is Representative</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Why Not Representative</dt><dd><ul class='formatted-list'><li>Participants were selected based on conditions known to manifest in voice, which may affect generalizability.</li></ul></dd><dt>Strategies</dt><dd><ul class='formatted-list'><li>Purposive sampling by predefined condition groups</li></ul></dd></dl></li></ol></dd><dt>Missing Information</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>privacy-removals</dd><dt>Missing</dt><dd><ul class='formatted-list'><li>Free speech transcripts</li><li>Raw audio waveforms</li></ul></dd><dt>Why Missing</dt><dd><ul class='formatted-list'><li>Removed to reduce re-identification risk and comply with HIPAA Safe Harbor.</li></ul></dd></dl></li></ul></dd></dl></li></ol></dd><dt>Subpopulations</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>adult-cohort</dd><dt>Identification</dt><dd><ul class='formatted-list'><li>Adults only in v1.1</li></ul></dd><dt>Distribution</dt><dd><ul class='formatted-list'><li>306 participants; 12,523 recordings across predefined clinical condition groups</li></ul></dd></dl></li></ul></dd><dt>Sensitive Elements</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>health-data</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Contains de-identified health-related data (clinical and questionnaire information).</li></ul></dd></dl></li></ul></dd><dt>Is Deidentified</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>hipaa-safe-harbor</dd><dt>Description</dt><dd><ul class='formatted-list'><li><div class="long-description">HIPAA Safe Harbor de-identification applied; removal of identifiers (e.g., names, fine-grained dates, contact details, geographic locators below country), removal of free speech transcripts, and omission of raw audio in v1.1.</div></li></ul></dd></dl></dd><dt>Acquisition Methods</dt><dd><ol class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>acquisition-overview</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Voice recordings directly observed; phenotype/questionnaire data reported by participants; derived features computed from recordings.</li></ul></dd><dt>Was Directly Observed</dt><dd>True</dd><dt>Was Reported By Subjects</dt><dd>True</dd><dt>Was Inferred Derived</dt><dd>True</dd><dt>Was Validated Verified</dt><dd>Standardized protocols and multi-site QA were used; derived features computed via established toolkits.</dd></dl></li></ol></dd><dt>Collection Mechanisms</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>data-capture</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Custom tablet application and headset microphones used when possible; data exported from REDCap and converted using an open-source library.</li></ul></dd></dl></li></ul></dd><dt>Data Collectors</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>site-teams</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Researchers and clinicians at five North American specialty-clinic sites.</li></ul></dd></dl></li></ul></dd><dt>Collection Timeframes</dt><dd></dd><dt>Ethical Reviews</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>irb-approval</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Data collection and sharing approved by the University of South Florida Institutional Review Board.</li></ul></dd></dl></li></ul></dd><dt>Preprocessing Strategies</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>waveform-prep-and-feature-extraction</dd><dt>Description</dt><dd><ul class='formatted-list'><li><div class="long-description">Waveforms converted to mono and resampled to 16 kHz with anti-aliasing; spectrograms via STFT (25 ms window, 10 ms hop, 512-point FFT, power); 60-coefficient MFCCs computed; static acoustic features extracted (e.g., openSMILE) and phonetic/prosodic features via Parselmouth/Praat; transcripts generated by OpenAI Whisper Large (free speech transcripts removed prior to release).</div></li></ul></dd><dt>Used Software</dt><dd><table class="data-table"><thead><tr><th>Description</th><th>Name</th><th>URL</th></tr></thead><tbody><tr><td>Open-source library used to preprocess waveforms and merge phenotype data.</td><td>b2aiprep</td><td>https://github.com/sensein/b2aiprep</td></tr><tr><td>Acoustic feature extraction toolkit.</td><td>openSMILE</td><td></td></tr><tr><td>Speech analysis software for phonetics.</td><td>Praat</td><td></td></tr><tr><td>Python interface to Praat.</td><td>Parselmouth</td><td></td></tr><tr><td>Audio processing components for PyTorch.</td><td>torchaudio</td><td></td></tr><tr><td>ASR model used to generate transcripts (free speech transcripts removed before release).</td><td>OpenAI Whisper Large</td><td></td></tr></tbody></table></dd></dl></li></ul></dd><dt>Cleaning Strategies</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>de-identification-and-redactions</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Removal of HIPAA Safe Harbor identifiers; removal of state/province with retention of country; removal of free speech transcripts; omission of raw audio waveforms in v1.1.</li></ul></dd></dl></li></ul></dd><dt>Labeling Strategies</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>transcription</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Automatic speech recognition using OpenAI Whisper Large; transcripts of free speech removed prior to release.</li></ul></dd></dl></li></ul></dd><dt>Raw Sources</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>raw-audio-availability</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Raw audio waveforms were collected but are not distributed in v1.1; only derived representations are provided.</li></ul></dd></dl></li></ul></dd><dt>Existing Uses</dt><dd><table class="data-table"><thead><tr><th>Description</th><th>Name</th></tr></thead><tbody><tr><td>{'Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice': 'An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155'}</td><td>dataset-citation</td></tr><tr><td>Goldberger, A., et al. (2000). PhysioBank, PhysioToolkit, and PhysioNet. Circulation. RRID:SCR_007345.</td><td>platform-citation</td></tr></tbody></table></dd><dt>Use Repository</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>project-and-platform-links</dd><dt>Description</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Project Documentation Site</dt><dd><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></dd></dl></li><li><dl class='nested-dict'><dt>DOI Landing For V1.1 On Physionet</dt><dd><a href="https://doi.org/10.13026/249v-w155" target="_blank">https://doi.org/10.13026/249v-w155</a></dd></dl></li></ul></dd></dl></li></ul></dd><dt>Other Tasks</dt><dd></dd><dt>Future Use Impacts</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>derived-only-constraints</dd><dt>Description</dt><dd><ul class='formatted-list'><li>The absence of raw audio may limit certain analyses (e.g., new feature extraction requiring original waveforms) but reduces re-identification risk.</li></ul></dd></dl></li></ul></dd><dt>Discouraged Uses</dt><dd></dd><dt>Distribution Formats</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>formats</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Parquet</li><li>TSV</li><li>JSON</li></ul></dd></dl></li></ul></dd><dt>Distribution Dates</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>initial-release</dd><dt>Description</dt><dd><ul class='formatted-list'><li>2025-01-17</li></ul></dd></dl></li></ul></dd><dt>License And Use Terms</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>access-and-licensing</dd><dt>Description</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Platform</dt><dd>PhysioNet</dd></dl></li><li><dl class='nested-dict'><dt>Access</dt><dd>Registered/Restricted Access; only registered users who sign the Bridge2AI Voice Registered Access Agreement may access files.</dd></dl></li><li><dl class='nested-dict'><dt>License</dt><dd>Bridge2AI Voice Registered Access License; applicable Data Use Agreement required.</dd></dl></li></ul></dd></dl></dd><dt>Ip Restrictions</dt><dd></dd><dt>Regulatory Restrictions</dt><dd></dd><dt>Maintainers</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>maintainers</dd><dt>Description</dt><dd><ul class='formatted-list'><li>PhysioNet platform team</li><li>Bridge2AI-Voice consortium</li></ul></dd></dl></li></ul></dd><dt>Errata</dt><dd></dd><dt>Updates</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>version-history</dd><dt>Description</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>V1.0 (2024)</dt><dd>Initial release.</dd></dl></li><li><dl class='nested-dict'><dt>V1.1 (2025 01 17)</dt><dd>Added MFCCs; files for v1.1 are no longer available on the platform.</dd></dl></li><li><dl class='nested-dict'><dt>Newer Versions Available</dt><dd>2.0.0 (2025-04-16), 2.0.1 (2025-08-18).</dd></dl></li></ul></dd></dl></dd><dt>Retention Limit</dt><dd></dd><dt>Version Access</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>older-version-availability</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Older version 1.1 files are no longer available; latest version on the platform is 2.0.1.</li></ul></dd></dl></dd><dt>Extension Mechanism</dt><dd></dd><dt>Is Tabular</dt><dd>yes</dd></dl></li><li><dl class='nested-dict'><dt>ID</dt><dd>healthdatanexus-voice-1.0</dd><dt>Name</dt><dd>Health Data Nexus VOICE 1.0</dd><dt>Title</dt><dd>VOICE 1.0</dd><dt>Description</dt><dd>Resource page for the VOICE 1.0 project on Health Data Nexus.</dd><dt>Page</dt><dd><a href="https://healthdatanexus.ai/content/b2ai-voice/1.0/" target="_blank">https://healthdatanexus.ai/content/b2ai-voice/1.0/</a></dd><dt>Version</dt><dd>1.0</dd><dt>DOI</dt><dd><a href="https://doi.org/10.57764/qb6h-em84" target="_blank">doi:10.57764/qb6h-em84</a></dd><dt>Keywords</dt><dd><ul class='formatted-list'><li>VOICE</li><li>Health Data Nexus</li><li>b2ai-voice</li><li>healthdatanexus.ai</li></ul></dd><dt>Created By</dt><dd><ul class='formatted-list'><li>Health Data Nexus</li></ul></dd><dt>Distribution Formats</dt><dd></dd><dt>Distribution Dates</dt><dd></dd><dt>License And Use Terms</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>unspecified-license</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Resource/landing page for the VOICE 1.0 project; licensing for downloadable data not specified on this page.</li></ul></dd></dl></dd></dl></li></ol></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-10-29 00:01:38 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>