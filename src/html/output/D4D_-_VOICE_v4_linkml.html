
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>VOICE (Claude Code Synthesized) - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>VOICE (Claude Code Synthesized)</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üéØ</span>
                    <div>
                        <h2 class="section-title">Motivation</h2>
                        <p class="section-description">Why was the dataset created?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Purposes
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Response</dt><dd>Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice representations to demographic, clinical, and questionnaire data.
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Funding And Acknowledgements
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Funding</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Agency</dt><dd>National Institutes of Health</dd><dt>Award Number</dt><dd>3OT2OD032720-01S1</dd><dt>Project Title</dt><dd>Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database to understand disease like never before</dd></dl></li></ul></dd><dt>Acknowledgements</dt><dd>We acknowledge the contribution of study participants and the NIH for continued support of the project.</dd><dt>Platform Support</dt><dd>National Institute of Biomedical Imaging and Bioengineering under NIH grant number R01EB030362 supported PhysioNet infrastructure.</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üìä</span>
                    <div>
                        <h2 class="section-title">Composition</h2>
                        <p class="section-description">What do the instances represent?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Instances
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Representation</dt><dd>Adult participants with voice, neurological, mood, and respiratory disorders</dd><dt>Instance Type</dt><dd>Participants and their voice-derived features with clinical phenotype data</dd><dt>Data Type</dt><dd><div class="long-description">Spectrograms derived from audio; mel-frequency cepstral coefficients; acoustic feature sets (openSMILE); phonetic and prosodic features (Parselmouth and Praat); transcriptions generated by OpenAI Whisper Large (free speech transcripts removed); phenotype and questionnaire data.
</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Distribution Formats
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Parquet files (spectrograms, MFCC); TSV files (phenotype, static features); JSON files (data dictionaries)
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Distribution Dates
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>v1.1 released 2025-01-17</dd></dl></li></ul></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value"><a href="https://doi.org/10.13026/249v-w155" target="_blank">doi:10.13026/249v-w155</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. Recent advances in artificial intelligence have provided techniques to extract previously unknown prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. Here we present Bridge2AI-Voice, a comprehensive collection of data derived from voice recordings with corresponding clinical information. Bridge2AI-Voice v1.0, the initial release, provides 12,523 recordings for 306 participants collected across five sites in North America. Participants were selected based on known conditions which manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and respiratory disorders. The initial release contains data considered low risk, including derivations such as spectrograms but not the original voice recordings. Detailed demographic, clinical, and validated questionnaire data are also made available.
</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Language
                            
                        </label>
                        <div class="item-value">en</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Rrid
                            
                        </label>
                        <div class="item-value">SCR_007345</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Project Website
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Publisher
                            
                        </label>
                        <div class="item-value">PhysioNet</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Release Date
                            
                        </label>
                        <div class="item-value">2025-01-17</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://healthdatanexus.ai/content/b2ai-voice/1.0/" target="_blank">https://healthdatanexus.ai/content/b2ai-voice/1.0/</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>VOICE</li><li>voice</li><li>bridge2ai</li><li>biomarker</li><li>dementia</li><li>mood disorders</li><li>cancer</li><li>voice disorders</li><li>neurological disorders</li><li>respiratory disorders</li><li>spectrograms</li><li>acoustic features</li><li>Health Data Nexus</li><li>PhysioNet</li><li>ethical data</li><li>AI</li><li>machine learning</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Addressing Gaps
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Response</dt><dd><div class="long-description">Create an ethically sourced flagship dataset to enable AI research on voice as a biomarker, supporting critical insights into voice-health relationships not previously available in standardized datasets.
</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Composition
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Overview</dt><dd>Derived audio representations and associated phenotype data from adult participants recruited at specialty clinics.</dd><dt>Population</dt><dd><dl class='nested-dict'><dt>Cohort Scope</dt><dd>Adult cohort only as of v1.1</dd><dt>Recruitment Region</dt><dd>Five sites in North America</dd><dt>Participants</dt><dd>306</dd><dt>Recordings</dt><dd>12,523</dd></dl></dd><dt>Condition Groups</dt><dd><ul class='formatted-list'><li>Voice disorders</li><li>Neurological and neurodegenerative disorders</li><li>Mood and psychiatric disorders</li><li>Respiratory disorders</li><li>Pediatric voice and speech disorders (planned; not included in v1.1)</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Data Characteristics
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Modalities</dt><dd><ul class='formatted-list'><li>Spectrograms derived from audio</li><li>Mel-frequency cepstral coefficients</li><li>Acoustic feature sets (openSMILE)</li><li>Phonetic and prosodic features (Parselmouth and Praat)</li><li>Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)</li><li>Phenotype and questionnaire data</li></ul></dd><dt>Data Formats</dt><dd><ul class='formatted-list'><li>Parquet</li><li>TSV</li><li>JSON</li></ul></dd><dt>Identifiers In Files</dt><dd><ul class='formatted-list'><li>participant_id</li><li>session_id</li><li>task_name</li></ul></dd><dt>Sampling And Dimensions</dt><dd>Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional to recording length.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Process
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Setting</dt><dd>Specialty clinics and institutions</dd><dt>Participant Selection</dt><dd>Screened for inclusion and exclusion criteria within five predetermined groups.</dd><dt>Consent</dt><dd>Participants provided consent for data collection and sharing of de-identified research data.</dd><dt>Procedure</dt><dd>Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice, disease specific information, and voice tasks such as sustained vowel phonation.</dd><dt>Data Capture</dt><dd>Custom tablet application used for collection; headset used when possible.</dd><dt>Sessions</dt><dd>Most participants completed one session; a subset required multiple sessions.</dd><dt>Data Export And Merge</dt><dd>Exported from REDCap and converted using an open source library.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Acquisition Methods
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice, disease specific information, and voice tasks such as sustained vowel phonation.
</dd><dt>Was Directly Observed</dt><dd>Yes (voice recordings via tablet application)</dd><dt>Was Reported By Subjects</dt><dd>Yes (questionnaires)</dd><dt>Was Validated Verified</dt><dd>Standardized data collection protocol; validated questionnaires; REDCap data capture
</dd></dl></li></ol></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Mechanisms
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Custom tablet application used for collection; headset used when possible; REDCap for phenotype data
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Data Collectors
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Five data collection sites in North America (specialty clinics)
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Timeframes
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Initial release (v1.0) in 2024; v1.1 released 2025-01-17; latest version 2.0.1 released 2025-08-18
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Preprocessing Strategies
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Raw audio processing: Converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter. Spectrograms: Short-time FFT with 25 ms window, 10 ms hop, 512-point FFT; stored in power representation. MFCC: 60 coefficients computed from spectrograms. Acoustic features: Extracted using openSMILE capturing temporal dynamics and acoustic characteristics. Phonetic/prosodic features: Computed using Parselmouth and Praat; includes measures of fundamental frequency, formants, and voice quality. Transcription: Generated using OpenAI Whisper Large; transcripts of free speech audio were removed prior to release. Open source code: b2aiprep library used to preprocess waveforms and merge phenotype data.
</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Cleaning Strategies
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">De-identification using HIPAA Safe Harbor approach; removal of identifiers including names, geographic locators, dates at finer than year resolution, phone/fax numbers, email addresses, IP addresses, Social Security Numbers, medical record numbers, health plan beneficiary numbers, device identifiers, license numbers, account numbers, vehicle identifiers, website URLs, full face photos, biometric identifiers, and any unique identifiers. Removal of state and province; retention of country of data collection. Removal of transcripts of free speech audio. Omission of raw audio waveforms in v1.1; only spectrograms and other derived features are released.
</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Is Deidentified
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><ul class='formatted-list'><li>HIPAA Safe Harbor de-identification applied</li><li>No raw audio waveforms in v1.1; only derived representations released</li><li>Free speech transcripts removed to reduce re-identification risk</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sensitive Elements
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Health condition information (voice disorders, neurological disorders, mood disorders, respiratory disorders) under restricted access with data use agreement
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Files
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Version Notice</dt><dd>Files for version 1.1 are no longer available; the latest version of this project is 2.0.1.</dd><dt>Listing</dt><dd><table class="data-table"><thead><tr><th>Description</th><th>Path</th><th>Type</th></tr></thead><tbody><tr><td>Dense time-frequency representations derived from voice waveforms; includes participant_id, session_...</td><td>spectrograms.parquet</td><td>Parquet</td></tr><tr><td>Mel-frequency cepstral coefficients derived from spectrograms; arrays of size 60 x N per recording.</td><td>mfcc.parquet</td><td>Parquet</td></tr><tr><td>One row per participant; demographics, acoustic confounders, and responses to validated questionnair...</td><td>phenotype.tsv</td><td>TSV</td></tr><tr><td>Data dictionary for phenotype.tsv with one sentence descriptions per column.</td><td>phenotype.json</td><td>JSON</td></tr><tr><td>One row per audio recording; features derived using openSMILE, Praat, parselmouth, and torchaudio.</td><td>static_features.tsv</td><td>TSV</td></tr><tr><td>Data dictionary for static_features.tsv with feature descriptions.</td><td>static_features.json</td><td>JSON</td></tr></tbody></table></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Limitations
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>Adult cohort only in v1.1; pediatric data not included.</li><li>No raw audio is released in v1.1; analyses are limited to derived representations.</li><li>Participants were selected based on conditions known to manifest in voice, which may affect generalizability.</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Maintainers
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Bridge2AI-Voice project team; hosted on PhysioNet
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Release Notes
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Date</th><th>Notes</th><th>Version</th></tr></thead><tbody><tr><td>2025-01-17</td><td>This release added Mel-frequency cepstral coefficients.</td><td>1.1</td></tr><tr><td>2024</td><td>Initial release of the dataset.</td><td>1.0</td></tr><tr><td>2025-04-16</td><td>Major update (details not provided in source)</td><td>2.0.0</td></tr><tr><td>2025-08-18</td><td>Latest version (details not provided in source)</td><td>2.0.1</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Authors
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Name</th></tr></thead><tbody><tr><td>Alistair Johnson</td></tr><tr><td>Jean-Christophe B√©lisle-Pipon</td></tr><tr><td>David Dorr</td></tr><tr><td>Satrajit Ghosh</td></tr><tr><td>Philip Payne</td></tr><tr><td>Maria Powell</td></tr><tr><td>Anais Rameau</td></tr><tr><td>Vardit Ravitsky</td></tr><tr><td>Alexandros Sigaras</td></tr><tr><td>Olivier Elemento</td></tr><tr><td>Yael Bensoussan</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Corresponding Author
                            
                        </label>
                        <div class="item-value">Not publicly listed; contact information requires login.</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Software And Tools
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Preprocessing Code</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>b2aiprep</dd><dt>URL</dt><dd><a href="https://github.com/sensein/b2aiprep" target="_blank">https://github.com/sensein/b2aiprep</a></dd><dt>Description</dt><dd>Open source library used to preprocess raw audio and merge phenotype data.</dd></dl></dd><dt>Referenced Tools</dt><dd><ul class='formatted-list'><li>openSMILE</li><li>Praat</li><li>Parselmouth</li><li>torchaudio</li><li>OpenAI Whisper Large</li><li>librosa (example usage for visualization)</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Citations
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Dataset Citation</dt><dd><div class="long-description">Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155</div></dd><dt>Platform Citation</dt><dd><div class="long-description">Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215‚Äìe220. RRID:SCR_007345.</div></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            External Resources
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>External Resources</dt><dd><ul class='formatted-list'><li>PhysioNet platform (https://physionet.org/)</li><li>Health Data Nexus (https://healthdatanexus.ai/content/b2ai-voice/1.0/)</li><li>Project documentation (https://docs.b2ai-voice.org)</li><li>b2aiprep GitHub repository (https://github.com/sensein/b2aiprep)</li><li>Bridge2AI Voice REDCap on Zenodo (https://doi.org/10.5281/zenodo.14148755)</li></ul></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            References
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><div class="long-description">Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D., Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926</div></li><li><div class="long-description">Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D., Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P., Tang, J., Sigaras, A., Elemento, O., Dorr, D., ‚Ä¶ Bridge2AI-Voice. (2024). Bridge2AI Voice REDCap (v3.20.0). Zenodo. https://doi.org/10.5281/zenodo.14148755</div></li><li><div class="long-description">Florian Eyben, Martin W√∂llmer, Bj√∂rn Schuller: openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010.</div></li><li>Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.</li><li>Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics. 2018 Nov 1;71:1-5.</li><li><div class="long-description">Hwang, J., Hira, M., Chen, C., Zhang, X., Ni, Z., Sun, G., Ma, P., Huang, R., Pratap, V., Zhang, Y., Kumar, A., Yu, C.-Y., Zhu, C., Liu, C., Kahn, J., Ravanelli, M., Sun, P., Watanabe, S., Shi, Y., Tao, T., Scheibler, R., Cornell, S., Kim, S., & Petridis, S. (2023). TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch. arXiv preprint arXiv:2310.17864</div></li><li><div class="long-description">Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-B√©lair, V, & Shi, Y. (2021). TorchAudio: Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018.</div></li><li>Bevers, I., Ghosh, S., Johnson, A., Brito, R., Bedrick, S., Catania, F., & Ng, E. (2017). My Research Software (Version 0.21.0) [Computer software]. https://github.com/sensein/b2aiprep</li><li><div class="long-description">Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84</div></li></ul></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üöÄ</span>
                    <div>
                        <h2 class="section-title">Uses</h2>
                        <p class="section-description">What (other) tasks could the dataset be used for?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Tasks
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Response</th></tr></thead><tbody><tr><td>Development and benchmarking of models to associate voice-derived features with health conditions.
</td></tr><tr><td>Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived da...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Intended Uses
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Primary</dt><dd>Artificial intelligence and clinical research on voice as a biomarker of health.</dd><dt>Examples</dt><dd><ul class='formatted-list'><li>Development and benchmarking of models to associate voice-derived features with health conditions.</li><li>Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data.</li></ul></dd><dt>Usage Notes</dt><dd>Data are provided as derived representations without raw audio to reduce re-identification risk.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Existing Uses
                            
                        </label>
                        <div class="item-value"></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Other Tasks
                            
                        </label>
                        <div class="item-value"></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Future Use Impacts
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Adult cohort only in v1.1; pediatric data not included, which may limit generalizability. Participants were selected based on conditions known to manifest in voice, which may affect generalizability. Users should account for these sampling characteristics.
</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Discouraged Uses
                            
                        </label>
                        <div class="item-value"></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License And Use Terms
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><ul class='formatted-list'><li>Restricted Access: Only registered users who sign the specified data use agreement can access the files.</li><li>Bridge2AI Voice Registered Access License</li><li>Bridge2AI Voice Registered Access Agreement</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Use Repository
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>PhysioNet restricted access repository</td></tr><tr><td>Health Data Nexus</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üì§</span>
                    <div>
                        <h2 class="section-title">Distribution</h2>
                        <p class="section-description">How will the dataset be distributed?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            DOI
                            
                        </label>
                        <div class="item-value">10.13026/249v-w155</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            DOI URL
                            
                        </label>
                        <div class="item-value"><a href="https://doi.org/10.13026/249v-w155" target="_blank">https://doi.org/10.13026/249v-w155</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Latest Version DOI
                            
                        </label>
                        <div class="item-value">10.13026/37yb-1t42</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Access And Licensing
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Platform</dt><dd>PhysioNet</dd><dt>Access Policy</dt><dd>Restricted Access</dd><dt>Access Conditions</dt><dd>Only registered users who sign the specified data use agreement can access the files.</dd><dt>License</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Data Use Agreement</dt><dd>Bridge2AI Voice Registered Access Agreement</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Version Access
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><ul class='formatted-list'><li>Multiple versions available on PhysioNet platform (v1.1, v2.0.0, v2.0.1)</li><li>Also available on Health Data Nexus (b2ai-voice version 1.0)</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîÑ</span>
                    <div>
                        <h2 class="section-title">Maintenance</h2>
                        <p class="section-description">How will the dataset be maintained?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Version
                            
                        </label>
                        <div class="item-value">1.1</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Updates
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><ul class='formatted-list'><li>Periodic updates planned; v1.1 released 2025-01-17 adding MFCC; v2.0.0 released 2025-04-16; v2.0.1 released 2025-08-18</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üë•</span>
                    <div>
                        <h2 class="section-title">Human Subjects</h2>
                        <p class="section-description">Does the dataset relate to people?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Ethics
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>IRB Approval</dt><dd>Data collection and sharing approved by the University of South Florida Institutional Review Board.</dd><dt>Ethical Position</dt><dd>Dataset is ethically sourced with privacy protections; derived data released for low risk.</dd><dt>Conflicts Of Interest</dt><dd>None to declare.</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-11-16 17:37:50 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>