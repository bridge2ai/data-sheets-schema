# D4D Datasheet for Bridge2AI-Voice Dataset
# Generated by Claude Code (Sonnet 4.5) from source documentation
# Source: Concatenated original documentation files from data/preprocessed/individual/VOICE/
# Generation Date: 2025-11-20
# Model: claude-sonnet-4-5-20250929, Temperature: 0.0 (deterministic)

id: Bridge2AI-Voice Dataset
name: Bridge2AI-Voice Dataset
title: Bridge2AI-Voice - An ethically-sourced, diverse voice dataset linked to health information
description: >
  The Bridge2AI-Voice dataset is a comprehensive collection of data derived from voice recordings
  with corresponding clinical information, designed to enable research in artificial intelligence
  and support critical insights into the use of voice as a biomarker of health. The dataset contains
  samples from conventional acoustic tasks including respiratory sounds, cough sounds, and free speech
  prompts, capturing voice, speech, and language data relating to health. Participants were selected
  based on known conditions which manifest within the voice waveform including voice disorders,
  neurological disorders, mood disorders, respiratory disorders, and pediatric voice and speech
  disorders. Version 1.1 provides 12,523 recordings for 306 participants collected across five sites
  in North America. The dataset includes spectrograms, MFCCs, acoustic features, and detailed
  demographic and clinical information, with original audio waveforms withheld for privacy. Data
  are distributed via PhysioNet with restricted access under Data Transfer and Use Agreement.

language: en

keywords:
  - voice
  - speech
  - biomarker
  - acoustic features
  - spectrograms
  - MFCC
  - voice disorders
  - neurological disorders
  - mood disorders
  - respiratory disorders
  - pediatric speech
  - artificial intelligence
  - machine learning
  - Bridge2AI
  - PhysioNet
  - ethically sourced

# ============================================================================
# MOTIVATION
# ============================================================================

purposes:
  - response: >
      Create an ethically sourced flagship dataset to enable future research in artificial
      intelligence and support critical insights into the use of voice as a biomarker of health,
      particularly for voice disorders, neurological/neurodegenerative disorders, mood/psychiatric
      disorders, respiratory disorders, and pediatric speech disorders.

tasks:
  - response: >
      Enable AI/ML research for disease screening, therapeutic monitoring, and diagnostic applications
      using voice and speech analysis; support machine learning models for detection of various
      health conditions that manifest in voice characteristics such as fundamental frequency,
      prosody, articulation, and acoustic features.

addressing_gaps:
  - response: >
      Address the pressing need for large, high quality, multi-institutional and diverse voice
      databases linked to other health biomarkers to fuel voice AI research; overcome limitations
      of existing research with small datasets, limited demographic diversity, lack of standardized
      protocols, and external validity concerns.

# ============================================================================
# COMPOSITION
# ============================================================================

instances:
  - representation: >
      Individual voice recordings from participants with various health conditions, represented
      as spectrograms, Mel-frequency cepstral coefficients (MFCCs), and derived acoustic features
      linked to detailed phenotypic and clinical information.
    instance_type: Audio recordings and derived acoustic features per participant session
    data_type: >
      Spectrograms (time-frequency representations), MFCCs (60 coefficients), acoustic features
      (openSMILE, Praat, parselmouth, torchaudio), phonetic and prosodic features (fundamental
      frequency, formants, voice quality), transcriptions (Whisper Large model), demographic
      data, validated questionnaires, clinical information.

subsets:
  - name: Voice Disorders cohort
    description: >
      Participants with laryngeal disorders including benign and malignant lesions affecting
      vocal fold shape, mass, density, and tension resulting in changes in vibratory function
      and phonation.
  - name: Neurological and Neurodegenerative Disorders cohort
    description: >
      Participants with conditions such as Parkinson's, ALS, acute strokes presenting with
      dysarthria or aphasia, characterized by slowed, low frequency, monotonous speech, and
      vocal tremor.
  - name: Mood and Psychiatric Disorders cohort
    description: >
      Participants with depression (decreased fundamental frequency, monotonous speech) and
      anxiety disorders (significant increase in fundamental frequency).
  - name: Respiratory Disorders cohort
    description: >
      Participants with respiratory conditions affecting breath, cough, and voice sounds used
      for diagnostic purposes and therapeutic monitoring.
  - name: Pediatric Voice and Speech Disorders cohort
    description: >
      Pediatric participants with conditions including autism spectrum disorder and speech delays
      (data collection ongoing, not included in v1.1 release).

sampling_strategies:
  - is_sample:
      - >
        Yes. Participants recruited from specialty clinics and institutions across five sites in
        North America based on predetermined disease cohort categories.
  - is_random:
      - >
        No. Purposive sampling based on membership to five predetermined groups (Voice disorders,
        Neurological disorders, Mood disorders, Respiratory disorders, Pediatric disorders) and
        meeting inclusion/exclusion criteria.
  - is_representative:
      - >
        Designed to represent diversity across disease categories and demographics; recruitment
        aimed at ethically sourced, diverse participant representation; v1.1 includes 306 adult
        participants from 5 North American sites.
  - strategies:
      - >
        Participants selected from specialty clinics; screened for inclusion/exclusion criteria
        prior to visit; standardized data collection protocol across all sites; focus on
        conditions with recognized voice/speech manifestations.

subpopulations:
  - identification:
      - Voice Disorders
      - Neurological and Neurodegenerative Disorders
      - Mood and Psychiatric Disorders
      - Respiratory Disorders
      - Pediatric cohort (planned for future release)
    distribution:
      - >
        v1.1 includes 306 adult participants with 12,523 recordings; demographic and clinical
        diversity documented in phenotype data; pediatric data to be included in future releases.

# ============================================================================
# COLLECTION
# ============================================================================

acquisition_methods:
  - description:
      - >
        Standardized protocol using custom tablet application with headset for voice recording
        tasks; data collection included sustained phonation of vowel sounds, respiratory sounds,
        cough sounds, and free speech prompts; single session sufficient for most participants,
        with subset requiring multiple sessions.
    was_directly_observed: >
      Yes (voice recordings collected during standardized protocol sessions)
    was_reported_by_subjects: >
      Yes (demographic information, health questionnaires, validated questionnaires, disease-specific
      information, known voice confounders)

collection_mechanisms:
  - description:
      - >
        Hardware: Tablet with custom data collection application, headset microphone for audio
        recording; Software: Custom REDCap-based application for data collection (available as
        Bridge2AI Voice REDCap v3.20.0 on Zenodo), b2aiprep library for preprocessing and data
        export; Audio processing: monaural conversion, 16 kHz resampling with Butterworth
        anti-aliasing filter, short-time FFT (25ms window, 10ms hop, 512-point FFT), openSMILE,
        Praat, parselmouth, torchaudio for feature extraction, OpenAI Whisper Large for transcription.

collection_timeframes:
  - description:
      - >
        v1.0 released January 2024; v1.1 released January 17, 2025 (added MFCCs); v2.0.0 released
        April 16, 2025; v2.0.1 released August 18, 2025; ongoing data collection with periodic
        releases planned.

data_collectors:
  - description:
      - >
        Multi-institutional team across five North American sites; principal investigators include
        Alistair Johnson, Yael Bensoussan, Olivier Elemento, Satrajit Ghosh, and others; data
        collection conducted by project investigators at specialty clinics under University of
        South Florida IRB approval.

# ============================================================================
# PREPROCESSING, CLEANING, AND LABELING
# ============================================================================

preprocessing_strategies:
  - description:
      - >
        Raw audio standardized to monaural 16 kHz with Butterworth anti-aliasing filter; spectrograms
        computed using short-time FFT (25ms window, 10ms hop, 512-point FFT); 60 MFCCs extracted
        from spectrograms; acoustic features extracted using openSMILE; phonetic and prosodic
        features computed using Parselmouth and Praat (fundamental frequency, formants, voice
        quality); transcriptions generated using OpenAI Whisper Large model; data exported and
        converted from REDCap using b2aiprep open source library.

cleaning_strategies:
  - description:
      - >
        De-identification following HIPAA Safe Harbor standards: removed names, geographic locators
        (state/province removed, country retained), dates (resolution finer than years), phone/fax,
        email, IP addresses, SSN, medical record numbers, device identifiers, license numbers,
        account numbers, vehicle identifiers, URLs, full face photos, biometric identifiers, unique
        identifiers; transcripts of free speech audio removed; original audio waveforms omitted
        from release (only spectrograms and derived features available).

# ============================================================================
# USES
# ============================================================================

past_uses:
  - description:
      - >
        Literature has demonstrated use of voice analysis for: detection of depression and mood
        disorders via fundamental frequency changes; screening for Parkinson's and ALS via speech
        characteristics; diagnosis of pediatric croup based on barking cough and stridor; detection
        of autism and speech delays in pediatric population using machine learning models.

future_use_impacts:
  - description:
      - >
        Intended for AI/ML research in voice-based disease screening, therapeutic monitoring, and
        diagnostic applications; users must comply with Data Transfer and Use Agreement (DTUA)
        and Certificate of Confidentiality protections; data considered Personally Identifiable
        Information not covered under HIPAA/FERPA.
      - >
        Users must establish appropriate administrative, technical, and physical safeguards; prohibited
        from re-identifying individuals or contacting research participants without IRB approval;
        results encouraged to be published in open-access journals; attribution required citing
        dataset DOI and PhysioNet.

tasks_not_suitable:
  - description:
      - >
        Not suitable for attempts to re-identify participants or link data to other sources for
        identification purposes; not for commercial use without separate agreement; original audio
        waveforms not available in current release to ensure additional data security; transcripts
        of free speech removed to protect privacy.

# ============================================================================
# DISTRIBUTION
# ============================================================================

distribution_formats:
  - description:
      - >
        Distributed via PhysioNet managed by MIT Laboratory for Computational Physiology; three
        main files: spectrograms.parquet (Parquet file storing 513xN dimension spectrograms),
        mfcc.parquet (Parquet file storing 60xN MFCCs), phenotype.tsv (tab-delimited demographics
        and clinical data), phenotype.json (data dictionary), static_features.tsv (features per
        recording), static_features.json (features data dictionary).
      - >
        Data loadable using Python datasets library and pandas; preprocessed using b2aiprep open
        source library available on GitHub; persistent identifiers via DOI (v1.1:
        10.13026/249v-w155, latest: 10.13026/37yb-1t42).

license_and_use_terms:
  description:
    - >
      Bridge2AI Voice Registered Access License; restricted access requiring registration and
      signed Data Transfer and Use Agreement (DTUA); users must agree to terms including: use
      solely for approved research project by authorized persons, no redistribution without
      provider consent, compliance with applicable laws and professional standards, appropriate
      security safeguards for Personally Identifiable Information.
    - >
      Data protected under Certificate of Confidentiality which must be asserted against compulsory
      legal demands; two-year agreement term with data destruction required upon termination;
      attribution required citing dataset DOI and PhysioNet citation; encouraged publication in
      open-access journals.
    - >
      Access policy: Only registered users who sign the specified data use agreement can access
      files; managed by PhysioNet Data Access Compliance Office (DACO).

# ============================================================================
# MAINTENANCE
# ============================================================================

maintainers:
  - description:
      - >
        Bridge2AI-Voice project team led by principal investigators at multiple institutions:
        Alistair Johnson (MIT), Yael Bensoussan (University of South Florida), Olivier Elemento,
        Satrajit Ghosh, and others; supported by University of South Florida IRB; distributed
        via PhysioNet/MIT Laboratory for Computational Physiology.

updates:
  description:
    - >
      Periodic version releases: v1.0 (January 2024, initial release), v1.1 (January 17, 2025,
      added MFCCs), v2.0.0 (April 16, 2025), v2.0.1 (August 18, 2025); future releases aim to
      include original voice data with additional security precautions, pediatric cohort data,
      and expanded participant enrollment.

version_access:
  description:
    - >
      Multiple versions available on PhysioNet with distinct DOIs; v1.1 DOI: 10.13026/249v-w155;
      latest version DOI: 10.13026/37yb-1t42; users can access specific versions or latest version;
      version-specific citations required.

errata:
  description:
    - >
      v1.1 note: only data from adult cohort available, pediatric data to be included in future
      releases; v1.1 files no longer available, latest version is v2.0.1; updates and corrections
      documented in release notes.

# ============================================================================
# RELATED RESOURCES
# ============================================================================

related_resources:
  - id: https://doi.org/10.13026/249v-w155
    type: dataset
    description: Bridge2AI-Voice v1.1 on PhysioNet
  - id: https://doi.org/10.13026/37yb-1t42
    type: dataset
    description: Bridge2AI-Voice latest version on PhysioNet
  - id: https://doi.org/10.57764/qb6h-em84
    type: dataset
    description: Bridge2AI-Voice v1.0 on Health Data Nexus
  - id: https://docs.b2ai-voice.org
    type: documentation
    description: Bridge2AI-Voice project documentation
  - id: https://doi.org/10.5281/zenodo.14148755
    type: software
    description: Bridge2AI Voice REDCap v3.20.0 data collection application
  - id: https://github.com/sensein/b2aiprep
    type: software
    description: b2aiprep library for preprocessing audio and merging source data
  - id: https://doi.org/10.21437/Interspeech.2024-1926
    type: publication
    description: >
      Rameau A, et al. Developing Multi-Disorder Voice Protocols. Interspeech 2024.
  - id: https://physionet.org
    type: repository
    description: >
      PhysioNet repository managed by MIT Laboratory for Computational Physiology (NIBIB grant
      R01EB030362)

# ============================================================================
# PROVENANCE
# ============================================================================

generation_metadata:
  generated_by: Claude Code (Anthropic)
  model: claude-sonnet-4-5-20250929
  temperature: 0.0
  generation_date: "2025-11-20"
  source_documents:
    - B2AI-Voice DTUA 2025 2025-09-04.pdf (192000 bytes, 9 pages)
    - physionet_b2ai-voice_1.1_row14.txt (21379 bytes)
    - healthnexus_row13.json (109 bytes)
  concatenated_source: data/preprocessed/concatenated/sources/VOICE_sources_concatenated.txt
  total_source_size: 213488 bytes
