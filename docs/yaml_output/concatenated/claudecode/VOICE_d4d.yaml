# D4D extracted from concatenated documents
# Project: VOICE
# Model: Claude Sonnet 4.5 (temperature=0.0)
# Schema: data_sheets_schema_all.yaml
# Generated: 2025-11-15 (deterministic synthesis by Claude Code)
# Metadata: See VOICE_d4d_metadata.yaml

id: "doi:10.13026/249v-w155"
name: Bridge2AI-Voice
title: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information"
description: >
  The human voice contains complex acoustic markers which have been linked to important health conditions
  including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising
  characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility.
  Recent advances in artificial intelligence have provided techniques to extract previously unknown
  prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project
  seeks to create an ethically sourced flagship dataset to enable future research in artificial
  intelligence and support critical insights into the use of voice as a biomarker of health. Here we
  present Bridge2AI-Voice, a comprehensive collection of data derived from voice recordings with
  corresponding clinical information. Bridge2AI-Voice v1.0, the initial release, provides 12,523 recordings
  for 306 participants collected across five sites in North America. Participants were selected based on
  known conditions which manifest within the voice waveform including voice disorders, neurological
  disorders, mood disorders, and respiratory disorders. The initial release contains data considered low
  risk, including derivations such as spectrograms but not the original voice recordings. Detailed
  demographic, clinical, and validated questionnaire data are also made available.
language: en
doi: "10.13026/249v-w155"
doi_url: "https://doi.org/10.13026/249v-w155"
latest_version_doi: "10.13026/37yb-1t42"
rrid: "SCR_007345"
project_website: "https://docs.b2ai-voice.org"
publisher: "PhysioNet"
release_date: "2025-01-17"
version: "1.1"
page: "https://healthdatanexus.ai/content/b2ai-voice/1.0/"
keywords:
  - VOICE
  - voice
  - bridge2ai
  - biomarker
  - dementia
  - mood disorders
  - cancer
  - voice disorders
  - neurological disorders
  - respiratory disorders
  - spectrograms
  - acoustic features
  - Health Data Nexus
  - PhysioNet
  - ethical data
  - AI
  - machine learning
purposes:
  - response: >
      Enable ethically sourced, large-scale research on voice as a biomarker of health by linking
      derived voice representations to demographic, clinical, and questionnaire data.
tasks:
  - response: >
      Development and benchmarking of models to associate voice-derived features with health conditions.
  - response: >
      Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data.
addressing_gaps:
  - response: >
      Create an ethically sourced flagship dataset to enable AI research on voice as a biomarker,
      supporting critical insights into voice-health relationships not previously available in
      standardized datasets.
instances:
  - representation: "Adult participants with voice, neurological, mood, and respiratory disorders"
    instance_type: "Participants and their voice-derived features with clinical phenotype data"
    data_type: >
      Spectrograms derived from audio; mel-frequency cepstral coefficients; acoustic feature sets
      (openSMILE); phonetic and prosodic features (Parselmouth and Praat); transcriptions generated
      by OpenAI Whisper Large (free speech transcripts removed); phenotype and questionnaire data.
composition:
  overview: "Derived audio representations and associated phenotype data from adult participants recruited at specialty clinics."
  population:
    cohort_scope: "Adult cohort only as of v1.1"
    recruitment_region: "Five sites in North America"
    participants: 306
    recordings: 12523
  condition_groups:
    - "Voice disorders"
    - "Neurological and neurodegenerative disorders"
    - "Mood and psychiatric disorders"
    - "Respiratory disorders"
    - "Pediatric voice and speech disorders (planned; not included in v1.1)"
data_characteristics:
  modalities:
    - "Spectrograms derived from audio"
    - "Mel-frequency cepstral coefficients"
    - "Acoustic feature sets (openSMILE)"
    - "Phonetic and prosodic features (Parselmouth and Praat)"
    - "Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)"
    - "Phenotype and questionnaire data"
  data_formats:
    - "Parquet"
    - "TSV"
    - "JSON"
  identifiers_in_files:
    - "participant_id"
    - "session_id"
    - "task_name"
  sampling_and_dimensions: "Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional to recording length."
collection_process:
  setting: "Specialty clinics and institutions"
  participant_selection: "Screened for inclusion and exclusion criteria within five predetermined groups."
  consent: "Participants provided consent for data collection and sharing of de-identified research data."
  procedure: "Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice, disease specific information, and voice tasks such as sustained vowel phonation."
  data_capture: "Custom tablet application used for collection; headset used when possible."
  sessions: "Most participants completed one session; a subset required multiple sessions."
  data_export_and_merge: "Exported from REDCap and converted using an open source library."
acquisition_methods:
  - description: >
      Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice,
      disease specific information, and voice tasks such as sustained vowel phonation.
    was_directly_observed: "Yes (voice recordings via tablet application)"
    was_reported_by_subjects: "Yes (questionnaires)"
    was_validated_verified: >
      Standardized data collection protocol; validated questionnaires; REDCap data capture
collection_mechanisms:
  - description: >
      Custom tablet application used for collection; headset used when possible; REDCap for phenotype data
data_collectors:
  - description: >
      Five data collection sites in North America (specialty clinics)
collection_timeframes:
  - description: >
      Initial release (v1.0) in 2024; v1.1 released 2025-01-17; latest version 2.0.1 released 2025-08-18
preprocessing_strategies:
  - description: >
      Raw audio processing: Converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter.
      Spectrograms: Short-time FFT with 25 ms window, 10 ms hop, 512-point FFT; stored in power representation.
      MFCC: 60 coefficients computed from spectrograms.
      Acoustic features: Extracted using openSMILE capturing temporal dynamics and acoustic characteristics.
      Phonetic/prosodic features: Computed using Parselmouth and Praat; includes measures of fundamental frequency, formants, and voice quality.
      Transcription: Generated using OpenAI Whisper Large; transcripts of free speech audio were removed prior to release.
      Open source code: b2aiprep library used to preprocess waveforms and merge phenotype data.
cleaning_strategies:
  - description: >
      De-identification using HIPAA Safe Harbor approach; removal of identifiers including names, geographic
      locators, dates at finer than year resolution, phone/fax numbers, email addresses, IP addresses,
      Social Security Numbers, medical record numbers, health plan beneficiary numbers, device identifiers,
      license numbers, account numbers, vehicle identifiers, website URLs, full face photos, biometric
      identifiers, and any unique identifiers. Removal of state and province; retention of country of data
      collection. Removal of transcripts of free speech audio. Omission of raw audio waveforms in v1.1;
      only spectrograms and other derived features are released.
is_deidentified:
  description:
    - "HIPAA Safe Harbor de-identification applied"
    - "No raw audio waveforms in v1.1; only derived representations released"
    - "Free speech transcripts removed to reduce re-identification risk"
sensitive_elements:
  - description: >
      Health condition information (voice disorders, neurological disorders, mood disorders, respiratory
      disorders) under restricted access with data use agreement
files:
  version_notice: "Files for version 1.1 are no longer available; the latest version of this project is 2.0.1."
  listing:
    - path: "spectrograms.parquet"
      type: "Parquet"
      description: "Dense time-frequency representations derived from voice waveforms; includes participant_id, session_id, task_name, spectrogram of size 513 x N."
    - path: "mfcc.parquet"
      type: "Parquet"
      description: "Mel-frequency cepstral coefficients derived from spectrograms; arrays of size 60 x N per recording."
    - path: "phenotype.tsv"
      type: "TSV"
      description: "One row per participant; demographics, acoustic confounders, and responses to validated questionnaires."
    - path: "phenotype.json"
      type: "JSON"
      description: "Data dictionary for phenotype.tsv with one sentence descriptions per column."
    - path: "static_features.tsv"
      type: "TSV"
      description: "One row per audio recording; features derived using openSMILE, Praat, parselmouth, and torchaudio."
    - path: "static_features.json"
      type: "JSON"
      description: "Data dictionary for static_features.tsv with feature descriptions."
intended_uses:
  primary: "Artificial intelligence and clinical research on voice as a biomarker of health."
  examples:
    - "Development and benchmarking of models to associate voice-derived features with health conditions."
    - "Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data."
  usage_notes: "Data are provided as derived representations without raw audio to reduce re-identification risk."
existing_uses: []
other_tasks: []
future_use_impacts:
  - description: >
      Adult cohort only in v1.1; pediatric data not included, which may limit generalizability.
      Participants were selected based on conditions known to manifest in voice, which may affect
      generalizability. Users should account for these sampling characteristics.
discouraged_uses: []
limitations:
  - "Adult cohort only in v1.1; pediatric data not included."
  - "No raw audio is released in v1.1; analyses are limited to derived representations."
  - "Participants were selected based on conditions known to manifest in voice, which may affect generalizability."
distribution_formats:
  - description: >
      Parquet files (spectrograms, MFCC); TSV files (phenotype, static features);
      JSON files (data dictionaries)
distribution_dates:
  - description: "v1.1 released 2025-01-17"
license_and_use_terms:
  description:
    - "Restricted Access: Only registered users who sign the specified data use agreement can access the files."
    - "Bridge2AI Voice Registered Access License"
    - "Bridge2AI Voice Registered Access Agreement"
access_and_licensing:
  platform: "PhysioNet"
  access_policy: "Restricted Access"
  access_conditions: "Only registered users who sign the specified data use agreement can access the files."
  license: "Bridge2AI Voice Registered Access License"
  data_use_agreement: "Bridge2AI Voice Registered Access Agreement"
maintainers:
  - description: >
      Bridge2AI-Voice project team; hosted on PhysioNet
updates:
  description:
    - "Periodic updates planned; v1.1 released 2025-01-17 adding MFCC; v2.0.0 released 2025-04-16; v2.0.1 released 2025-08-18"
version_access:
  description:
    - "Multiple versions available on PhysioNet platform (v1.1, v2.0.0, v2.0.1)"
    - "Also available on Health Data Nexus (b2ai-voice version 1.0)"
release_notes:
  - version: "1.1"
    date: "2025-01-17"
    notes: "This release added Mel-frequency cepstral coefficients."
  - version: "1.0"
    date: "2024"
    notes: "Initial release of the dataset."
  - version: "2.0.0"
    date: "2025-04-16"
    notes: "Major update (details not provided in source)"
  - version: "2.0.1"
    date: "2025-08-18"
    notes: "Latest version (details not provided in source)"
authors:
  - name: "Alistair Johnson"
  - name: "Jean-Christophe Bélisle-Pipon"
  - name: "David Dorr"
  - name: "Satrajit Ghosh"
  - name: "Philip Payne"
  - name: "Maria Powell"
  - name: "Anais Rameau"
  - name: "Vardit Ravitsky"
  - name: "Alexandros Sigaras"
  - name: "Olivier Elemento"
  - name: "Yael Bensoussan"
corresponding_author: "Not publicly listed; contact information requires login."
ethics:
  irb_approval: "Data collection and sharing approved by the University of South Florida Institutional Review Board."
  ethical_position: "Dataset is ethically sourced with privacy protections; derived data released for low risk."
  conflicts_of_interest: "None to declare."
funding_and_acknowledgements:
  funding:
    - agency: "National Institutes of Health"
      award_number: "3OT2OD032720-01S1"
      project_title: "Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database to understand disease like never before"
  acknowledgements: "We acknowledge the contribution of study participants and the NIH for continued support of the project."
  platform_support: "National Institute of Biomedical Imaging and Bioengineering under NIH grant number R01EB030362 supported PhysioNet infrastructure."
software_and_tools:
  preprocessing_code:
    name: "b2aiprep"
    url: "https://github.com/sensein/b2aiprep"
    description: "Open source library used to preprocess raw audio and merge phenotype data."
  referenced_tools:
    - "openSMILE"
    - "Praat"
    - "Parselmouth"
    - "torchaudio"
    - "OpenAI Whisper Large"
    - "librosa (example usage for visualization)"
citations:
  dataset_citation: "Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155"
  platform_citation: "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220. RRID:SCR_007345."
external_resources:
  - external_resources:
      - "PhysioNet platform (https://physionet.org/)"
      - "Health Data Nexus (https://healthdatanexus.ai/content/b2ai-voice/1.0/)"
      - "Project documentation (https://docs.b2ai-voice.org)"
      - "b2aiprep GitHub repository (https://github.com/sensein/b2aiprep)"
      - "Bridge2AI Voice REDCap on Zenodo (https://doi.org/10.5281/zenodo.14148755)"
use_repository:
  - description: "PhysioNet restricted access repository"
  - description: "Health Data Nexus"
references:
  - "Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D., Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926"
  - "Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D., Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P., Tang, J., Sigaras, A., Elemento, O., Dorr, D., … Bridge2AI-Voice. (2024). Bridge2AI Voice REDCap (v3.20.0). Zenodo. https://doi.org/10.5281/zenodo.14148755"
  - "Florian Eyben, Martin Wöllmer, Björn Schuller: openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010."
  - "Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7."
  - "Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics. 2018 Nov 1;71:1-5."
  - "Hwang, J., Hira, M., Chen, C., Zhang, X., Ni, Z., Sun, G., Ma, P., Huang, R., Pratap, V., Zhang, Y., Kumar, A., Yu, C.-Y., Zhu, C., Liu, C., Kahn, J., Ravanelli, M., Sun, P., Watanabe, S., Shi, Y., Tao, T., Scheibler, R., Cornell, S., Kim, S., & Petridis, S. (2023). TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch. arXiv preprint arXiv:2310.17864"
  - "Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-Bélair, V, & Shi, Y. (2021). TorchAudio: Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018."
  - "Bevers, I., Ghosh, S., Johnson, A., Brito, R., Bedrick, S., Catania, F., & Ng, E. (2017). My Research Software (Version 0.21.0) [Computer software]. https://github.com/sensein/b2aiprep"
  - "Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84"
