
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>VOICE Dataset Documentation - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>VOICE Dataset Documentation</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üéØ</span>
                    <div>
                        <h2 class="section-title">Motivation</h2>
                        <p class="section-description">Why was the dataset created?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Purposes
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>ID</th><th>Response</th></tr></thead><tbody><tr><td>purpose-001</td><td>Integrate the use of voice as a biomarker of health in clinical care by generating a substantial mul...</td></tr><tr><td>purpose-002</td><td>Create an ethically sourced flagship dataset to enable future research in artificial intelligence an...</td></tr><tr><td>purpose-003</td><td>Establish standards, best practices, and guidelines for voice data collection and analysis to advanc...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Funders
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Funded through National Institutes of Health grant 3OT2OD032720-01S3 (Bridge2AI: Voice as a Biomarke...</td><td>funder-001</td><td>NIH Office of the Director</td></tr><tr><td>NIBIB supports PhysioNet managed by MIT Laboratory for Computational Physiology under NIH grant numb...</td><td>funder-002</td><td>National Institute of Biomedical Imaging and Bioengineering</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üìä</span>
                    <div>
                        <h2 class="section-title">Composition</h2>
                        <p class="section-description">What do the instances represent?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Instances
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>instance-001</dd><dt>Description</dt><dd><div class="long-description">Adult participants presenting at specialty clinics and institutions across five sites in North America. Participants were selected based on membership to five predetermined disease cohort groups: Respiratory disorders, Voice disorders, Neurological disorders, Mood disorders, and Pediatric. As of v1.1, only data from the adult cohort is available. The initial release (v1.0) provides 306 participants with 12,523 recordings collected through standardized protocols.
</div></dd><dt>Instance Type</dt><dd>Human participants recruited from specialty clinics at multi-institutional sites. Data collection conducted between 2022 and 2026 through IRB-approved protocols with informed consent.
</dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subpopulations
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Participants with laryngeal disorders including laryngeal cancers, vocal fold paralysis, and benign ...</td><td>subpop-001</td><td>Voice Disorders cohort</td></tr><tr><td>Participants with conditions such as Alzheimer's disease, Parkinson's disease, stroke, and ALS exhib...</td><td>subpop-002</td><td>Neurological and Neurodegenerative Disorders cohort</td></tr><tr><td>Participants with depression, schizophrenia, bipolar disorders, and anxiety disorders showing vocal ...</td><td>subpop-003</td><td>Mood and Psychiatric Disorders cohort</td></tr><tr><td>Participants with respiratory conditions including pneumonia, COPD, heart failure, and obstructive s...</td><td>subpop-004</td><td>Respiratory Disorders cohort</td></tr><tr><td>Pediatric participants with voice and speech disorders including autism spectrum disorder and speech...</td><td>subpop-005</td><td>Pediatric cohort</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Distribution Formats
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Access Urls</th><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Spectrograms stored in Parquet format (spectrograms.parquet). Each element contains participant_id, ...</td><td>format-001</td><td>Parquet for spectrograms</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Mel-frequency cepstral coefficients stored in Parquet format (mfcc.parquet). Contains 60xN dimension...</td><td>format-002</td><td>Parquet for MFCCs</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Tab-delimited phenotype file (phenotype.tsv) with one row per unique participant. Contains demograph...</td><td>format-003</td><td>TSV for phenotype data</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Tab-delimited static features file (static_features.tsv) with one row per unique recording. Contains...</td><td>format-004</td><td>TSV for static features</td></tr><tr><td>Contact DACO@b2ai-voice.org</td><td>Original raw audio waveforms available through controlled access only. Interested users contact DACO...</td><td>format-005</td><td>Raw audio (controlled access only)</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value"><a href="https://doi.org/10.13026/37yb-1t42" target="_blank">https://doi.org/10.13026/37yb-1t42</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice - An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. This comprehensive collection provides voice recordings with corresponding clinical information from participants selected based on known conditions which manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and respiratory disorders. The dataset is designed to fuel voice AI research, establish data standards, and promote ethical and trustworthy AI/ML development for voice biomarkers of health. Data collection occurs through a multi-institutional collaborative effort using standardized protocols, custom smartphone applications, and rigorous ethical oversight. The initial release (v1.0) provides 12,523 recordings for 306 participants collected across five sites in North America, with derived features such as spectrograms, MFCCs, acoustic features, and clinical phenotype data. Raw audio data is available through controlled access to protect participant privacy.
</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Language
                            
                        </label>
                        <div class="item-value">en</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice biomarker</li><li>acoustic biomarker</li><li>Bridge2AI</li><li>voice AI</li><li>voice disorders</li><li>neurological disorders</li><li>neurodegenerative disorders</li><li>mood disorders</li><li>psychiatric disorders</li><li>respiratory disorders</li><li>pediatric voice disorders</li><li>speech disorders</li><li>Parkinson's disease</li><li>Alzheimer's disease</li><li>depression</li><li>schizophrenia</li><li>bipolar disorder</li><li>stroke</li><li>ALS</li><li>autism</li><li>speech delay</li><li>laryngeal cancer</li><li>vocal fold paralysis</li><li>pneumonia</li><li>COPD</li><li>heart failure</li><li>obstructive sleep apnea</li><li>spectrogram</li><li>MFCC</li><li>mel-frequency cepstral coefficients</li><li>OpenSMILE</li><li>Praat</li><li>Parselmouth</li><li>federated learning</li><li>ethical AI</li><li>multimodal health data</li><li>electronic health records</li><li>EHR</li><li>radiomics</li><li>genomics</li><li>FAIR principles</li><li>CARE principles</li><li>PhysioNet</li><li>Health Data Nexus</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Addressing Gaps
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>ID</th><th>Response</th></tr></thead><tbody><tr><td>gap-001</td><td>Address the lack of large, high quality, multi-institutional and diverse voice databases linked to m...</td></tr><tr><td>gap-002</td><td>Overcome limitations in existing voice and psychiatric disorder research that has relied on small da...</td></tr><tr><td>gap-003</td><td>Fill the gap in pediatric voice and speech analysis research, which is sparser partly due to ethical...</td></tr><tr><td>gap-004</td><td>Establish missing standards for voice data collection, acoustic analysis, and ethical frameworks for...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Creators
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td>Yael Bensoussan</td><td>creator-001</td><td>-</td></tr><tr><td>Contributor</td><td>Jean-Christophe B√©lisle-Pipon</td><td>creator-002</td><td>-</td></tr><tr><td>Contributor</td><td>David Dorr</td><td>creator-003</td><td>-</td></tr><tr><td>Contributor</td><td>Satrajit Ghosh</td><td>creator-004</td><td>-</td></tr><tr><td>Contributor</td><td>Philip R.O. Payne</td><td>creator-005</td><td>-</td></tr><tr><td>Contributor</td><td>Maria Ellen Powell</td><td>creator-006</td><td>-</td></tr><tr><td>Contributor</td><td>Anais Rameau</td><td>creator-007</td><td>-</td></tr><tr><td>Contributor</td><td>Vardit Ravitsky</td><td>creator-008</td><td>-</td></tr><tr><td>Contributor</td><td>Alexandros Sigaras</td><td>creator-009</td><td>-</td></tr><tr><td>Contributor</td><td>Olivier Elemento</td><td>creator-010</td><td>-</td></tr><tr><td>Contributor</td><td>Alistair Johnson</td><td>creator-011</td><td>-</td></tr><tr><td>Contributor</td><td>Jennifer Siu</td><td>creator-012</td><td>-</td></tr><tr><td>Contributor</td><td>Bridge2AI-Voice Consortium</td><td>creator-013</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subsets
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Contains derived features from voice recordings including spectrograms, MFCCs, acoustic features (Op...</td><td>subset-001</td><td>Public Access Dataset (PhysioNet Registered Access)</td></tr><tr><td>Original raw audio waveforms available through controlled access only to protect participant privacy...</td><td>subset-002</td><td>Controlled Access Raw Audio Dataset</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sampling Strategies
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>sampling-001</dd><dt>Description</dt><dd><div class="long-description">Patients presenting at specialty clinics and institutions were screened for inclusion and exclusion criteria prior to their visit by project investigators. Participants were selected based on membership to five predetermined disease cohort groups to ensure representation across conditions affecting voice: (1) Voice Disorders - laryngeal cancers, vocal fold paralysis, benign laryngeal lesions; (2) Neurological and Neurodegenerative Disorders - Alzheimer's, Parkinson's, stroke, ALS; (3) Mood and Psychiatric Disorders - depression, schizophrenia, bipolar disorders; (4) Respiratory disorders - pneumonia, COPD, heart failure, obstructive sleep apnea; (5) Pediatric diseases - autism, speech delay.
</div></dd><dt>Is Sample</dt><dd><ul class='formatted-list'><li>True</li></ul></dd><dt>Is Random</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Is Representative</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Strategies</dt><dd><ul class='formatted-list'><li>Targeted recruitment from specialty clinics representing five disease cohort categories</li><li>Screening based on known conditions manifesting within voice waveform</li><li>Multi-institutional enrollment across five sites in North America</li><li>Standardized inclusion and exclusion criteria applied by investigators</li></ul></dd></dl></li></ol></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Mechanisms
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th></tr></thead><tbody><tr><td>Data collection conducted using a custom smartphone application on tablet with headset used when pos...</td><td>collection-001</td></tr><tr><td>Multi-institutional data collection across five specialty clinic sites in North America. Patients pr...</td><td>collection-002</td></tr><tr><td>Data collection protocol involved: (1) demographic information collection, (2) health questionnaires...</td><td>collection-003</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Acquisition Methods
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th></tr></thead><tbody><tr><td>Voice recording tasks capturing voice, speech, and language data relating to health. Tasks include s...</td><td>acquisition-001</td></tr><tr><td>Self-reported demographic and medical history questionnaires completed by participants who consent. ...</td><td>acquisition-002</td></tr><tr><td>Electronic health record (EHR) access for participants who consent, permitting investigators to acce...</td><td>acquisition-003</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Preprocessing Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Preprocessing Details</th></tr></thead><tbody><tr><td>Raw audio preprocessing by converting to monaural and resampling to 16 kHz with Butterworth anti-ali...</td><td>preproc-001</td><td>Conversion to monaural audio, Resampling to 16 kHz sampling rate, ... (+2 more)</td></tr><tr><td>Spectrogram extraction - Time-frequency representations computed using short-time Fast Fourier Trans...</td><td>preproc-002</td><td>Short-time FFT with 25ms window, 10ms hop length, ... (+2 more)</td></tr><tr><td>Mel-frequency cepstral coefficients (MFCC) extraction - 60 MFCCs extracted from spectrograms. MFCCs ...</td><td>preproc-003</td><td>60 MFCC coefficients extracted, Derived from spectrograms, ... (+2 more)</td></tr><tr><td>Acoustic feature extraction using OpenSMILE (Speech and Music Interpretation by Large-space Extracti...</td><td>preproc-004</td><td>OpenSMILE feature extraction, Temporal dynamics captured, ... (+2 more)</td></tr><tr><td>Phonetic and prosodic feature computation using Parselmouth and Praat, providing measures of fundame...</td><td>preproc-005</td><td>Parselmouth and Praat feature extraction, Fundamental frequency (F0) measurement, ... (+2 more)</td></tr><tr><td>Transcription generation using OpenAI's Whisper Large model. Automated speech recognition applied to...</td><td>preproc-006</td><td>OpenAI Whisper Large model, Automated transcription of audio, ... (+2 more)</td></tr><tr><td>Data export and conversion from REDCap using open source b2aiprep library developed by the team. Phe...</td><td>preproc-007</td><td>REDCap data export, b2aiprep library conversion, ... (+2 more)</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Cleaning Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Cleaning Details</th><th>Description</th><th>ID</th></tr></thead><tbody><tr><td>HIPAA Safe Harbor compliance, 18 identifier categories removed, ... (+3 more)</td><td>HIPAA Safe Harbor de-identification applied. Identifiers removed include: names, geographic locators...</td><td>cleaning-001</td></tr><tr><td>Audio waveforms excluded from public release, Derived features only in public dataset, ... (+2 more)</td><td>Privacy protection measures for public release - Audio waveforms omitted from public dataset, only d...</td><td>cleaning-002</td></tr><tr><td>Standardized collection protocols, Common smartphone application, ... (+2 more)</td><td>Data standardization across multi-institutional sites through use of standardized protocols, common ...</td><td>cleaning-003</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Maintainers
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Maintainer Details</th><th>Name</th></tr></thead><tbody><tr><td>Multidisciplinary consortium responsible for dataset maintenance including data collection, curation...</td><td>maintainer-001</td><td>University of South Florida (lead institution), Multi-institutional data collection sites (five sites in North America), ... (+5 more)</td><td>Bridge2AI-Voice Consortium</td></tr><tr><td>PhysioNet platform managed by MIT Laboratory for Computational Physiology serves as primary distribu...</td><td>maintainer-002</td><td></td><td>PhysioNet / MIT Laboratory for Computational Physiology</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Retention Limit
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>retention-001</dd><dt>Name</dt><dd>Data retention and disposition</dd><dt>Description</dt><dd><div class="long-description">Data Transfer and Use Agreement specifies retention requirements. Upon termination or expiration of agreement (two years after start date, project completion, or ethics approval expiration), data shall be destroyed per provider instructions with written certification required within 30 days. Recipient may retain one copy to extent necessary to comply with records retention requirements under law, regulation, institutional policy, and for research integrity and verification purposes. Restrictions apply to archival copies as long as recipient holds data.
</div></dd><dt>Retention Details</dt><dd><ul class='formatted-list'><li>Two-year agreement term from start date</li><li>Data destruction required upon termination unless retention justified</li><li>One archival copy permitted for compliance and verification</li><li>Written certification of destruction required within 30 days</li><li>Ongoing restrictions apply to retained copies</li><li>Provider may unilaterally amend if federal sponsor requires</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sensitive Elements
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Sensitive Elements Present</th><th>Sensitivity Details</th></tr></thead><tbody><tr><td>Voice recordings contain personally identifiable information and are considered biometric identifier...</td><td>sensitive-001</td><td>True</td><td>Voice as biometric identifier, Raw audio waveforms, ... (+2 more)</td></tr><tr><td>Electronic health record (EHR) data accessed with participant consent for gold standard validation o...</td><td>sensitive-002</td><td>True</td><td>EHR medical information, Diagnoses and symptoms, ... (+2 more)</td></tr><tr><td>Demographic information and geographic data collected but de-identified for public release. State an...</td><td>sensitive-003</td><td>True</td><td>Demographic data (de-identified), Geographic information (state/province removed), ... (+2 more)</td></tr><tr><td>Dataset covered under Certificate of Confidentiality which must be asserted against compulsory legal...</td><td>sensitive-004</td><td>True</td><td>Certificate of Confidentiality coverage, Protection against compulsory legal demands, ... (+2 more)</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            External Resources
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>External Resources</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Primary distribution platform for public access dataset with registered access</td><td>https://physionet.org/content/b2ai-voice/</td><td>resource-001</td><td>PhysioNet Dataset Landing Page</td></tr><tr><td>Comprehensive project documentation and resources</td><td>https://docs.b2ai-voice.org</td><td>resource-002</td><td>Bridge2AI-Voice Project Documentation</td></tr><tr><td>Open source code repository including b2aiprep library and documentation dashboard</td><td>https://github.com/eipm/bridge2ai-docs</td><td>resource-003</td><td>Bridge2AI-Voice GitHub Repository</td></tr><tr><td>Federal grant information and project details</td><td>https://reporter.nih.gov/project-details/11376382</td><td>resource-004</td><td>NIH RePORTER Project Details</td></tr><tr><td>Alternative data repository platform</td><td>https://healthdatanexus.ai/content/b2ai-voice/1.0/</td><td>resource-005</td><td>Health Data Nexus</td></tr><tr><td>Additional dataset documentation and software releases</td><td>https://doi.org/10.5281/zenodo.13834653</td><td>resource-006</td><td>Zenodo Archive</td></tr><tr><td>Research resource for complex physiologic signals</td><td>https://physionet.org</td><td>resource-007</td><td>PhysioNet Platform</td></tr><tr><td>Publication describing multi-disorder voice protocol development through team science approach invol...</td><td>https://doi.org/10.21437/Interspeech.2024-1926</td><td>resource-008</td><td>Interspeech 2024 Protocol Publication</td></tr><tr><td>Contact for controlled access to raw audio data</td><td>mailto:DACO@b2ai-voice.org</td><td>resource-009</td><td>Data Access Compliance Office</td></tr><tr><td>Parent NIH Common Fund program supporting AI-ready biomedical datasets</td><td>https://bridge2ai.org</td><td>resource-010</td><td>Bridge2AI Program</td></tr><tr><td>Open source library for preprocessing raw audio and phenotype data</td><td>https://github.com/sensein/b2aiprep</td><td>resource-011</td><td>b2aiprep Software Library</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üöÄ</span>
                    <div>
                        <h2 class="section-title">Uses</h2>
                        <p class="section-description">What (other) tasks could the dataset be used for?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Tasks
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>ID</th><th>Response</th></tr></thead><tbody><tr><td>task-001</td><td>Enable development of AI/ML predictive models for screening, diagnosis, and treatment of voice disor...</td></tr><tr><td>task-002</td><td>Support machine learning models for neurological and neurodegenerative disorders including Alzheimer...</td></tr><tr><td>task-003</td><td>Develop AI algorithms for mood and psychiatric disorder detection including depression, schizophreni...</td></tr><tr><td>task-004</td><td>Create machine learning models for respiratory disorder screening and therapeutic monitoring using r...</td></tr><tr><td>task-005</td><td>Build AI models for pediatric voice and speech disorder detection including autism spectrum disorder...</td></tr><tr><td>task-006</td><td>Promote application of AI/ML for voice research through workforce development, curriculum creation, ...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Intended Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th></tr></thead><tbody><tr><td>Primary intended use is development and validation of AI/ML models for voice as a biomarker of healt...</td><td>use-001</td></tr><tr><td>Research into acoustic biomarkers and development of standards for voice data collection and analysi...</td><td>use-002</td></tr><tr><td>Training and education in voice AI research through workforce development initiatives, curriculum cr...</td><td>use-003</td></tr><tr><td>Multimodal health research combining voice data with EHR information, radiomics, genomics, and other...</td><td>use-004</td></tr><tr><td>Model dataset for ethical AI development in healthcare, demonstrating integration of bioethics guida...</td><td>use-005</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Discouraged Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td></td><td>discouraged-001</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>discouraged-002</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>discouraged-003</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>discouraged-004</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License And Use Terms
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>license-001</dd><dt>Name</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Description</dt><dd><div class="long-description">Public access dataset distributed through PhysioNet under Bridge2AI Voice Registered Access License. Only registered users who sign the specified Data Use Agreement (Bridge2AI Voice Registered Access Agreement) can access files. Data covered under Certificate of Confidentiality which must be asserted against compulsory legal demands. Raw audio data available through controlled access only via Data Access Compliance Office (DACO) requiring distinct application. Recipient must adhere to PhysioNet requirements managed by MIT Laboratory for Computational Physiology, supported by NIBIB under grant R01EB030362.
</div></dd><dt>License Terms</dt><dd><ul class='formatted-list'><li>Registered access required</li><li>Data Use Agreement signature mandatory</li><li>Use restricted to authorized persons listed in agreement</li><li>No sharing with third parties without prior written consent</li><li>Appropriate administrative, technical, physical safeguards required</li><li>Compliance with applicable laws, rules, regulations, professional standards</li><li>Public disclosure of results encouraged in open-access journals</li><li>Recognition of data source required in publications</li><li>Certificate of Confidentiality protections apply</li><li>Raw audio requires separate controlled access application</li><li>PhysioNet platform requirements apply</li><li>Two-year term from start date or project completion</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üì§</span>
                    <div>
                        <h2 class="section-title">Distribution</h2>
                        <p class="section-description">How will the dataset be distributed?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License
                            
                        </label>
                        <div class="item-value">Bridge2AI Voice Registered Access License</div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîÑ</span>
                    <div>
                        <h2 class="section-title">Maintenance</h2>
                        <p class="section-description">How will the dataset be maintained?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Updates
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>updates-001</dd><dt>Name</dt><dd>Versioned releases with ongoing data collection</dd><dt>Description</dt><dd><div class="long-description">Dataset updated with versioned releases as data collection progresses. Initial release v1.0 published January 17, 2025 with 12,523 recordings from 306 participants. v1.1 released January 17, 2025 adding MFCC features. v2.0.0 released April 16, 2025. v2.0.1 released August 18, 2025. Latest version available at https://doi.org/10.13026/37yb-1t42. Data collection ongoing through November 30, 2026. Version-specific documentation maintained. As of v1.1, only adult cohort data available; pediatric cohort data planned for future releases with additional privacy precautions.
</div></dd><dt>Frequency</dt><dd>Periodic versioned releases during data collection period (2022-2026)</dd><dt>Update Details</dt><dd><ul class='formatted-list'><li>v1.0 released January 17, 2025 - initial release with 306 participants, 12,523 recordings</li><li>v1.1 released January 17, 2025 - added MFCC features</li><li>v2.0.0 released April 16, 2025 - expanded participant cohort</li><li>v2.0.1 released August 18, 2025 - latest version</li><li>Ongoing data collection through November 30, 2026</li><li>Future releases planned with additional participants and pediatric cohort</li><li>Raw audio data access planned for future releases with additional security precautions</li><li>Version-specific documentation maintained</li><li>DOI for latest version vs version-specific DOIs</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üë•</span>
                    <div>
                        <h2 class="section-title">Human Subjects</h2>
                        <p class="section-description">Does the dataset relate to people?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Human Subject Research
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>hsr-001</dd><dt>Name</dt><dd>Bridge2AI-Voice Human Subjects Research</dd><dt>Description</dt><dd><div class="long-description">Data collection and sharing approved by University of South Florida Institutional Review Board. Participants provided written informed consent for data collection initiative and data sharing. Consent process includes authorization for voice data collection, access to medical information through EHR platforms for gold standard validation, and permission to share research data. Bioethics guidance integrated throughout study design and conduct. Ethics module develops new guidelines for consenting to voice data collection, voice data sharing, and utilization in context of voice AI technology. Project addresses ethical and trustworthy issues from voice data generation and AI/ML research through clinical adoption and downstream health decisions.
</div></dd><dt>Involves Human Subjects</dt><dd>True</dd><dt>IRB Approval</dt><dd><ul class='formatted-list'><li>University of South Florida Institutional Review Board approval</li></ul></dd><dt>Ethics Review Board</dt><dd><ul class='formatted-list'><li>University of South Florida Institutional Review Board</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-12-20 19:23:28 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>