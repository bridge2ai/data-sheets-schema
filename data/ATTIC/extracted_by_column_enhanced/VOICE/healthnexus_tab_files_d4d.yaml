# === YAML Fixing Applied ===
id: bridge2ai-voice-v1.0
name: Bridge2AI-Voice
title: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information v1.0"
description: >-
  Bridge2AI-Voice is a multi-site, ethically sourced dataset of human voice
  linked to clinical and questionnaire information to enable research on voice
  as a biomarker of health. Version 1.0 includes 12,523 recordings from 306
  participants across five North American sites, focusing on cohorts with known
  voice-relevant conditions (voice disorders, neurological/neurodegenerative
  disorders, mood/psychiatric disorders, and respiratory disorders). The initial
  release contains only low-risk, derived data (e.g., spectrograms and acoustic
  features) and de-identified phenotype tables; original audio waveforms and
  free-speech transcripts are not included. Data were collected via a standardized
  protocol using a custom tablet application, with preprocessing that included
  resampling to 16 kHz with a Butterworth anti-aliasing filter, STFT-based
  spectrogram generation, extraction of acoustic and phonetic/prosodic features
  (OpenSMILE, Parselmouth/Praat), and automatic transcription using OpenAI
  Whisper Large. Documentation: "https://docs.b2ai-voice.org."
doi: "doi:10.57764/qb6h-em84"
page: "https://docs.b2ai-voice.org"
issued: "2024-11-27"
version: "1.0"
keywords:
  - voice
  - bridge2ai
  - audio
  - VOICE
creators:
  - name: Alistair Johnson
  - name: Jean-Christophe Bélisle-Pipon
  - name: David Dorr
  - name: Satrajit Ghosh
  - name: Philip Payne
  - name: Maria Powell
  - name: Anaïs Rameau
  - name: Vardit Ravitsky
  - name: Alexandros Sigaras
  - name: Olivier Elemento
  - name: Yael Bensoussan
funders:
  - grantor:
      name: National Institutes of Health
    grant:
      name: "Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database to understand disease like never before"
      grant_number: 3OT2OD032720-01S1
purposes:
  - response: >-
      Create an ethically sourced, diverse, multi-institutional dataset of voice
      linked to clinical and questionnaire data to enable AI research on voice as
      a biomarker of health and support clinically meaningful insights.
tasks:
  - response: >-
      Research and development of AI/ML methods for analyzing voice-derived
      features and their associations with health conditions; exploratory and
      hypothesis-driven studies on voice as a biomarker.
addressing_gaps:
  - response: >-
      Addresses the lack of large, high-quality, standardized, and demographically
      diverse voice datasets linked to clinical and questionnaire data across
      multiple institutions.
instances:
  - representation: Voice-derived recordings (spectrograms/features) per recording
    instance_type: Audio-derived instance
    data_type: Derived features (spectrograms, acoustic, phonetic/prosodic); no raw audio waveforms in v1.0
    counts: 12523
    sampling_strategies:
      - is_sample:
          - "yes"
        is_random:
          - "no"
        source_data:
          - Patients presenting at specialty clinics across five North American sites
        is_representative:
          - "no"
        why_not_representative:
          - Participants were selected based on membership in predefined disease cohorts
        strategies:
          - Targeted enrollment by predefined disease categories
    missing_information:
      - missing:
          - Original audio waveforms
        why_missing:
          - Omitted in initial low-risk release; planned for future releases with additional safeguards
      - missing:
          - Transcripts of free speech audio
        why_missing:
          - Removed during de-identification
  - representation: Participant-level phenotype records
    instance_type: Participant
    data_type: Demographics, clinical, and validated questionnaire responses
    counts: 306
    sampling_strategies:
      - is_sample:
          - "yes"
        is_random:
          - "no"
        source_data:
          - Specialty clinics at five North American sites
        is_representative:
          - "no"
        why_not_representative:
          - Cohort-based selection for voice-relevant conditions
relationships:
  - description:
      - Recordings are linked to participants and sessions via participant_id and session_id; tasks identified by task_name
subpopulations:
  - identification:
      - Adult cohort (v1.0); disease categories include voice disorders, neurological/neurodegenerative disorders, mood/psychiatric disorders, respiratory disorders
    distribution:
      - Participants selected based on membership in predefined disease cohorts
is_deidentified:
  description:
    - HIPAA Safe Harbor identifiers removed (e.g., names, contact details, precise dates, etc.)
    - State and province removed; country retained
    - Free speech transcripts removed; original audio waveforms omitted in v1.0
sensitive_elements:
  - description:
      - Contains de-identified health-related information (demographics, clinical variables, questionnaire responses)
confidential_elements:
  - description:
      - De-identified clinical and questionnaire data; identifiable communications removed
acquisition_methods:
  - description:
      - Standardized protocol with voice tasks (e.g., sustained vowel), demographics, health and targeted questionnaires
      - Custom tablet application; headset used when possible
      - Data exported from REDCap via open-source tooling
    was_directly_observed: "yes"
    was_reported_by_subjects: "yes"
    was_inferred_derived: "yes"
collection_mechanisms:
  - description:
      - Custom tablet app and headset for data capture; REDCap used for data entry/export
data_collectors:
  - description:
      - Project investigators at five North American sites; patients presenting at specialty clinics were screened and consented
ethical_reviews:
  - description:
      - Data collection and sharing approved by the University of South Florida IRB; submitted for review to the University of Toronto Research Ethics Board
preprocessing_strategies:
  - description:
      - Raw audio converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter
      - Spectrograms computed via short-time FFT (25 ms window, 10 ms hop, 512-point FFT)
      - Acoustic features extracted with OpenSMILE
      - Phonetic and prosodic features computed with Parselmouth and Praat
      - Transcriptions generated with OpenAI Whisper Large model
    used_software:
      - name: OpenSMILE
      - name: Parselmouth
      - name: Praat
      - name: torchaudio
      - name: OpenAI Whisper Large
      - name: b2aiprep
        url: "https://github.com/sensein/b2aiprep"
cleaning_strategies:
  - description:
      - HIPAA Safe Harbor de-identification (removal of identifiers and fine-grained dates)
      - Removal of free speech transcripts
      - Omission of original audio waveforms from initial release
labeling_strategies:
  - description:
      - Automatic transcription using OpenAI Whisper Large (note: free speech transcripts are not included in v1.0)
raw_sources:
  - description:
      - Original audio waveforms collected; not distributed in v1.0. Future releases aim to include voice data with additional precautions.
distribution_formats:
  - description:
      - Parquet
      - TSV
      - JSON
distribution_dates:
  - description:
      - "2024-11-27"
license_and_use_terms:
  description:
    - Bridge2AI Voice Registered Access License
    - Bridge2AI Voice Registered Access Agreement (DUA)
    - Access policy: Only credentialed users who sign the DUA can access the files
    - Required training: "TCPS 2: CORE 2022"
maintainers:
  - description:
      - Health Data Nexus
      - Temerty Centre for AI Research and Education in Medicine
updates:
  description:
    - Future releases aim to include voice data with additional precautions to ensure data security
version_access:
  description:
    - Latest version DOI: "https://doi.org/10.57764/3sg0-7440"
is_tabular: mixed (Parquet dense arrays and tabular TSV/JSON)
subsets:
  - id: spectrograms-parquet
    name: spectrograms.parquet
    title: Spectrograms (derived from voice waveforms)
    description: >-
      Parquet dataset containing time-frequency spectrograms derived from voice
      recordings; includes participant_id, session_id, task_name, and a 513xN
      spectrogram matrix.
    path: spectrograms.parquet
    media_type: application/x-parquet
  - id: phenotype-tsv
    name: phenotype.tsv
    title: Phenotype table
    description: >-
      Tab-delimited file with one row per participant containing demographics,
      acoustic confounders, and validated questionnaire responses.
    path: phenotype.tsv
    media_type: text/tab-separated-values
  - id: phenotype-json
    name: phenotype.json
    title: Phenotype data dictionary
    description: >-
      JSON data dictionary describing columns in phenotype.tsv; includes a one-sentence
      description for each field.
    path: phenotype.json
    format: JSON
    media_type: application/json
  - id: static-features-tsv
    name: static_features.tsv
    title: Static audio-derived features
    description: >-
      Tab-delimited file with one row per recording containing features derived
      from OpenSMILE, Praat/Parselmouth, and torchaudio.
    path: static_features.tsv
    media_type: text/tab-separated-values
  - id: static-features-json
    name: static_features.json
    title: Static features data dictionary
    description: >-
      JSON data dictionary describing features in static_features.tsv; includes
      a description for each feature.
    path: static_features.json
    format: JSON
    media_type: application/json