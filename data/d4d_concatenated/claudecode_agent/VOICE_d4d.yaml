# D4D Datasheet for VOICE Dataset
# Generation Method: Claude Code Agent (Comprehensive)
# Generated: 2025-12-06
# Source: data/preprocessed/concatenated/VOICE_preprocessed.txt (89K, 9 source files)
# Schema: src/data_sheets_schema/schema/data_sheets_schema_all.yaml
# Model: claude-sonnet-4-5-20250929

id: bridge2ai-voice-dataset
name: Bridge2AI-Voice
title: Bridge2AI-Voice - An ethically-sourced, diverse voice dataset linked to health information
description: >
  The Bridge2AI-Voice dataset contains comprehensive voice, speech, and language data linked to health information,
  collected through a multi-institutional initiative funded by NIH's Bridge to Artificial Intelligence program.
  The dataset includes samples from conventional acoustic tasks such as respiratory sounds, cough sounds, and free
  speech prompts. Participants perform speaking tasks and complete self-reported demographic and medical history
  questionnaires, as well as disease-specific validated questionnaires. The project aims to integrate voice as a
  biomarker of health in clinical care by generating a substantial, ethically sourced, and diverse voice database
  linked to multimodal health biomarkers (EHR, radiomics, genomics) to fuel voice AI research and build predictive
  models for screening, diagnosis, and treatment across a broad range of diseases. Data collection is conducted via
  smartphone application linked to electronic health records, supported by federated learning technology to protect
  data privacy. Version 1.1 provides 12,523 recordings for 306 participants collected across five sites in North
  America. The dataset is distributed through PhysioNet and Health Data Nexus under a registered access license.
page: https://physionet.org/content/b2ai-voice/
version: "1.1"
license: Bridge2AI Voice Registered Access License
language: en
keywords:
  - voice
  - speech
  - bridge2ai
  - voice biomarker
  - acoustic biomarker
  - AI
  - machine learning
  - health
  - disease screening
  - voice disorders
  - neurological disorders
  - mood disorders
  - respiratory disorders
  - pediatric
  - PhysioNet
  - federated learning
  - ethical AI
  - FAIR data
  - CARE principles
  - multimodal biomarkers

# =============================================================================
# MOTIVATION
# =============================================================================
purposes:
  - description: >
      To integrate the use of voice as a biomarker of health in clinical care by generating a substantial
      multi-institutional, ethically sourced, and diverse voice database linked to multimodal health biomarkers
      (EHR, radiomics, genomics) to fuel voice AI research and build predictive models to assist in screening,
      diagnosis, and treatment of a broad range of diseases.
    purpose_details:
      - Voice is a promising biomarker as it is simple to collect, cost-effective, and has broad clinical utility
      - Recent AI advances enable extraction of prognostically useful information from voice data
      - Multi-institutional collaboration ensures diverse, representative datasets
      - Ethical framework ensures trustworthy AI development from data generation to clinical adoption

  - description: >
      To develop new standards of acoustic and voice data collection and analysis for voice AI research,
      introducing the field of acoustic biomarkers through standardized protocols and quality measures.
    purpose_details:
      - Standardized voice data collection protocols across sites
      - Acoustic quality standardization and calibration
      - Integration of acoustic amplifiers and quality control tools
      - Development of best practices for voice AI research

  - description: >
      To create software and cloud infrastructure for automated voice data collection through smartphone
      application that allows non-invasive, user-friendly, high quality voice data collection while minimizing
      human manipulation.
    purpose_details:
      - Custom tablet/smartphone application for voice recording
      - Integrated acoustic quality standardization
      - Federated learning technology for privacy-preserving multi-institutional analysis
      - Cloud infrastructure supporting scalable data collection

tasks:
  - description: >
      Enable AI/ML research for disease screening, diagnosis, and treatment monitoring across five disease
      categories: (1) Vocal Pathologies (laryngeal cancers, vocal fold paralysis, benign laryngeal lesions),
      (2) Neurological and Neurodegenerative Disorders (Alzheimer's, Parkinson's, Stroke, ALS), (3) Mood and
      Psychiatric Disorders (Depression, Schizophrenia, Bipolar Disorders), (4) Respiratory disorders (Pneumonia,
      COPD, Heart Failure, OSA), and (5) Pediatric diseases (Autism, Speech Delay).
    task_type: AI/ML model development
    target_populations:
      - Adults with voice disorders
      - Adults with neurological/neurodegenerative conditions
      - Adults with mood and psychiatric disorders
      - Adults with respiratory disorders
      - Pediatric patients (future releases)

  - description: >
      Discovery and validation of novel acoustic biomarkers associated with health conditions, expanding beyond
      currently recognized voice-disease associations to identify new clinical applications.
    task_type: Biomarker discovery
    target_applications:
      - Voice changes in depression (decreased fundamental frequency, monotonous speech)
      - Voice changes in anxiety (increased fundamental frequency)
      - Voice/speech changes in acute stroke (dysarthria, aphasia)
      - Voice changes in Parkinson's and ALS (slowed, low frequency, monotonous speech, vocal tremor)
      - Respiratory sounds for disease screening (croup, pneumonia, COPD)

  - description: >
      Development of clinical decision support tools integrating voice biomarkers into healthcare workflows
      for screening, diagnosis, and therapeutic monitoring.
    task_type: Clinical application
    target_applications:
      - Point-of-care voice screening tools
      - Remote patient monitoring using voice
      - EHR-integrated voice biomarker dashboards
      - Longitudinal disease progression tracking

  - description: >
      Multi-modal biomarker research integrating voice with EHR, radiomics, genomics, and other data sources
      to build comprehensive predictive models.
    task_type: Multi-modal integration
    target_applications:
      - Voice + EHR integration for diagnosis validation
      - Voice + genomics for personalized medicine
      - Voice + radiomics for disease staging
      - Federated learning across data modalities

addressing_gaps:
  - description: >
      Address the pressing need for large, high quality, multi-institutional and diverse voice databases linked
      to other health biomarkers from various data modalities (demographics, imaging, genomics, risk factors, etc.)
      to fuel voice AI research and answer tangible clinical questions through multi-institutional collaborations
      between voice experts and AI engineers, supported by bioethicists and social scientists to ensure ethically
      sourced voice databases representing our populations.
    gap_type: Dataset availability
    existing_limitations:
      - Previous literature used small datasets with limited demographic diversity reporting
      - Lack of standardized data collection protocols precluding meta-analysis
      - Possible confounders not controlled across studies
      - Limited external validity and clinical usability
      - Sparse pediatric voice/speech analysis data
      - Ethical concerns in data acquisition

  - description: >
      Address ethical concerns about patient privacy protection, fair representation of populations, and clinical
      accuracy as voice AI gains attention from multi-nationals (Google, Amazon, Mozilla, Apple). Influence and
      guide the world of voice AI by ensuring patient protection through ethical and fairness principles.
    gap_type: Ethical framework
    existing_limitations:
      - Industry development lacks comprehensive ethical oversight
      - Privacy protection inadequate in commercial voice AI
      - Population representation biased toward majority groups
      - Clinical validation often insufficient
      - Lack of standards for consent and data governance

  - description: >
      Build bridges between the medical voice research world, acoustic engineers, and the AI/ML community to
      promote integration of tangible clinical applications for voice AI algorithms.
    gap_type: Interdisciplinary collaboration
    existing_limitations:
      - Siloed research communities
      - Limited clinical translation of voice AI research
      - Gap between acoustic engineering and medical applications
      - Insufficient cross-training of researchers

creators:
  - description: >
      Dr. Yael Emilie Bensoussan (Assistant Professor, University of South Florida, Department of Otolaryngology,
      Tampa, FL, Congressional District 15, Principal Investigator/Contact PI)
    creator_role: Principal Investigator
    affiliation: University of South Florida

  - description: >
      Dr. Jean-Christophe Bélisle-Pipon (Co-Investigator, Bioethics expert)
    creator_role: Co-Investigator
    affiliation: University of South Florida

  - description: >
      Dr. David A. Dorr (Co-Investigator, EHR integration and clinical informatics)
    creator_role: Co-Investigator
    affiliation: Oregon Health & Science University

  - description: >
      Dr. Satrajit Sujit Ghosh (Co-Investigator, Neuroimaging and voice analysis, MIT)
    creator_role: Co-Investigator
    affiliation: Massachusetts Institute of Technology

  - description: >
      Dr. Philip R.O. Payne (Co-Investigator, Biomedical informatics)
    creator_role: Co-Investigator
    affiliation: Washington University in St. Louis

  - description: >
      Dr. Maria Ellen Powell (Co-Investigator, Ethics module)
    creator_role: Co-Investigator
    affiliation: University of South Florida

  - description: >
      Dr. Anais Rameau (Co-Investigator, Voice disorders specialist, Weill Cornell Medicine)
    creator_role: Co-Investigator
    affiliation: Weill Cornell Medicine

  - description: >
      Dr. Vardit Ravitsky (Co-Investigator, Bioethics)
    creator_role: Co-Investigator
    affiliation: Université de Montréal

  - description: >
      Dr. Alexandros Sigaras (Co-Investigator, Technical lead, Weill Cornell Medicine)
    creator_role: Co-Investigator
    affiliation: Weill Cornell Medicine

  - description: >
      Dr. Olivier Elemento (Co-Investigator, Computational biology, Weill Cornell Medicine)
    creator_role: Co-Investigator
    affiliation: Weill Cornell Medicine

  - description: >
      Dr. Alistair Johnson (Co-Investigator, Clinical data management, The Hospital for Sick Children)
    creator_role: Co-Investigator
    affiliation: The Hospital for Sick Children

  - description: >
      Bridge2AI-Voice Consortium including University of South Florida (lead institution), Massachusetts Institute
      of Technology, Weill Cornell Medicine, Oregon Health & Science University, Washington University in St. Louis,
      and other participating North American clinical sites.
    creator_role: Consortium

funders:
  - description: >
      NIH Office of the Director, Bridge to Artificial Intelligence (Bridge2AI) program, grant 3OT2OD032720-01S1
      (originally 3OT2OD032720-01S3), supporting creation of ethically sourced, AI-ready biomedical datasets.
      Project title: "Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic
      database to understand disease like never before."
    funder_name: National Institutes of Health (NIH)
    grant_info:
      - Grant number 3OT2OD032720-01S1 (current), 3OT2OD032720-01S3 (referenced)
      - Administering IC: NIH Office of the Director
      - Opportunity Number: OTA-21-008
      - Study Section: Data Coordination, Mapping, and Modeling [DCMM]
      - Fiscal Year 2025 funding: $4,660,942 (total), $4,072,321 (direct costs), $588,621 (indirect costs)
      - Award Notice Date: 05-September-2025
      - Project Start Date: 01-September-2022
      - Project End Date: 30-November-2026
      - Budget Start Date: 15-September-2025
      - Budget End Date: 30-November-2026
      - No Cost Extension: N
      - Assistance Listing Number: 93.310
      - DUNS Number: 069687242
      - UEI: NKAZLXLL7Z91

  - description: >
      Additional infrastructure support from National Institute of Biomedical Imaging and Bioengineering (NIBIB)
      grant R01EB030362 for PhysioNet data distribution platform managed by MIT Laboratory for Computational
      Physiology.
    funder_name: NIBIB
    grant_info:
      - Grant number R01EB030362
      - Supports PhysioNet infrastructure
      - MIT Laboratory for Computational Physiology

# =============================================================================
# COMPOSITION
# =============================================================================
instances:
  - description: >
      Voice and speech audio recordings from 306 participants across five clinical sites in North America,
      totaling 12,523 recordings. Recordings include sustained phonation of vowel sounds (e.g., /e/),
      respiratory sounds, cough sounds, and free speech prompts. Raw audio preprocessed to monaural 16 kHz
      format with Butterworth anti-aliasing filter. Original audio waveforms omitted from v1.0/v1.1 public
      releases for privacy protection.
    instance_type: Audio recordings
    count: 12523
    format: Derived features (spectrograms, MFCCs) in Parquet format

  - description: >
      Spectrograms computed using short-time Fast Fourier Transform (FFT) with 25ms window size, 10ms hop length,
      and 512-point FFT, resulting in 513xN dimension time-frequency representations where N is proportional to
      recording length.
    instance_type: Spectrograms
    count: 12523
    format: Parquet (spectrograms.parquet)

  - description: >
      Mel-frequency cepstral coefficients (MFCCs) with 60 coefficients extracted from spectrograms, resulting
      in 60xN dimension arrays. Added in version 1.1 release.
    instance_type: MFCCs
    count: 12523
    format: Parquet (mfcc.parquet)

  - description: >
      Acoustic features extracted using OpenSMILE (Speech and Music Interpretation by Large-space Extraction),
      capturing temporal dynamics and acoustic characteristics of voice recordings.
    instance_type: Acoustic features
    format: TSV (static_features.tsv)

  - description: >
      Phonetic and prosodic features computed using Parselmouth (Python interface to Praat), providing measures
      of fundamental frequency (f0), formants, and voice quality parameters.
    instance_type: Prosodic features
    format: TSV (static_features.tsv)

  - description: >
      Demographic data from 306 participants including de-identified geographic information (country retained,
      state/province removed), age (years only), and other HIPAA Safe Harbor compliant variables.
    instance_type: Demographics
    count: 306
    format: TSV (phenotype.tsv)

  - description: >
      Self-reported medical history questionnaires covering health status, disease history, medication use,
      and lifestyle factors relevant to voice production.
    instance_type: Medical history
    count: 306
    format: TSV (phenotype.tsv)

  - description: >
      Disease-specific validated questionnaires tailored to participant's disease cohort membership (voice
      disorders, neurological, mood, respiratory, pediatric).
    instance_type: Clinical questionnaires
    format: TSV (phenotype.tsv)

  - description: >
      Targeted questionnaires on known confounders for voice including smoking status, vocal use patterns,
      environmental factors, and acute conditions affecting voice.
    instance_type: Voice confounders
    format: TSV (phenotype.tsv)

  - description: >
      Electronic health record (EHR) data accessed with participant consent for gold standard validation of
      diagnoses and symptoms, linked through institutional EHR platforms.
    instance_type: EHR data
    access: With participant consent

  - description: >
      Automated transcriptions generated using OpenAI's Whisper Large model. Free speech transcripts removed
      from public release for privacy protection (only task-based transcriptions for non-identifying prompts).
    instance_type: Transcriptions
    privacy_note: Free speech transcripts removed for privacy

subsets:
  - id: voice:spectrograms
    name: Spectrograms
    description: >
      Parquet file (spectrograms.parquet) containing time-frequency representations with participant_id,
      session_id, task_name, and 513xN dimension spectrogram arrays. One element per recording.
    format: Parquet
    file_size: Large (dense array data)

  - id: voice:mfcc
    name: Mel-frequency Cepstral Coefficients
    description: >
      Parquet file (mfcc.parquet) containing 60xN dimension MFCC arrays derived from spectrograms. Added in
      version 1.1 release (January 17, 2025).
    format: Parquet
    version_added: "1.1"

  - id: voice:phenotype
    name: Phenotype Data
    description: >
      Tab-delimited file (phenotype.tsv) with one row per unique participant (306 rows), containing demographics,
      acoustic confounders, and responses to validated questionnaires. Each column represents a question asked
      during clinical data collection within the custom data collection app.
    format: TSV
    data_dictionary: phenotype.json
    rows: 306

  - id: voice:static-features
    name: Static Acoustic Features
    description: >
      Tab-delimited file (static_features.tsv) containing features derived from raw audio using OpenSMILE,
      Praat, parselmouth, and torchaudio libraries. One row per unique recording (12,523 rows). Features include
      temporal dynamics, acoustic characteristics, fundamental frequency, formants, and voice quality measures.
    format: TSV
    data_dictionary: static_features.json
    rows: 12523

  - id: voice:cohort-voice-disorders
    name: Voice Disorders Cohort
    description: >
      Participants with vocal pathologies including laryngeal cancers, vocal fold paralysis, and benign
      laryngeal lesions. Voice disorders are the most studied pathologies linked to vocal changes due to
      effects on vocal fold shape, mass, density, and tension.
    cohort: Voice disorders

  - id: voice:cohort-neuro
    name: Neurological Disorders Cohort
    description: >
      Participants with neurological and neurodegenerative conditions including Alzheimer's disease, Parkinson's
      disease, stroke, and ALS. Voice and speech alterations include dysarthria, aphasia, slowed speech, low
      frequency speech, monotonous speech, and vocal tremor.
    cohort: Neurological disorders

  - id: voice:cohort-mood
    name: Mood and Psychiatric Disorders Cohort
    description: >
      Participants with mood and psychiatric conditions including depression, schizophrenia, and bipolar disorders.
      Voice changes associated with depression include decreased fundamental frequency and monotonous speech, while
      anxiety disorders show increased fundamental frequency.
    cohort: Mood and psychiatric disorders

  - id: voice:cohort-respiratory
    name: Respiratory Disorders Cohort
    description: >
      Participants with respiratory conditions including pneumonia, COPD, heart failure, and obstructive sleep
      apnea (OSA). Respiratory sounds (breath, cough, voice) have long been used for diagnostic purposes.
    cohort: Respiratory disorders

  - id: voice:cohort-pediatric
    name: Pediatric Cohort
    description: >
      Pediatric participants with conditions including autism and speech delay. Not included in version 1.1
      release; planned for future versions. Pediatric voice/speech analysis literature is sparser due to
      ethical concerns and data acquisition challenges.
    cohort: Pediatric
    status: Not included in v1.1 (adult cohort only)

subpopulations:
  - description: >
      Multi-institutional participants recruited from five clinical sites across North America to ensure
      geographic and demographic diversity. Sites include University of South Florida (lead), MIT,
      Weill Cornell Medicine, Oregon Health & Science University, and other participating institutions.
    subpopulation_type: Geographic diversity

  - description: >
      Disease cohort-based sampling targeting five categories with known voice manifestations: (1) Voice disorders,
      (2) Neurological/neurodegenerative disorders, (3) Mood/psychiatric disorders, (4) Respiratory disorders,
      (5) Pediatric conditions.
    subpopulation_type: Disease cohort stratification

  - description: >
      Intentional recruitment of diverse participants to address historical underrepresentation in voice AI
      research, following fairness, equity, diversity, and inclusion (DEI) principles integrated into project
      design.
    subpopulation_type: DEI-focused recruitment

sampling_strategies:
  - description: >
      Participants selected based on membership to five predetermined disease cohort groups identified from
      literature review as having well-recognized voice manifestations and unmet clinical needs. Patients
      presenting at specialty clinics screened for inclusion/exclusion criteria prior to enrollment.
    sampling_method: Disease cohort-based selection
    rationale: Target conditions with established voice-disease associations and clinical unmet needs

  - description: >
      Multi-institutional recruitment across five sites in North America to ensure geographic diversity,
      site-specific clinical expertise, and representation of different healthcare systems and patient populations.
    sampling_method: Multi-site geographic sampling
    rationale: Generalizability and reduced site-specific bias

  - description: >
      Intentional focus on recruiting diverse participants historically underrepresented in voice AI research,
      addressing fairness and equity principles integrated throughout the project's DEI module.
    sampling_method: Diversity-targeted recruitment
    rationale: Fairness, representativeness, and reduction of algorithmic bias

  - description: >
      Patients screened at specialty clinics based on predetermined inclusion/exclusion criteria developed
      by project investigators, ensuring appropriate clinical phenotyping and data quality.
    sampling_method: Clinician-guided screening
    rationale: Clinical validity and gold standard diagnosis

# =============================================================================
# COLLECTION
# =============================================================================
acquisition_methods:
  - description: >
      Voice recordings collected using custom tablet application with headsets at clinical sites during scheduled
      study visits. Participants perform standardized voice tasks including sustained phonation of vowel sounds
      (e.g., /e/), respiratory sounds, cough sounds, and free speech prompts following structured protocols.
    was_directly_observed: true
    collection_mode: In-person at clinical sites
    equipment:
      - Custom tablet application (REDCap-based v3.20.0)
      - Headsets for audio capture with acoustic quality control
      - Acoustic amplifiers for standardization
      - Smartphones (planned for future scalable collection)

  - description: >
      Structured questionnaires administered via custom data collection application on tablets, capturing
      demographic information, medical history, voice confounders, and disease-specific validated instruments.
    was_directly_observed: true
    collection_mode: Self-report via tablet application
    instruments:
      - Demographic questionnaires
      - Medical history questionnaires
      - Voice confounder questionnaires (smoking, vocal use, environment)
      - Disease-specific validated questionnaires

  - description: >
      Electronic health record (EHR) data accessed through institutional platforms with participant consent for
      gold standard validation of diagnoses and symptoms. EHR linkage enables verification of clinical phenotypes
      and longitudinal health information.
    was_directly_observed: false
    collection_mode: EHR data extraction with consent
    data_sources:
      - Institutional EHR systems at participating sites
      - Diagnostic codes and clinical notes
      - Laboratory values and imaging reports
      - Medication histories

collection_mechanisms:
  - description: >
      Hardware infrastructure including tablets with integrated headsets for standardized audio capture, acoustic
      amplifiers, and quality control monitoring tools.
    mechanism_type: Hardware
    components:
      - Tablets for application deployment
      - Headsets with acoustic specifications
      - Acoustic amplifiers for standardization
      - Quality monitoring sensors

  - description: >
      Software infrastructure including REDCap electronic data capture framework (v3.20.0), custom voice recording
      application, automated preprocessing pipelines (b2aiprep v0.21.0), and federated learning platform for
      privacy-preserving multi-institutional analysis.
    mechanism_type: Software
    components:
      - REDCap v3.20.0 (doi:10.5281/zenodo.14148755)
      - Custom tablet/smartphone application
      - b2aiprep preprocessing library (v0.21.0, https://github.com/sensein/b2aiprep)
      - Federated learning infrastructure
      - Cloud storage and compute infrastructure
      - OpenSMILE feature extraction
      - Praat/parselmouth prosodic analysis
      - OpenAI Whisper transcription
      - torchaudio audio processing

  - description: >
      EHR integration platforms enabling secure linkage between voice data and clinical records across participating
      institutions while maintaining patient privacy and HIPAA compliance.
    mechanism_type: Data integration
    components:
      - Institutional EHR APIs
      - Secure data linkage protocols
      - De-identification pipelines
      - Federated query systems

data_collectors:
  - description: >
      Clinical research coordinators and trained study personnel at participating sites responsible for participant
      enrollment, consent administration, and supervised data collection sessions.
    collector_type: Human - Clinical research staff
    sites:
      - University of South Florida
      - Massachusetts Institute of Technology
      - Weill Cornell Medicine
      - Oregon Health & Science University
      - Other participating North American sites

  - description: >
      Automated computational systems for audio preprocessing, feature extraction, transcription, and quality control.
    collector_type: Automated - Computational pipelines
    systems:
      - b2aiprep preprocessing library
      - OpenSMILE acoustic feature extraction
      - Praat/parselmouth prosodic analysis
      - OpenAI Whisper Large model transcription
      - torchaudio audio processing
      - Custom quality control algorithms

collection_timeframes:
  - description: >
      Project initiated September 1, 2022 with planned completion November 30, 2026. Ongoing data collection
      with periodic versioned releases.
    start_date: "2022-09-01"
    end_date: "2026-11-30"
    collection_status: Ongoing

  - description: >
      Version release timeline: v1.0 released January 2024 (initial release with 306 participants, 12,523 recordings),
      v1.1 released January 17, 2025 (added MFCCs), v2.0.0 planned April 16, 2025, v2.0.1 planned August 18, 2025.
    release_schedule:
      - v1.0: January 2024
      - v1.1: January 17, 2025
      - v2.0.0: April 16, 2025 (planned)
      - v2.0.1: August 18, 2025 (planned)

  - description: >
      Most participants complete data collection in a single session. Subset of participants require multiple
      sessions to complete protocol, resulting in multiple sessions per participant for some individuals.
    session_structure: Single or multi-session per participant

# =============================================================================
# PREPROCESSING, CLEANING, AND LABELING
# =============================================================================
preprocessing_strategies:
  - description: >
      Raw audio preprocessing pipeline standardizes all recordings to monaural (single-channel) format and
      resamples to 16 kHz sampling rate with Butterworth anti-aliasing filter applied to prevent frequency
      aliasing artifacts.
    preprocessing_type: Audio standardization
    methods:
      - Conversion to monaural (mono) audio
      - Resampling to 16 kHz
      - Butterworth anti-aliasing filter

  - description: >
      Spectrogram extraction using short-time Fast Fourier Transform (FFT) with 25ms window size, 10ms hop length,
      and 512-point FFT, producing 513xN dimension time-frequency representations stored in Parquet format for
      efficient access.
    preprocessing_type: Time-frequency transformation
    methods:
      - Short-time FFT with 25ms window
      - 10ms hop length
      - 512-point FFT
      - Power spectrum representation
      - Output: 513xN dimension arrays

  - description: >
      Mel-frequency cepstral coefficient (MFCC) extraction with 60 coefficients computed from spectrograms,
      capturing perceptually-relevant acoustic features commonly used in speech and voice analysis. Added in v1.1.
    preprocessing_type: Perceptual feature extraction
    methods:
      - 60 MFCC coefficients
      - Derived from spectrograms
      - Output: 60xN dimension arrays
      - Added in version 1.1

  - description: >
      Acoustic feature extraction using OpenSMILE (Speech and Music Interpretation by Large-space Extraction)
      toolkit to compute extensive set of low-level descriptors (LLDs), statistical functionals, and temporal
      features capturing voice dynamics.
    preprocessing_type: Acoustic feature extraction
    tools:
      - OpenSMILE (Eyben et al. 2010)
      - LLD computation
      - Statistical functionals
      - Temporal dynamics

  - description: >
      Phonetic and prosodic feature computation using Parselmouth (Python interface to Praat) for fundamental
      frequency (f0) estimation, formant extraction (F1, F2, F3), voice quality parameters (jitter, shimmer,
      harmonics-to-noise ratio), and intensity measures.
    preprocessing_type: Prosodic analysis
    tools:
      - Parselmouth (Jadoul et al. 2018)
      - Praat phonetic analysis
      - Fundamental frequency (f0)
      - Formants (F1, F2, F3)
      - Voice quality (jitter, shimmer, HNR)

  - description: >
      Automated speech transcription using OpenAI's Whisper Large model for accurate transcription of voice
      recordings. Free speech transcripts removed from public release for privacy; only task-based transcriptions
      for non-identifying prompts retained.
    preprocessing_type: Automated transcription
    tools:
      - OpenAI Whisper Large model
      - Automatic speech recognition (ASR)
    privacy_measures:
      - Free speech transcripts removed
      - Only non-identifying task transcriptions retained

  - description: >
      REDCap data export and conversion using open-source b2aiprep library (v0.21.0) for standardized extraction,
      transformation, and formatting of clinical questionnaire data and voice recordings into analysis-ready formats.
    preprocessing_type: Data export and formatting
    tools:
      - b2aiprep v0.21.0 (https://github.com/sensein/b2aiprep)
      - REDCap API integration
      - Parquet file generation
      - TSV/JSON data dictionaries

cleaning_strategies:
  - description: >
      HIPAA Safe Harbor de-identification method applied systematically to remove all 18 categories of identifiers
      specified in 45 CFR §164.514(b)(2), ensuring dataset meets regulatory requirements for de-identified health
      information.
    cleaning_type: HIPAA Safe Harbor de-identification
    identifiers_removed:
      - Names
      - Geographic subdivisions smaller than state (state/province removed, country retained)
      - All elements of dates except year (dates at resolution finer than years removed)
      - Telephone numbers
      - Fax numbers
      - Email addresses
      - Social Security numbers
      - Medical record numbers
      - Health plan beneficiary numbers
      - Account numbers
      - Certificate/license numbers
      - Vehicle identifiers and serial numbers
      - Device identifiers and serial numbers
      - Web Universal Resource Locators (URLs)
      - Internet Protocol (IP) addresses
      - Biometric identifiers (including finger and voice prints)
      - Full-face photographs and comparable images
      - Any other unique identifying number, characteristic, or code

  - description: >
      Raw audio waveforms excluded from public releases v1.0 and v1.1 to protect participant privacy and prevent
      potential biometric identification. Only derived features (spectrograms, MFCCs, acoustic features) provided
      in public release. Raw audio available through controlled access by contacting DACO@b2ai-voice.org.
    cleaning_type: Privacy-preserving feature extraction
    privacy_measures:
      - Raw audio omitted from v1.0 and v1.1
      - Only derived features publicly released
      - Raw audio available via controlled access (DACO@b2ai-voice.org)
      - Future releases will include additional privacy protections for raw audio

  - description: >
      Free speech transcripts removed from all public releases to prevent disclosure of potentially identifying
      or sensitive information contained in participant utterances. Only transcriptions of standardized task-based
      prompts retained where content is non-identifying.
    cleaning_type: Transcript privacy protection
    privacy_measures:
      - Free speech transcripts removed
      - Task-based transcriptions retained (non-identifying prompts only)
      - Reduces re-identification risk from unique speech patterns

  - description: >
      Data quality control procedures including acoustic quality validation, outlier detection, completeness checks,
      and consistency verification across linked data sources (voice, questionnaires, EHR).
    cleaning_type: Quality assurance
    quality_measures:
      - Acoustic quality thresholds
      - Outlier detection and flagging
      - Completeness validation
      - Cross-source consistency checks
      - Session-level metadata verification

# =============================================================================
# USES
# =============================================================================
existing_uses:
  - description: >
      Dataset publicly released through PhysioNet and Health Data Nexus for voice AI research community access
      under registered access license. Initial research outputs include protocol development publication and
      open-source software tools.
    publication_references:
      - "Rameau A, et al. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI. Proc. Interspeech 2024, 1445-1449, doi:10.21437/Interspeech.2024-1926"
      - "Bensoussan Y, et al. (2024) Bridge2AI Voice REDCap (v3.20.0). Zenodo, doi:10.5281/zenodo.14148755"
      - "Sigaras A, et al. (2024) eipm/bridge2ai-docs. Zenodo, doi:10.5281/zenodo.13834653"
      - "Johnson A, et al. (2024) Bridge2AI-Voice v1.0. Health Data Nexus, doi:10.57764/qb6h-em84"

future_use_impacts:
  - description: >
      Voice biomarker discovery for disease screening and diagnosis may enable earlier detection, non-invasive
      monitoring, and broader access to diagnostic tools, but requires careful validation to avoid false positives/
      negatives and ensure clinical utility across diverse populations.
    impact_type: Clinical decision support
    potential_benefits:
      - Earlier disease detection through voice screening
      - Non-invasive monitoring tools
      - Cost-effective screening at scale
      - Remote patient monitoring capabilities
      - Broader access to diagnostic tools
    potential_harms:
      - False positive results causing unnecessary anxiety and interventions
      - False negative results delaying diagnosis and treatment
      - Over-reliance on AI tools without clinical judgment
      - Generalization failures across populations
      - Algorithmic bias if training data not representative

  - description: >
      Multi-modal AI model development integrating voice with EHR, genomics, and imaging data may provide
      comprehensive patient assessments, but raises privacy concerns about data linkage and potential for
      re-identification through combined data sources.
    impact_type: Multi-modal integration
    potential_benefits:
      - Comprehensive patient phenotyping
      - Improved diagnostic accuracy through data fusion
      - Personalized medicine applications
      - Longitudinal disease progression tracking
    potential_harms:
      - Increased re-identification risk from linked data
      - Privacy concerns about comprehensive patient profiles
      - Data security vulnerabilities from multiple sources
      - Consent complexities for multi-modal sharing

  - description: >
      Federated learning applications may enable privacy-preserving collaborative research across institutions,
      but requires careful governance to prevent model inversion attacks and ensure equitable benefit sharing.
    impact_type: Privacy-preserving collaboration
    potential_benefits:
      - Multi-institutional model training without data sharing
      - Preservation of patient privacy
      - Larger effective training datasets
      - Institutional autonomy over data
    potential_harms:
      - Model inversion attacks extracting training data
      - Gradient leakage revealing patient information
      - Unequal contribution and benefit distribution
      - Technical barriers for smaller institutions

  - description: >
      Commercial voice AI applications (e.g., smartphone-based screening) may increase accessibility but raise
      concerns about data exploitation, surveillance, and equitable access across socioeconomic groups.
    impact_type: Commercial applications
    potential_benefits:
      - Consumer-accessible health monitoring
      - Scalable screening tools
      - Integration with consumer devices
      - Remote healthcare delivery
    potential_harms:
      - Data exploitation for profit
      - Biometric surveillance concerns
      - Digital divide excluding disadvantaged groups
      - Lack of clinical oversight
      - Privacy erosion through continuous monitoring

intended_uses:
  - description: >
      Development and validation of AI/ML models for voice-based disease screening, diagnosis, and monitoring
      across five target disease categories (voice disorders, neurological, mood, respiratory, pediatric).
    use_case: AI/ML model development

  - description: >
      Discovery and validation of novel acoustic biomarkers associated with health conditions not previously
      recognized as clinically relevant, expanding clinical applications of voice analysis.
    use_case: Biomarker discovery

  - description: >
      Development of clinical decision support tools integrating voice biomarkers into healthcare workflows
      for point-of-care screening, remote monitoring, and EHR integration.
    use_case: Clinical decision support

  - description: >
      Multi-modal biomarker research integrating voice with EHR, radiomics, genomics, and other data sources
      to build comprehensive predictive models for disease screening and progression monitoring.
    use_case: Multi-modal data integration

  - description: >
      Federated learning applications for privacy-preserving collaborative research across institutions,
      enabling model training on distributed datasets without centralized data sharing.
    use_case: Federated learning research

  - description: >
      Development of standards, best practices, and quality measures for acoustic and voice data collection,
      analysis, and clinical validation in voice AI research.
    use_case: Standards development

  - description: >
      Education and training of interdisciplinary researchers in voice biomarkers, AI/ML methods, and ethical
      AI development through curriculum development and community building efforts.
    use_case: Workforce development

discouraged_uses:
  - description: >
      Any attempt to re-identify participants or contact research subjects without appropriate IRB approval,
      Provider consent, and individual informed consent is strictly prohibited under Data Transfer and Use
      Agreement terms.
    prohibition_reason: Privacy protection and ethical principles
    prohibited_activities:
      - Participant re-identification efforts
      - Contact attempts without triple approval (IRB + Provider + participant)
      - Linkage with external databases for re-identification
      - Biometric matching against other voice databases

  - description: >
      Unauthorized sharing, redistribution, selling, renting, leasing, or granting access to third parties
      without prior written consent from Provider Institution is prohibited. Collaborators must apply
      independently for access with separate Data Transfer and Use Agreement.
    prohibition_reason: Data governance and access control
    prohibited_activities:
      - Data sharing without Provider consent
      - Commercial sale or licensing without authorization
      - Third-party access without independent DTUA
      - Redistribution to unauthorized users

  - description: >
      Use of intellectual property protection, database rights, or related rights in ways that prevent or limit
      access to any element of the data or research conclusions derived from it is prohibited to ensure
      scientific openness and reproducibility.
    prohibition_reason: Open science principles
    prohibited_activities:
      - IP restrictions preventing data access
      - Database rights limiting research use
      - Proprietary claims on derived conclusions
      - Preventing replication or validation studies

  - description: >
      Biometric surveillance, profiling, or applications infringing on privacy rights or civil liberties are
      strongly discouraged and violate ethical principles underlying the dataset. Certificate of Confidentiality
      must be asserted against compulsory legal demands.
    prohibition_reason: Ethical framework and privacy rights
    discouraged_activities:
      - Biometric surveillance systems
      - Unauthorized profiling or tracking
      - Privacy-infringing applications
      - Civil liberties violations
    legal_protections:
      - Certificate of Confidentiality against legal demands

  - description: >
      Applications resulting in unfair discrimination or bias against individuals or groups based on voice
      characteristics are discouraged. Fairness and equity principles must be followed, with bias mitigation
      required for clinical applications.
    prohibition_reason: Fairness and equity principles
    discouraged_activities:
      - Discriminatory screening or diagnosis
      - Biased risk assessments
      - Unfair treatment decisions based on voice
      - Applications perpetuating health disparities

# =============================================================================
# DISTRIBUTION
# =============================================================================
distribution_formats:
  - description: >
      Parquet file containing spectrograms with participant_id, session_id, task_name, and 513xN dimension
      time-frequency representation arrays. Parquet format provides efficient columnar storage and fast queries.
    file_name: spectrograms.parquet
    format: Parquet
    structure: Columnar with metadata and dense arrays
    size: Large (dense spectrogram data)

  - description: >
      Parquet file containing 60xN dimension MFCC arrays derived from spectrograms. Added in version 1.1 release
      (January 17, 2025).
    file_name: mfcc.parquet
    format: Parquet
    structure: Columnar with metadata and dense arrays
    version_added: "1.1"

  - description: >
      Tab-delimited phenotype data with one row per unique participant (306 rows total), containing demographics,
      acoustic confounders, and responses to validated questionnaires. Accompanied by JSON data dictionary
      (phenotype.json) with column descriptions.
    file_name: phenotype.tsv
    format: TSV (tab-separated values)
    structure: One row per participant
    rows: 306
    data_dictionary: phenotype.json

  - description: >
      Tab-delimited static features with one row per unique recording (12,523 rows total), containing features
      derived from OpenSMILE, Praat, parselmouth, and torchaudio. Accompanied by JSON data dictionary
      (static_features.json) with feature descriptions.
    file_name: static_features.tsv
    format: TSV (tab-separated values)
    structure: One row per recording
    rows: 12523
    data_dictionary: static_features.json

  - description: >
      Primary distribution through PhysioNet registered access system managed by MIT Laboratory for Computational
      Physiology, supported by NIBIB grant R01EB030362. Requires Data Access Compliance Office (DACO) approval
      with Data Transfer and Use Agreement (DTUA).
    platform: PhysioNet
    url: https://physionet.org/content/b2ai-voice/
    doi_v1_1: https://doi.org/10.13026/249v-w155
    doi_latest: https://doi.org/10.13026/37yb-1t42
    access_mechanism: Registered access with DTUA

  - description: >
      Secondary distribution through Health Data Nexus platform providing alternative access point for AI-ready
      biomedical datasets.
    platform: Health Data Nexus
    url: https://healthdatanexus.ai/content/b2ai-voice/1.0/
    access_mechanism: Registered access

  - description: >
      Project documentation, protocols, and software tools available through official website and GitHub
      repositories under open-source licenses (MIT License for software).
    platform: Project website and GitHub
    url_documentation: https://docs.b2ai-voice.org
    url_github_docs: https://github.com/eipm/bridge2ai-docs
    url_github_b2aiprep: https://github.com/sensein/b2aiprep
    license: MIT License (software)

  - description: >
      Raw audio data available through controlled access only by contacting Data Access Compliance Office
      (DACO@b2ai-voice.org). Raw audio waveforms disseminated with additional privacy protections to protect
      participant confidentiality.
    platform: Controlled access (DACO)
    contact: DACO@b2ai-voice.org
    data_type: Raw audio waveforms
    privacy_level: Enhanced (controlled access only)

license_and_use_terms:
  description: >
    Bridge2AI Voice Registered Access License with Data Transfer and Use Agreement (DTUA) required for all data
    access. Registered users must sign DTUA and obtain approval from Data Access Compliance Office (DACO) before
    accessing files. Recipients must establish administrative, technical, and physical safeguards to protect
    Personally Identifiable Information (PII) per OMB M-07-16 and ensure only authorized persons access data.
    Data provided "AS IS" without warranties of any kind. Recipients assume all liability for use, storage,
    disclosure, or disposal. No unauthorized disclosure to third parties; collaborators must apply independently.
    Attribution required citing both dataset DOI and PhysioNet platform. Commercial use allowed under DTUA terms.
    Recipients may retain derivative works with proper attribution and may publish results (open-access encouraged).
    Two-year use period from DTUA start date upon completion of project, expiration of ethics approval, or
    termination, whichever occurs first; renewable with Provider approval. One archival copy allowed for records
    retention compliance. Provider Institution (University of South Florida) may unilaterally amend if Federal
    sponsor requires; recipient may object resulting in immediate termination. Certificate of Confidentiality
    protections apply and must be asserted against compulsory legal demands. DTUA approved for use through
    August 31, 2025.
  license_name: Bridge2AI Voice Registered Access License
  agreement_required: Data Transfer and Use Agreement (DTUA)
  approval_authority: Data Access Compliance Office (DACO)
  provider_institution: University of South Florida Board of Trustees
  effective_through: August 31, 2025
  key_terms:
    - Registered access with DACO approval required
    - Data classified as Personally Identifiable Information (PII, OMB M-07-16)
    - Administrative, technical, physical safeguards required
    - Certificate of Confidentiality protections (must assert against legal demands)
    - Data provided "AS IS" without warranties
    - Recipients assume liability for use
    - No unauthorized third-party disclosure
    - Collaborators apply independently
    - Attribution requirements (dataset DOI + PhysioNet)
    - Commercial use allowed
    - Open-access publication encouraged
    - Two-year use period (renewable)
    - Archival copy allowed for records retention
    - Provider may amend if Federal sponsor requires
    - Termination results in data destruction (certification required)

# =============================================================================
# MAINTENANCE
# =============================================================================
maintainers:
  - description: >
      Long-term dataset stewardship provided through PhysioNet infrastructure maintained by MIT Laboratory for
      Computational Physiology, supported by NIBIB grant R01EB030362. Ensures persistent access, version control,
      and DOI assignment for citability.
    maintainer: MIT Laboratory for Computational Physiology (PhysioNet)
    funding: NIBIB grant R01EB030362
    responsibilities:
      - Data hosting and distribution
      - Version control and DOI assignment
      - Technical infrastructure maintenance
      - User support for data access
      - Long-term preservation

  - description: >
      Data access governance managed by University of South Florida as Provider Institution through Data Access
      Compliance Office (DACO), responsible for DTUA review, approval, and compliance monitoring.
    maintainer: University of South Florida (Provider Institution)
    unit: Data Access Compliance Office (DACO)
    contact: DACO@b2ai-voice.org
    responsibilities:
      - DTUA review and approval
      - User access management
      - Compliance monitoring
      - Agreement amendments
      - Violation investigation

  - description: >
      Bridge2AI-Voice Consortium led by University of South Florida maintains dataset curation, quality control,
      ongoing data collection, version releases, and scientific oversight throughout project lifecycle (2022-2026)
      and beyond.
    maintainer: Bridge2AI-Voice Consortium
    lead_institution: University of South Florida
    project_period: 2022-09-01 to 2026-11-30
    responsibilities:
      - Ongoing data collection
      - Dataset curation and quality control
      - Version releases and documentation
      - Scientific oversight
      - Ethics compliance
      - Community engagement
      - Workforce development

  - description: >
      Technical support and documentation maintained through project website (docs.b2ai-voice.org), GitHub
      repositories, and community forums. Open-source software tools (b2aiprep, bridge2ai-docs) maintained by
      development team.
    maintainer: Technical development team
    resources:
      - https://docs.b2ai-voice.org
      - https://github.com/eipm/bridge2ai-docs
      - https://github.com/sensein/b2aiprep
    responsibilities:
      - Documentation maintenance
      - Software tool development and support
      - Tutorial and example creation
      - User community building

updates:
  - description: >
      Initial release of Bridge2AI-Voice dataset with 12,523 recordings from 306 participants across five
      clinical sites. Included spectrograms, acoustic features, phenotype data, and static features.
    version: "1.0"
    release_date: January 2024
    doi: https://doi.org/10.57764/qb6h-em84
    platform: Health Data Nexus
    changes:
      - Initial public release
      - 12,523 recordings, 306 participants
      - Spectrograms (513xN)
      - OpenSMILE acoustic features
      - Praat/parselmouth prosodic features
      - Phenotype data with demographics and questionnaires
      - Static features (one per recording)
      - Adult cohort only

  - description: >
      Added Mel-frequency cepstral coefficients (MFCCs) with 60 coefficients per recording, providing additional
      perceptually-relevant acoustic features commonly used in speech and voice analysis.
    version: "1.1"
    release_date: January 17, 2025
    doi: https://doi.org/10.13026/249v-w155
    platform: PhysioNet
    changes:
      - Added mfcc.parquet file
      - 60 MFCC coefficients (60xN dimension)
      - Derived from existing spectrograms

  - description: >
      Planned future release with additional participants, enhanced features, and expanded cohorts. Details to
      be announced.
    version: "2.0.0"
    release_date: April 16, 2025 (planned)
    status: Planned
    anticipated_changes:
      - Additional participants
      - Expanded disease cohorts
      - Enhanced feature sets
      - Potentially pediatric cohort data

  - description: >
      Planned maintenance release with bug fixes, documentation updates, and minor enhancements. Currently the
      latest version available.
    version: "2.0.1"
    release_date: August 18, 2025 (planned)
    status: Planned (latest version)
    anticipated_changes:
      - Bug fixes and corrections
      - Documentation improvements
      - Minor feature enhancements

version_access:
  description: >
    All versions available through PhysioNet with version-specific DOIs for citability and reproducibility.
    Latest version DOI always points to most recent release. Users can access specific versions for replication
    or access latest version for most current data.
  version_doi_1_0: https://doi.org/10.57764/qb6h-em84
  version_doi_1_1: https://doi.org/10.13026/249v-w155
  latest_doi: https://doi.org/10.13026/37yb-1t42
  current_latest_version: "2.0.1"

update_frequency: >
  Ongoing data collection with periodic versioned releases. Major releases planned approximately every 6-12 months
  during active project period (2022-2026). Future releases will include additional participants from adult cohorts,
  pediatric cohort data (currently not included), and potentially raw audio waveforms with enhanced privacy
  protections. Post-project maintenance will continue through PhysioNet infrastructure with updates as needed for
  corrections, documentation, and community contributions.

# =============================================================================
# DATA GOVERNANCE
# =============================================================================
governance_model:
  - description: >
      Data Access Compliance Office (DACO) oversight model with University of South Florida as Provider Institution.
      All data access requires DACO review and approval of Data Transfer and Use Agreement (DTUA). Recipients must
      have IRB approval for their research use. DACO monitors compliance with agreement terms and investigates
      violations.
    governance_authority: University of South Florida
    oversight_body: Data Access Compliance Office (DACO)
    contact: DACO@b2ai-voice.org
    requirements:
      - DTUA application and approval
      - Recipient IRB approval required
      - Institutional authorized official signature
      - Compliance monitoring
      - Periodic renewal
      - Violation investigation procedures

  - description: >
      Provider Institution (University of South Florida) retains ultimate authority over data sharing decisions,
      agreement amendments, and termination. May unilaterally amend DTUA if Federal sponsor requires; recipients
      may object resulting in immediate termination and data destruction.
    governance_authority: Provider Institution
    decision_making:
      - Approval/denial of access requests
      - DTUA amendment authority
      - Unilateral amendment if Federal sponsor requires
      - Termination authority
      - Interpretation of agreement terms

privacy_controls:
  - description: >
      Data classified as Personally Identifiable Information (PII) per OMB Memorandum M-07-16. Not covered under
      HIPAA, FERPA, or similar personal information regulations. HIPAA Safe Harbor de-identification applied but
      data still treated as sensitive requiring protection.
    classification: Personally Identifiable Information (PII, OMB M-07-16)
    not_covered_by:
      - HIPAA
      - FERPA
      - Similar personal information regulations
    de_identification: HIPAA Safe Harbor method applied

  - description: >
      Certificate of Confidentiality (CoC) protects participant privacy against compulsory legal demands such as
      court orders and subpoenas. Recipients must assert CoC protections if legally compelled to disclose data.
    protection_mechanism: Certificate of Confidentiality
    legal_protections:
      - Protection against court orders
      - Protection against subpoenas
      - Mandatory assertion by recipients
      - Federal protections for research participants
    reference: https://grants.nih.gov/policy/humansubjects/coc.htm

  - description: >
      Recipients required to establish administrative, technical, and physical safeguards adequate to protect PII.
      Safeguards must ensure only authorized persons access data and maintain appropriate control at all times.
      Controls include those for electronic protected health information security.
    safeguard_requirements:
      - Administrative safeguards for access control
      - Technical safeguards preventing unauthorized use
      - Physical safeguards for data storage and transmission
      - Access limited to authorized persons
      - Electronic PHI security standards
      - Continuous monitoring and control

  - description: >
      Federated learning infrastructure implemented for multi-institutional analysis while minimizing data sharing
      and preserving patient privacy. Enables collaborative model development without centralized data pooling.
    privacy_technology: Federated learning
    privacy_benefits:
      - No centralized data sharing required
      - Institutional data sovereignty maintained
      - Privacy-preserving collaborative research
      - Reduced data transfer and exposure

third_party_restrictions:
  - description: >
      Collaborators at other research organizations and other research teams at the same organization must apply
      independently for data access and sign separate Data Transfer and Use Agreement (DTUA) with Provider before
      accessing data. No sub-licensing or derivative access permissions.
    restriction_type: Independent access requirement
    policy: Each collaborator requires independent DTUA
    prohibited:
      - Sub-licensing to collaborators
      - Shared access credentials
      - Data transfer to non-approved users

  - description: >
      No disclosure, release, sale, rent, lease, loan, or granting access to data to any third party except
      authorized persons without prior written consent of Provider. Data must be retained under recipient control
      at all times.
    restriction_type: Third-party disclosure prohibition
    policy: Written Provider consent required for any third-party access
    prohibited:
      - Unauthorized disclosure
      - Commercial sale without approval
      - Data lending or leasing
      - Transfer to third parties

  - description: >
      Provider may unilaterally amend DTUA if Federal sponsor requires revision. If recipient objects to amendment,
      DTUA immediately terminates and recipient must immediately return or destroy all data.
    restriction_type: Federal sponsor requirements
    policy: Provider may amend for sponsor compliance
    recipient_rights:
      - May object to amendment
      - Objection results in immediate termination
      - Must return or destroy data upon termination

citation_requirements:
  - description: >
      Primary dataset citation required for all publications, presentations, and other uses of data. Should cite
      specific version used (via version DOI) for reproducibility.
    citation_type: Dataset citation
    format: >
      Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V.,
      Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice
      dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345.
      https://doi.org/10.13026/249v-w155

  - description: >
      PhysioNet platform citation required as standard acknowledgment of infrastructure supporting data distribution.
    citation_type: Platform citation
    format: >
      Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000).
      PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals.
      Circulation [Online]. 101 (23), pp. e215–e220. RRID:SCR_007345.

  - description: >
      Recipient agrees to recognize contribution of Provider as source of data in all written, visual, or oral
      public disclosures of research using data, as appropriate in accordance with scholarly standards.
    citation_type: Attribution requirement
    policy: Provider recognition required in all public disclosures

  - description: >
      Recipients encouraged to make results publicly available in open-access journals or pre-print servers where
      possible to maximize scientific dissemination and reproducibility.
    citation_type: Publication encouragement
    policy: Open-access publication encouraged

# =============================================================================
# ADDITIONAL METADATA
# =============================================================================
project_specific_aims:
  - aim_number: 1
    aim_title: Data Acquisition Module
    description: >
      To build a multi-modal, multi-institutional, large scale, diverse and ethically sourced human voice database
      linked to other biomarkers of health that is AI/ML friendly to fuel voice AI research.

  - aim_number: 2
    aim_title: Standard Module
    description: >
      To introduce the field of acoustic biomarkers by developing new standards of acoustic and voice data collection
      and analysis for voice AI research.

  - aim_number: 3
    aim_title: Tool Development and Optimization
    description: >
      To develop a software and cloud infrastructure for automated voice data collection through a smartphone
      application that allows non-invasive, user-friendly, high quality voice data collection while minimizing
      human manipulation. To implement Federated Learning technology to allow analysis of multi-institutional data
      while minimizing data sharing and preserving patient privacy.

  - aim_number: 4
    aim_title: Ethics Module
    description: >
      To integrate existing scholarship, tools, and guidance with development of new standard and normative insights
      for identifying, anticipating, addressing, and providing guidance on ethical and trustworthy issues from voice
      data generation and AI/ML research and development to clinical adoption and downstream health decisions and
      outcomes. To develop new guidelines for consenting to voice data collection, voice data sharing and utilization
      in the context of voice AI technology.

  - aim_number: 5
    aim_title: Teaming Module
    description: >
      To build bridges between the medical voice research world, the acoustic engineers, and the AI/ML world to
      promote the integration of tangible clinical application for Voice AI algorithms.

  - aim_number: 6
    aim_title: Skills and Workforce Development Module
    description: >
      To develop a unique curriculum on voice biomarkers of health and the development, validation, and implementation
      for AI models that are FAIR and CARE. To create a community of voice AI researchers, especially those from
      underserved communities, and foster collaborations to promote application of ML for Voice Research. To engage
      a broad range of learners with competency assessment and mentorship.

related_publications:
  - citation: >
      Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A.,
      Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D.,
      Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical
      expertise, bioethics, standards, and DEI. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926

  - citation: >
      Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D.,
      Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P.,
      Tang, J., Sigaras, A., Elemento, O., Dorr, D., ... Bridge2AI-Voice. (2024). Bridge2AI Voice REDCap (v3.20.0).
      Zenodo. https://doi.org/10.5281/zenodo.14148755

  - citation: >
      Sigaras, A., Zisimopoulos, P., Tang, J., Bevers, I., Gallois, H., Bernier, A., Bensoussan, Y., Ghosh, S. S.,
      Rameau, A., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Elemento, O., Dorr, D., ...
      Bridge2AI-Voice. (2024). eipm/bridge2ai-docs. Zenodo. https://zenodo.org/doi/10.5281/zenodo.13834653

  - citation: >
      Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V.,
      Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice
      dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84

software_and_tools:
  - name: b2aiprep
    version: "0.21.0"
    description: Open source library for preprocessing raw audio waveforms and merging source data into phenotype files
    url: https://github.com/sensein/b2aiprep
    license: Open source

  - name: Bridge2AI Voice REDCap
    version: v3.20.0
    description: Custom REDCap configuration for voice data collection
    doi: https://doi.org/10.5281/zenodo.14148755

  - name: bridge2ai-docs
    version: "2.0.5"
    description: Documentation dashboard and project documentation
    url: https://github.com/eipm/bridge2ai-docs
    doi: https://zenodo.org/doi/10.5281/zenodo.13834653
    license: MIT License
    topics:
      - ai
      - bridge2ai
      - bridge2ai-voice
      - python
      - streamlit-dashboard

  - name: OpenSMILE
    description: The Munich Versatile and Fast Open-Source Audio Feature Extractor
    reference: >
      Florian Eyben, Martin Wöllmer, Björn Schuller: "openSMILE - The Munich Versatile and Fast Open-Source Audio
      Feature Extractor", Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462,
      25.-29.10.2010.
    url: https://audeering.github.io/opensmile/

  - name: Praat
    description: Phonetic analysis software
    reference: >
      Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.
    url: http://www.praat.org/

  - name: Parselmouth
    description: Python interface to Praat for phonetic analysis
    reference: >
      Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics.
      2018 Nov 1;71:1-5.
    url: https://github.com/YannickJadoul/Parselmouth

  - name: TorchAudio
    description: Audio processing library for PyTorch
    reference: >
      Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D.,
      Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P.,
      Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-Bélair, V, & Shi, Y. (2021). TorchAudio:
      Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018.
    url: https://github.com/pytorch/audio

  - name: OpenAI Whisper
    description: Automatic speech recognition model (Large variant)
    url: https://github.com/openai/whisper
