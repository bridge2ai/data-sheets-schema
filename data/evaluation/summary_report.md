# D4D Evaluation Summary Report

Generated: 2025-11-17T22:11:53.072638

## Overview

Total evaluations: 11

Projects evaluated: AI_READI, CHORUS, CM4AI, VOICE

Methods evaluated: claudecode, curated, gpt5


## Overall Scores

### Rubric10 Scores (0-50)

| Project | Curated | GPT-5 | Claude Code |
|---------|---------|-------|-------------|
| AI_READI | 11.0 (22.0%) | 5.0 (10.0%) | 12.0 (24.0%) |
| CHORUS | N/A | 4.0 (8.0%) | 11.0 (22.0%) |
| CM4AI | 8.0 (16.0%) | 9.0 (18.0%) | 13.0 (26.0%) |
| VOICE | 13.0 (26.0%) | 5.0 (10.0%) | 39.0 (78.0%) |

### Rubric20 Scores (varies by max)

| Project | Curated | GPT-5 | Claude Code |
|---------|---------|-------|-------------|
| AI_READI | 41.0/84 (48.8%) | 14.0/84 (16.7%) | 39.0/84 (46.4%) |
| CHORUS | N/A | 12.0/84 (14.3%) | 31.0/84 (36.9%) |
| CM4AI | 25.0/84 (29.8%) | 18.0/84 (21.4%) | 38.0/84 (45.2%) |
| VOICE | 39.0/84 (46.4%) | 14.0/84 (16.7%) | 68.0/84 (81.0%) |

## Method Comparison


### CLAUDECODE
- Average Rubric10: 37.5%
- Average Rubric20: 52.4%
- Evaluations: 4

### CURATED
- Average Rubric10: 21.3%
- Average Rubric20: 41.7%
- Evaluations: 3

### GPT5
- Average Rubric10: 11.5%
- Average Rubric20: 17.3%
- Evaluations: 4