# D4D Evaluation Summary Report

Generated: 2025-11-21T11:37:05.670003

## Overview

Total evaluations: 11

Projects evaluated: AI_READI, CHORUS, CM4AI, VOICE

Methods evaluated: claudecode, curated, gpt5


## Overall Scores

### Rubric10 Scores (0-50)

| Project | Curated | GPT-5 | Claude Code |
|---------|---------|-------|-------------|
| AI_READI | 11.0 (22.0%) | 9.0 (18.0%) | 9.0 (18.0%) |
| CHORUS | N/A | 2.0 (4.0%) | 9.0 (18.0%) |
| CM4AI | 8.0 (16.0%) | 12.0 (24.0%) | 9.0 (18.0%) |
| VOICE | 13.0 (26.0%) | 2.0 (4.0%) | 9.0 (18.0%) |

### Rubric20 Scores (varies by max)

| Project | Curated | GPT-5 | Claude Code |
|---------|---------|-------|-------------|
| AI_READI | 41.0/84 (48.8%) | 36.0/84 (42.9%) | 36.0/84 (42.9%) |
| CHORUS | N/A | 8.0/84 (9.5%) | 36.0/84 (42.9%) |
| CM4AI | 25.0/84 (29.8%) | 37.0/84 (44.0%) | 36.0/84 (42.9%) |
| VOICE | 39.0/84 (46.4%) | 8.0/84 (9.5%) | 36.0/84 (42.9%) |

## Method Comparison


### CLAUDECODE
- Average Rubric10: 18.0%
- Average Rubric20: 42.9%
- Evaluations: 4

### CURATED
- Average Rubric10: 21.3%
- Average Rubric20: 41.7%
- Evaluations: 3

### GPT5
- Average Rubric10: 12.5%
- Average Rubric20: 26.5%
- Evaluations: 4