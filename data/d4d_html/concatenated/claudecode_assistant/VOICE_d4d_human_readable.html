
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>VOICE Dataset Documentation - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>VOICE Dataset Documentation</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üéØ</span>
                    <div>
                        <h2 class="section-title">Motivation</h2>
                        <p class="section-description">Why was the dataset created?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Purposes
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Integrate the use of voice as a biomarker of health in clinical care by generating a substantial mul...</td><td>purpose-001</td><td>Integrate voice as biomarker in clinical care</td></tr><tr><td>Create an ethically sourced flagship dataset to enable future research in artificial intelligence an...</td><td>purpose-002</td><td>Create ethically sourced flagship dataset</td></tr><tr><td>Establish standards, best practices, and guidelines for voice data collection and analysis to advanc...</td><td>purpose-003</td><td>Establish standards for voice data</td></tr><tr><td>Ensure patient protection through ethical and fairness principles, create safe and innovative infras...</td><td>purpose-004</td><td>Promote ethical AI development</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Funders
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Funded through National Institutes of Health grant 3OT2OD032720-01S3 (Bridge2AI: Voice as a Biomarke...</td><td>funder-001</td><td>NIH Office of the Director</td></tr><tr><td>NIBIB supports PhysioNet managed by MIT Laboratory for Computational Physiology under NIH grant numb...</td><td>funder-002</td><td>National Institute of Biomedical Imaging and Bioengineering</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üìä</span>
                    <div>
                        <h2 class="section-title">Composition</h2>
                        <p class="section-description">What do the instances represent?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Instances
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>instance-001</dd><dt>Name</dt><dd>Adult participants with voice-affecting conditions</dd><dt>Description</dt><dd><div class="long-description">Adult participants presenting at specialty clinics and institutions across five sites in North America. Participants were selected based on membership to five predetermined disease cohort groups: Respiratory disorders, Voice disorders, Neurological disorders, Mood disorders, and Pediatric. As of v1.1, only data from the adult cohort is available. The initial release (v1.0) provides 306 participants with 12,523 recordings collected through standardized protocols.
</div></dd><dt>Instance Type</dt><dd><div class="long-description">Human participants recruited from specialty clinics at multi-institutional sites. Data collection conducted between September 1, 2022 and November 30, 2026 through IRB-approved protocols with informed consent.
</div></dd></dl></li></ol></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subpopulations
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Participants with laryngeal disorders including laryngeal cancers, vocal fold paralysis, and benign ...</td><td>subpop-001</td><td>Voice Disorders cohort</td></tr><tr><td>Participants with conditions such as Alzheimer's disease, Parkinson's disease, stroke, and ALS exhib...</td><td>subpop-002</td><td>Neurological and Neurodegenerative Disorders cohort</td></tr><tr><td>Participants with depression, schizophrenia, bipolar disorders, and anxiety disorders showing vocal ...</td><td>subpop-003</td><td>Mood and Psychiatric Disorders cohort</td></tr><tr><td>Participants with respiratory conditions including pneumonia, COPD, heart failure, and obstructive s...</td><td>subpop-004</td><td>Respiratory Disorders cohort</td></tr><tr><td>Pediatric participants with voice and speech disorders including autism spectrum disorder and speech...</td><td>subpop-005</td><td>Pediatric cohort</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Distribution Formats
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Access Urls</th><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Spectrograms stored in Parquet format (spectrograms.parquet). Each element contains participant_id, ...</td><td>format-001</td><td>Parquet for spectrograms</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Mel-frequency cepstral coefficients stored in Parquet format (mfcc.parquet). Contains 60xN dimension...</td><td>format-002</td><td>Parquet for MFCCs</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Tab-delimited phenotype file (phenotype.tsv) with one row per unique participant. Contains demograph...</td><td>format-003</td><td>TSV for phenotype data</td></tr><tr><td>https://physionet.org/content/b2ai-voice/</td><td>Tab-delimited static features file (static_features.tsv) with one row per unique recording. Contains...</td><td>format-004</td><td>TSV for static features</td></tr><tr><td>Contact DACO@b2ai-voice.org</td><td>Original raw audio waveforms available through controlled access only. Interested users contact DACO...</td><td>format-005</td><td>Raw audio (controlled access only)</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value"><a href="https://doi.org/10.13026/37yb-1t42" target="_blank">https://doi.org/10.13026/37yb-1t42</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice - An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. This comprehensive collection provides voice recordings with corresponding clinical information from participants selected based on known conditions which manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and respiratory disorders. The dataset is designed to fuel voice AI research, establish data standards, and promote ethical and trustworthy AI/ML development for voice biomarkers of health. Data collection occurs through a multi-institutional collaborative effort using standardized protocols, custom smartphone applications, and rigorous ethical oversight. The initial release (v1.0) provides 12,523 recordings for 306 participants collected across five sites in North America, with derived features such as spectrograms, MFCCs, acoustic features, and clinical phenotype data. Raw audio data is available through controlled access to protect participant privacy.
</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Language
                            
                        </label>
                        <div class="item-value">en</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice biomarker</li><li>acoustic biomarker</li><li>Bridge2AI</li><li>voice AI</li><li>voice disorders</li><li>neurological disorders</li><li>neurodegenerative disorders</li><li>mood disorders</li><li>psychiatric disorders</li><li>respiratory disorders</li><li>pediatric voice disorders</li><li>speech disorders</li><li>Parkinson's disease</li><li>Alzheimer's disease</li><li>depression</li><li>schizophrenia</li><li>bipolar disorder</li><li>stroke</li><li>ALS</li><li>autism</li><li>speech delay</li><li>laryngeal cancer</li><li>vocal fold paralysis</li><li>pneumonia</li><li>COPD</li><li>heart failure</li><li>obstructive sleep apnea</li><li>spectrogram</li><li>MFCC</li><li>mel-frequency cepstral coefficients</li><li>OpenSMILE</li><li>Praat</li><li>Parselmouth</li><li>federated learning</li><li>ethical AI</li><li>multimodal health data</li><li>electronic health records</li><li>EHR</li><li>radiomics</li><li>genomics</li><li>FAIR principles</li><li>CARE principles</li><li>PhysioNet</li><li>Health Data Nexus</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Addressing Gaps
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Address the lack of large, high quality, multi-institutional and diverse voice databases linked to m...</td><td>gap-001</td><td>Lack of diverse voice databases</td></tr><tr><td>Overcome limitations in existing voice and psychiatric disorder research that has relied on small da...</td><td>gap-002</td><td>Limited psychiatric disorder datasets</td></tr><tr><td>Fill the gap in pediatric voice and speech analysis research, which is sparser partly due to ethical...</td><td>gap-003</td><td>Pediatric data scarcity</td></tr><tr><td>Establish missing standards for voice data collection, acoustic analysis, and ethical frameworks for...</td><td>gap-004</td><td>Missing voice data standards</td></tr><tr><td>Address important issues related to patient privacy protection, ethical and fair representation of p...</td><td>gap-005</td><td>Privacy and ethics concerns</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Creators
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td>Yael Emilie Bensoussan</td><td>creator-001</td><td>-</td></tr><tr><td>Contributor</td><td>Jean-Christophe B√©lisle-Pipon</td><td>creator-002</td><td>-</td></tr><tr><td>Contributor</td><td>David A. Dorr</td><td>creator-003</td><td>-</td></tr><tr><td>Contributor</td><td>Satrajit Sujit Ghosh</td><td>creator-004</td><td>-</td></tr><tr><td>Contributor</td><td>Philip R.O. Payne</td><td>creator-005</td><td>-</td></tr><tr><td>Contributor</td><td>Maria Ellen Powell</td><td>creator-006</td><td>-</td></tr><tr><td>Contributor</td><td>Anais Rameau</td><td>creator-007</td><td>-</td></tr><tr><td>Contributor</td><td>Vardit Ravitsky</td><td>creator-008</td><td>-</td></tr><tr><td>Contributor</td><td>Alexandros Sigaras</td><td>creator-009</td><td>-</td></tr><tr><td>Contributor</td><td>Olivier Elemento</td><td>creator-010</td><td>-</td></tr><tr><td>Contributor</td><td>Alistair Johnson</td><td>creator-011</td><td>-</td></tr><tr><td>Contributor</td><td>Jennifer Siu</td><td>creator-012</td><td>-</td></tr><tr><td>Contributor</td><td>Bridge2AI-Voice Consortium</td><td>creator-013</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subsets
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Contains derived features from voice recordings including spectrograms (513xN dimensions), MFCCs (60...</td><td>subset-001</td><td>Public Access Dataset (PhysioNet Registered Access)</td></tr><tr><td>Original raw audio waveforms available through controlled access only to protect participant privacy...</td><td>subset-002</td><td>Controlled Access Raw Audio Dataset</td></tr><tr><td>Time-frequency representations computed using short-time FFT with 25ms window, 10ms hop length, 512-...</td><td>subset-003</td><td>Spectrograms Parquet file</td></tr><tr><td>60 Mel-frequency cepstral coefficients extracted from spectrograms. Each element has 60xN dimension ...</td><td>subset-004</td><td>MFCC Parquet file</td></tr><tr><td>Tab-delimited file with one row per unique participant containing demographics, acoustic confounders...</td><td>subset-005</td><td>Phenotype TSV file</td></tr><tr><td>Tab-delimited file with one row per unique recording containing OpenSMILE, Praat, Parselmouth, and t...</td><td>subset-006</td><td>Static Features TSV file</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sampling Strategies
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>sampling-001</dd><dt>Name</dt><dd>Targeted disease cohort recruitment</dd><dt>Description</dt><dd><div class="long-description">Patients presenting at specialty clinics and institutions were screened for inclusion and exclusion criteria prior to their visit by project investigators. Participants were selected based on membership to five predetermined disease cohort groups to ensure representation across conditions affecting voice: (1) Voice Disorders - laryngeal cancers, vocal fold paralysis, benign laryngeal lesions; (2) Neurological and Neurodegenerative Disorders - Alzheimer's, Parkinson's, stroke, ALS; (3) Mood and Psychiatric Disorders - depression, schizophrenia, bipolar disorders; (4) Respiratory disorders - pneumonia, COPD, heart failure, obstructive sleep apnea; (5) Pediatric diseases - autism, speech delay.
</div></dd><dt>Is Sample</dt><dd><ul class='formatted-list'><li>True</li></ul></dd><dt>Is Random</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Is Representative</dt><dd><ul class='formatted-list'><li>False</li></ul></dd><dt>Strategies</dt><dd><ul class='formatted-list'><li>Targeted recruitment from specialty clinics representing five disease cohort categories</li><li>Screening based on known conditions manifesting within voice waveform</li><li>Multi-institutional enrollment across five sites in North America</li><li>Standardized inclusion and exclusion criteria applied by investigators</li><li>Protocols developed through team science approach involving clinical expertise, bioethics, standards, and DEI</li></ul></dd></dl></li></ol></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Mechanisms
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Data collection conducted using a custom smartphone application on tablet with headset used when pos...</td><td>collection-001</td><td>Smartphone application data collection</td></tr><tr><td>Multi-institutional data collection across five specialty clinic sites in North America. Patients pr...</td><td>collection-002</td><td>Multi-institutional clinic recruitment</td></tr><tr><td>Data collection protocol involved: (1) demographic information collection, (2) health questionnaires...</td><td>collection-003</td><td>Standardized data collection protocol</td></tr><tr><td>Cloud infrastructure for automated voice data collection developed to allow analysis of multi-instit...</td><td>collection-004</td><td>Federated learning infrastructure</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Acquisition Methods
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Voice recording tasks capturing voice, speech, and language data relating to health. Tasks include s...</td><td>acquisition-001</td><td>Voice recording tasks</td></tr><tr><td>Self-reported demographic and medical history questionnaires completed by participants who consent. ...</td><td>acquisition-002</td><td>Questionnaires and surveys</td></tr><tr><td>Electronic health record (EHR) access for participants who consent, permitting investigators to acce...</td><td>acquisition-003</td><td>Electronic health record access</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Preprocessing Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th><th>Preprocessing Details</th></tr></thead><tbody><tr><td>Raw audio preprocessing by converting to monaural and resampling to 16 kHz with Butterworth anti-ali...</td><td>preproc-001</td><td>Audio standardization</td><td>Conversion to monaural audio, Resampling to 16 kHz sampling rate, ... (+2 more)</td></tr><tr><td>Spectrogram extraction - Time-frequency representations computed using short-time Fast Fourier Trans...</td><td>preproc-002</td><td>Spectrogram generation</td><td>Short-time FFT with 25ms window, 10ms hop length, ... (+3 more)</td></tr><tr><td>Mel-frequency cepstral coefficients (MFCC) extraction - 60 MFCCs extracted from spectrograms. MFCCs ...</td><td>preproc-003</td><td>MFCC extraction</td><td>60 MFCC coefficients extracted, Derived from spectrograms, ... (+2 more)</td></tr><tr><td>Acoustic feature extraction using OpenSMILE (Speech and Music Interpretation by Large-space Extracti...</td><td>preproc-004</td><td>OpenSMILE feature extraction</td><td>OpenSMILE feature extraction, Temporal dynamics captured, ... (+2 more)</td></tr><tr><td>Phonetic and prosodic feature computation using Parselmouth and Praat, providing measures of fundame...</td><td>preproc-005</td><td>Praat and Parselmouth features</td><td>Parselmouth and Praat feature extraction, Fundamental frequency (F0) measurement, ... (+2 more)</td></tr><tr><td>Transcription generation using OpenAI's Whisper Large model. Automated speech recognition applied to...</td><td>preproc-006</td><td>Automated transcription</td><td>OpenAI Whisper Large model, Automated transcription of audio, ... (+2 more)</td></tr><tr><td>Data export and conversion from REDCap using open source b2aiprep library developed by the team. Phe...</td><td>preproc-007</td><td>REDCap export and conversion</td><td>REDCap data export, b2aiprep library conversion, ... (+2 more)</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Cleaning Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Cleaning Details</th><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>HIPAA Safe Harbor compliance, 18 identifier categories removed, ... (+3 more)</td><td>HIPAA Safe Harbor de-identification applied. Identifiers removed include: names, geographic locators...</td><td>cleaning-001</td><td>HIPAA Safe Harbor de-identification</td></tr><tr><td>Audio waveforms excluded from public release, Derived features only in public dataset, ... (+2 more)</td><td>Privacy protection measures for public release - Audio waveforms omitted from public dataset, only d...</td><td>cleaning-002</td><td>Privacy protection for public release</td></tr><tr><td>Standardized collection protocols, Common smartphone application, ... (+2 more)</td><td>Data standardization across multi-institutional sites through use of standardized protocols, common ...</td><td>cleaning-003</td><td>Multi-site data standardization</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Maintainers
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Maintainer Details</th><th>Name</th></tr></thead><tbody><tr><td>Multidisciplinary consortium responsible for dataset maintenance including data collection, curation...</td><td>maintainer-001</td><td>University of South Florida (lead institution, Tampa, FL), Multi-institutional data collection sites (five sites in North America), ... (+6 more)</td><td>Bridge2AI-Voice Consortium</td></tr><tr><td>PhysioNet platform managed by MIT Laboratory for Computational Physiology serves as primary distribu...</td><td>maintainer-002</td><td>MIT Laboratory for Computational Physiology, PhysioNet platform infrastructure, ... (+3 more)</td><td>PhysioNet / MIT Laboratory for Computational Physiology</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Retention Limit
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>retention-001</dd><dt>Name</dt><dd>Data retention and disposition</dd><dt>Description</dt><dd><div class="long-description">Data Transfer and Use Agreement specifies retention requirements. Upon termination or expiration of agreement (two years after start date, project completion, ethics approval expiration, or provider termination), data shall be destroyed per provider instructions with written certification required within 30 days. Recipient may retain one copy to extent necessary to comply with records retention requirements under law, regulation, institutional policy, and for research integrity and verification purposes. Restrictions apply to archival copies as long as recipient holds data. Provider may unilaterally amend agreement if federal sponsor requires revision.
</div></dd><dt>Retention Details</dt><dd><ul class='formatted-list'><li>Two-year agreement term from start date</li><li>Data destruction required upon termination unless retention justified</li><li>One archival copy permitted for compliance and verification</li><li>Written certification of destruction required within 30 days</li><li>Ongoing restrictions apply to retained copies</li><li>Provider may unilaterally amend if federal sponsor requires</li><li>Termination if recipient objects to amendments</li><li>Disposition instructions in DTUA Attachment 1</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sensitive Elements
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th><th>Sensitive Elements Present</th><th>Sensitivity Details</th></tr></thead><tbody><tr><td>Voice recordings contain personally identifiable information and are considered biometric identifier...</td><td>sensitive-001</td><td>Voice as biometric identifier</td><td>True</td><td>Voice as biometric identifier, Raw audio waveforms, ... (+3 more)</td></tr><tr><td>Electronic health record (EHR) data accessed with participant consent for gold standard validation o...</td><td>sensitive-002</td><td>Health information from EHR</td><td>True</td><td>EHR medical information, Diagnoses and symptoms (validated), ... (+3 more)</td></tr><tr><td>Demographic information and geographic data collected but de-identified for public release. State an...</td><td>sensitive-003</td><td>Demographic and geographic data</td><td>True</td><td>Demographic data (de-identified), Geographic information (state/province removed, country retained), ... (+3 more)</td></tr><tr><td>Dataset covered under Certificate of Confidentiality which must be asserted against compulsory legal...</td><td>sensitive-004</td><td>Certificate of Confidentiality protection</td><td>True</td><td>Certificate of Confidentiality coverage, Protection against compulsory legal demands, ... (+3 more)</td></tr><tr><td>Data is Personally Identifiable Information as defined in OMB Memorandum M-07-16, not covered under ...</td><td>sensitive-005</td><td>Personally Identifiable Information</td><td>True</td><td>Personally Identifiable Information (OMB M-07-16), Not covered under HIPAA/FERPA, ... (+3 more)</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            External Resources
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>External Resources</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Primary distribution platform for public access dataset with registered access</td><td>https://physionet.org/content/b2ai-voice/</td><td>resource-001</td><td>PhysioNet Dataset Landing Page</td></tr><tr><td>Comprehensive project documentation and resources</td><td>https://docs.b2ai-voice.org</td><td>resource-002</td><td>Bridge2AI-Voice Project Documentation</td></tr><tr><td>Open source code repository including b2aiprep library and documentation dashboard</td><td>https://github.com/eipm/bridge2ai-docs</td><td>resource-003</td><td>Bridge2AI-Voice GitHub Repository</td></tr><tr><td>Federal grant information and project details for grant 3OT2OD032720-01S3</td><td>https://reporter.nih.gov/project-details/11376382</td><td>resource-004</td><td>NIH RePORTER Project Details</td></tr><tr><td>Alternative data repository platform hosting v1.0 dataset</td><td>https://healthdatanexus.ai/content/b2ai-voice/1.0/</td><td>resource-005</td><td>Health Data Nexus</td></tr><tr><td>Additional dataset documentation, software releases, and Bridge2AI Voice REDCap v3.20.0</td><td>https://doi.org/10.5281/zenodo.13834653, https://doi.org/10.5281/zenodo.14148755</td><td>resource-006</td><td>Zenodo Archive</td></tr><tr><td>Research resource for complex physiologic signals (Goldberger et al. 2000, Circulation)</td><td>https://physionet.org</td><td>resource-007</td><td>PhysioNet Platform</td></tr><tr><td>Publication describing multi-disorder voice protocol development through team science approach invol...</td><td>https://doi.org/10.21437/Interspeech.2024-1926</td><td>resource-008</td><td>Interspeech 2024 Protocol Publication</td></tr><tr><td>Contact for controlled access to raw audio data</td><td>mailto:DACO@b2ai-voice.org</td><td>resource-009</td><td>Data Access Compliance Office</td></tr><tr><td>Parent NIH Common Fund program supporting AI-ready biomedical datasets</td><td>https://bridge2ai.org</td><td>resource-010</td><td>Bridge2AI Program</td></tr><tr><td>Open source library for preprocessing raw audio and phenotype data (Bevers et al.)</td><td>https://github.com/sensein/b2aiprep</td><td>resource-011</td><td>b2aiprep Software Library</td></tr><tr><td>DOI for latest version of dataset (v2.0.1 as of August 2025)</td><td>https://doi.org/10.13026/37yb-1t42</td><td>resource-012</td><td>Latest Version DOI</td></tr><tr><td>Version-specific DOI for v1.1 release</td><td>https://doi.org/10.13026/249v-w155</td><td>resource-013</td><td>Version 1.1 DOI</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üöÄ</span>
                    <div>
                        <h2 class="section-title">Uses</h2>
                        <p class="section-description">What (other) tasks could the dataset be used for?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Tasks
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Enable development of AI/ML predictive models for screening, diagnosis, and treatment of voice disor...</td><td>task-001</td><td>Voice disorder diagnosis and screening</td></tr><tr><td>Support machine learning models for neurological and neurodegenerative disorders including Alzheimer...</td><td>task-002</td><td>Neurological disorder detection</td></tr><tr><td>Develop AI algorithms for mood and psychiatric disorder detection including depression, schizophreni...</td><td>task-003</td><td>Mood and psychiatric disorder screening</td></tr><tr><td>Create machine learning models for respiratory disorder screening and therapeutic monitoring using r...</td><td>task-004</td><td>Respiratory disorder monitoring</td></tr><tr><td>Build AI models for pediatric voice and speech disorder detection including autism spectrum disorder...</td><td>task-005</td><td>Pediatric disorder detection</td></tr><tr><td>Promote application of AI/ML for voice research through workforce development, curriculum creation, ...</td><td>task-006</td><td>Workforce development</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Intended Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Primary intended use is development and validation of AI/ML models for voice as a biomarker of healt...</td><td>use-001</td><td>Voice AI model development</td></tr><tr><td>Research into acoustic biomarkers and development of standards for voice data collection and analysi...</td><td>use-002</td><td>Acoustic biomarker research</td></tr><tr><td>Training and education in voice AI research through workforce development initiatives, curriculum cr...</td><td>use-003</td><td>Workforce development and training</td></tr><tr><td>Multimodal health research combining voice data with EHR information, radiomics, genomics, and other...</td><td>use-004</td><td>Multimodal health research</td></tr><tr><td>Model dataset for ethical AI development in healthcare, demonstrating integration of bioethics guida...</td><td>use-005</td><td>Ethical AI development model</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Discouraged Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td>Direct clinical decision-making</td><td>discouraged-001</td><td>-</td></tr><tr><td>Contributor</td><td>Re-identification attempts</td><td>discouraged-002</td><td>-</td></tr><tr><td>Contributor</td><td>Data use agreement violations</td><td>discouraged-003</td><td>-</td></tr><tr><td>Contributor</td><td>Surveillance or discrimination</td><td>discouraged-004</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License And Use Terms
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>license-001</dd><dt>Name</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Description</dt><dd><div class="long-description">Public access dataset distributed through PhysioNet under Bridge2AI Voice Registered Access License. Only registered users who sign the specified Data Use Agreement (Bridge2AI Voice Registered Access Agreement) can access files. Data covered under Certificate of Confidentiality which must be asserted against compulsory legal demands such as court orders and subpoenas. Raw audio data available through controlled access only via Data Access Compliance Office (DACO) requiring distinct application. Agreement term is two years after start date, upon completion of project, upon termination, or upon expiration of applicable ethics approval, whichever occurs first. Recipient must adhere to PhysioNet requirements managed by MIT Laboratory for Computational Physiology, supported by NIBIB under grant R01EB030362. Data shall be destroyed upon termination per provider instructions with written certification required within 30 days.
</div></dd><dt>License Terms</dt><dd><ul class='formatted-list'><li>Registered access required with institutional email</li><li>Data Use Agreement signature mandatory</li><li>Use restricted to authorized persons listed in agreement</li><li>No sharing with third parties without prior written consent</li><li>Appropriate administrative, technical, physical safeguards required</li><li>Compliance with applicable laws, rules, regulations, professional standards</li><li>Public disclosure of results encouraged in open-access journals</li><li>Recognition of data source required in publications</li><li>Certificate of Confidentiality protections apply</li><li>Raw audio requires separate controlled access application</li><li>PhysioNet platform requirements apply</li><li>Two-year term from start date or project completion</li><li>Data destruction required upon termination with written certification</li><li>No use or disclosure other than permitted by agreement</li><li>Unauthorized use must be reported within 5 business days</li><li>IRB approval required for recipient use</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üì§</span>
                    <div>
                        <h2 class="section-title">Distribution</h2>
                        <p class="section-description">How will the dataset be distributed?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License
                            
                        </label>
                        <div class="item-value">Bridge2AI Voice Registered Access License</div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîÑ</span>
                    <div>
                        <h2 class="section-title">Maintenance</h2>
                        <p class="section-description">How will the dataset be maintained?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Updates
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>updates-001</dd><dt>Name</dt><dd>Versioned releases with ongoing data collection</dd><dt>Description</dt><dd><div class="long-description">Dataset updated with versioned releases as data collection progresses. Initial release v1.0 published January 17, 2025 with 12,523 recordings from 306 participants (adult cohort only). v1.1 released January 17, 2025 adding MFCC features. v2.0.0 released April 16, 2025. v2.0.1 released August 18, 2025 (latest version). Latest version DOI: https://doi.org/10.13026/37yb-1t42. Version-specific DOIs also available. Data collection ongoing through November 30, 2026. As of v1.1, only adult cohort data available; pediatric cohort data planned for future releases with additional privacy precautions. Raw audio data access planned for future releases with enhanced security measures.
</div></dd><dt>Frequency</dt><dd>Periodic versioned releases during data collection period (September 2022 - November 2026)</dd><dt>Update Details</dt><dd><ul class='formatted-list'><li>v1.0 released January 17, 2025 - initial release with 306 participants, 12,523 recordings, adult cohort only</li><li>v1.1 released January 17, 2025 - added MFCC features (60xN dimensions)</li><li>v2.0.0 released April 16, 2025 - expanded participant cohort</li><li>v2.0.1 released August 18, 2025 - latest version</li><li>Ongoing data collection through November 30, 2026</li><li>Future releases planned with additional participants and pediatric cohort</li><li>Raw audio data access planned with additional security precautions</li><li>Version-specific documentation maintained</li><li>DOI for latest version vs version-specific DOIs</li><li>Files for older versions may be removed when superseded</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üë•</span>
                    <div>
                        <h2 class="section-title">Human Subjects</h2>
                        <p class="section-description">Does the dataset relate to people?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Human Subject Research
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>ID</dt><dd>hsr-001</dd><dt>Name</dt><dd>Bridge2AI-Voice Human Subjects Research</dd><dt>Description</dt><dd><div class="long-description">Data collection and sharing approved by University of South Florida Institutional Review Board. Participants provided written informed consent for data collection initiative and data sharing. Consent process includes authorization for voice data collection, access to medical information through EHR platforms for gold standard validation, and permission to share research data. New guidelines developed for consenting to voice data collection, voice data sharing, and utilization in context of voice AI technology. Bioethics guidance integrated throughout study design and conduct through dedicated Ethics Module. Project integrates existing scholarship, tools, and guidance with development of new standards and normative insights for identifying, anticipating, addressing, and providing guidance on ethical and trustworthy issues from voice data generation and AI/ML research through clinical adoption and downstream health decisions. Expertise drawn from law, ethics, health services, biomedical science, engineering, and scientific publications.
</div></dd><dt>Involves Human Subjects</dt><dd>True</dd><dt>IRB Approval</dt><dd><ul class='formatted-list'><li>University of South Florida Institutional Review Board approval</li></ul></dd><dt>Ethics Review Board</dt><dd><ul class='formatted-list'><li>University of South Florida Institutional Review Board</li><li>Ethics Module team providing ongoing ethical oversight</li><li>Bioethics and social science advisory teams</li></ul></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-12-20 19:24:22 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>