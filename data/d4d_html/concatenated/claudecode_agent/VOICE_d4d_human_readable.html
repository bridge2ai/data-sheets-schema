
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>VOICE Dataset Documentation - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>VOICE Dataset Documentation</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üéØ</span>
                    <div>
                        <h2 class="section-title">Motivation</h2>
                        <p class="section-description">Why was the dataset created?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Purposes
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Response</th></tr></thead><tbody><tr><td>To integrate the use of voice as a biomarker of health in clinical care by generating a substantial ...</td><td>To integrate the use of voice as a biomarker of health in clinical care by generating a substantial ...</td></tr><tr><td>To develop new standards of acoustic and voice data collection and analysis for voice AI research, i...</td><td>To develop new standards of acoustic and voice data collection and analysis for voice AI research, i...</td></tr><tr><td>To create software and cloud infrastructure for automated voice data collection through smartphone a...</td><td>To create software and cloud infrastructure for automated voice data collection through smartphone a...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Funders
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Name</th></tr></thead><tbody><tr><td>NIH Office of the Director, Bridge to Artificial Intelligence (Bridge2AI) program, grant 3OT2OD03272...</td><td>National Institutes of Health (NIH)</td></tr><tr><td>Additional infrastructure support from National Institute of Biomedical Imaging and Bioengineering (...</td><td>NIBIB</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üìä</span>
                    <div>
                        <h2 class="section-title">Composition</h2>
                        <p class="section-description">What do the instances represent?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Instances
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Instance Type</th></tr></thead><tbody><tr><td>Count: 12523

Voice and speech audio recordings from 306 participants across five clinical sites in ...</td><td>Audio recordings</td></tr><tr><td>Count: 12523

Spectrograms computed using short-time Fast Fourier Transform (FFT) with 25ms window s...</td><td>Spectrograms</td></tr><tr><td>Count: 12523

Mel-frequency cepstral coefficients (MFCCs) with 60 coefficients extracted from spectr...</td><td>MFCCs</td></tr><tr><td>Acoustic features extracted using OpenSMILE (Speech and Music Interpretation by Large-space Extracti...</td><td>Acoustic features</td></tr><tr><td>Phonetic and prosodic features computed using Parselmouth (Python interface to Praat), providing mea...</td><td>Prosodic features</td></tr><tr><td>Count: 306

Demographic data from 306 participants including de-identified geographic information (c...</td><td>Demographics</td></tr><tr><td>Count: 306

Self-reported medical history questionnaires covering health status, disease history, me...</td><td>Medical history</td></tr><tr><td>Disease-specific validated questionnaires tailored to participant's disease cohort membership (voice...</td><td>Clinical questionnaires</td></tr><tr><td>Targeted questionnaires on known confounders for voice including smoking status, vocal use patterns,...</td><td>Voice confounders</td></tr><tr><td>Electronic health record (EHR) data accessed with participant consent for gold standard validation o...</td><td>EHR data</td></tr><tr><td>Automated transcriptions generated using OpenAI's Whisper Large model. Free speech transcripts remov...</td><td>Transcriptions</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subpopulations
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Type: Geographic diversity

Multi-institutional participants recruited from five clinical sites acro...</td></tr><tr><td>Type: Disease cohort stratification

Disease cohort-based sampling targeting five categories with kn...</td></tr><tr><td>Type: DEI-focused recruitment

Intentional recruitment of diverse participants to address historical...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Distribution Formats
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Parquet file containing spectrograms with participant_id, session_id, task_name, and 513xN dimension...</td></tr><tr><td>Parquet file containing 60xN dimension MFCC arrays derived from spectrograms. Added in version 1.1 r...</td></tr><tr><td>Tab-delimited phenotype data with one row per unique participant (306 rows total), containing demogr...</td></tr><tr><td>Tab-delimited static features with one row per unique recording (12,523 rows total), containing feat...</td></tr><tr><td>Primary distribution through PhysioNet registered access system managed by MIT Laboratory for Comput...</td></tr><tr><td>Secondary distribution through Health Data Nexus platform providing alternative access point for AI-...</td></tr><tr><td>Project documentation, protocols, and software tools available through official website and GitHub r...</td></tr><tr><td>Raw audio data available through controlled access only by contacting Data Access Compliance Office ...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Participant Privacy
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value">bridge2ai-voice-dataset</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice - An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">The Bridge2AI-Voice dataset contains comprehensive voice, speech, and language data linked to health information, collected through a multi-institutional initiative funded by NIH's Bridge to Artificial Intelligence program. The dataset includes samples from conventional acoustic tasks such as respiratory sounds, cough sounds, and free speech prompts. Participants perform speaking tasks and complete self-reported demographic and medical history questionnaires, as well as disease-specific validated questionnaires. The project aims to integrate voice as a biomarker of health in clinical care by generating a substantial, ethically sourced, and diverse voice database linked to multimodal health biomarkers (EHR, radiomics, genomics) to fuel voice AI research and build predictive models for screening, diagnosis, and treatment across a broad range of diseases. Data collection is conducted via smartphone application linked to electronic health records, supported by federated learning technology to protect data privacy. Version 1.1 provides 12,523 recordings for 306 participants collected across five sites in North America. The dataset is distributed through PhysioNet and Health Data Nexus under a registered access license.


Governance model:
description: Data Access Compliance Office (DACO) oversight model with University of South Florida as Provider Institution. All data access requires DACO review and approval of Data Transfer and Use Agreement (DTUA). Recipients must have IRB approval for their research use. DACO monitors compliance with agreement terms and investigates violations.

governance_authority: University of South Florida
oversight_body: Data Access Compliance Office (DACO)
contact: DACO@b2ai-voice.org
requirements:
  - DTUA application and approval
  - Recipient IRB approval required
  - Institutional authorized official signature
  - Compliance monitoring
  - Periodic renewal
  - Violation investigation procedures
description: Provider Institution (University of South Florida) retains ultimate authority over data sharing decisions, agreement amendments, and termination. May unilaterally amend DTUA if Federal sponsor requires; recipients may object resulting in immediate termination and data destruction.

governance_authority: Provider Institution
decision_making:
  - Approval/denial of access requests
  - DTUA amendment authority
  - Unilateral amendment if Federal sponsor requires
  - Termination authority
  - Interpretation of agreement terms</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://physionet.org/content/b2ai-voice/" target="_blank">https://physionet.org/content/b2ai-voice/</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Language
                            
                        </label>
                        <div class="item-value">en</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice</li><li>speech</li><li>bridge2ai</li><li>voice biomarker</li><li>acoustic biomarker</li><li>AI</li><li>machine learning</li><li>health</li><li>disease screening</li><li>voice disorders</li><li>neurological disorders</li><li>mood disorders</li><li>respiratory disorders</li><li>pediatric</li><li>PhysioNet</li><li>federated learning</li><li>ethical AI</li><li>FAIR data</li><li>CARE principles</li><li>multimodal biomarkers</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Addressing Gaps
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Response</th></tr></thead><tbody><tr><td>Gap type: Dataset availability

Address the pressing need for large, high quality, multi-institution...</td><td>Gap type: Dataset availability

Address the pressing need for large, high quality, multi-institution...</td></tr><tr><td>Gap type: Ethical framework

Address ethical concerns about patient privacy protection, fair represe...</td><td>Gap type: Ethical framework

Address ethical concerns about patient privacy protection, fair represe...</td></tr><tr><td>Gap type: Interdisciplinary collaboration

Build bridges between the medical voice research world, a...</td><td>Gap type: Interdisciplinary collaboration

Build bridges between the medical voice research world, a...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Creators
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Subsets
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Format</th><th>ID</th><th>Name</th></tr></thead><tbody><tr><td>Parquet file (spectrograms.parquet) containing time-frequency representations with participant_id, s...</td><td></td><td>voice:spectrograms</td><td>Spectrograms</td></tr><tr><td>Parquet file (mfcc.parquet) containing 60xN dimension MFCC arrays derived from spectrograms. Added i...</td><td></td><td>voice:mfcc</td><td>Mel-frequency Cepstral Coefficients</td></tr><tr><td>Tab-delimited file (phenotype.tsv) with one row per unique participant (306 rows), containing demogr...</td><td>TSV</td><td>voice:phenotype</td><td>Phenotype Data</td></tr><tr><td>Tab-delimited file (static_features.tsv) containing features derived from raw audio using OpenSMILE,...</td><td>TSV</td><td>voice:static-features</td><td>Static Acoustic Features</td></tr><tr><td>Participants with vocal pathologies including laryngeal cancers, vocal fold paralysis, and benign la...</td><td></td><td>voice:cohort-voice-disorders</td><td>Voice Disorders Cohort</td></tr><tr><td>Participants with neurological and neurodegenerative conditions including Alzheimer's disease, Parki...</td><td></td><td>voice:cohort-neuro</td><td>Neurological Disorders Cohort</td></tr><tr><td>Participants with mood and psychiatric conditions including depression, schizophrenia, and bipolar d...</td><td></td><td>voice:cohort-mood</td><td>Mood and Psychiatric Disorders Cohort</td></tr><tr><td>Participants with respiratory conditions including pneumonia, COPD, heart failure, and obstructive s...</td><td></td><td>voice:cohort-respiratory</td><td>Respiratory Disorders Cohort</td></tr><tr><td>Pediatric participants with conditions including autism and speech delay. Not included in version 1....</td><td></td><td>voice:cohort-pediatric</td><td>Pediatric Cohort</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Sampling Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Participants selected based on membership to five predetermined disease cohort groups identified fro...</td></tr><tr><td>Multi-institutional recruitment across five sites in North America to ensure geographic diversity, s...</td></tr><tr><td>Intentional focus on recruiting diverse participants historically underrepresented in voice AI resea...</td></tr><tr><td>Patients screened at specialty clinics based on predetermined inclusion/exclusion criteria developed...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Acquisition Methods
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Was Directly Observed</th></tr></thead><tbody><tr><td>Voice recordings collected using custom tablet application with headsets at clinical sites during sc...</td><td>True</td></tr><tr><td>Structured questionnaires administered via custom data collection application on tablets, capturing ...</td><td>True</td></tr><tr><td>Electronic health record (EHR) data accessed through institutional platforms with participant consen...</td><td>False</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Mechanisms
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Hardware infrastructure including tablets with integrated headsets for standardized audio capture, a...</td></tr><tr><td>Software infrastructure including REDCap electronic data capture framework (v3.20.0), custom voice r...</td></tr><tr><td>EHR integration platforms enabling secure linkage between voice data and clinical records across par...</td></tr><tr><td>Software tool: b2aiprep
Open source library for preprocessing raw audio waveforms and merging source...</td></tr><tr><td>Software tool: Bridge2AI Voice REDCap
Custom REDCap configuration for voice data collection
Mechanis...</td></tr><tr><td>Software tool: bridge2ai-docs
Documentation dashboard and project documentation
Mechanism Type: Soft...</td></tr><tr><td>Software tool: OpenSMILE
The Munich Versatile and Fast Open-Source Audio Feature Extractor
Mechanism...</td></tr><tr><td>Software tool: Praat
Phonetic analysis software
Mechanism Type: Software

Components:
- Praat</td></tr><tr><td>Software tool: Parselmouth
Python interface to Praat for phonetic analysis
Mechanism Type: Software
...</td></tr><tr><td>Software tool: TorchAudio
Audio processing library for PyTorch
Mechanism Type: Software

Components:...</td></tr><tr><td>Software tool: OpenAI Whisper
Automatic speech recognition model (Large variant)
Mechanism Type: Sof...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Data Collectors
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Clinical research coordinators and trained study personnel at participating sites responsible for pa...</td></tr><tr><td>Automated computational systems for audio preprocessing, feature extraction, transcription, and qual...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Timeframes
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><dl class='nested-dict'><dt>Description</dt><dd>Project initiated September 1, 2022 with planned completion November 30, 2026. Ongoing data collection with periodic versioned releases.

Collection Status: Ongoing</dd><dt>Start Date</dt><dd>2022-09-01</dd><dt>End Date</dt><dd>2026-11-30</dd></dl></li><li><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Version release timeline: v1.0 released January 2024 (initial release with 306 participants, 12,523 recordings), v1.1 released January 17, 2025 (added MFCCs), v2.0.0 planned April 16, 2025, v2.0.1 planned August 18, 2025.

Release Schedule:
v1.0: January 2024
v1.1: January 17, 2025
v2.0.0: April 16, 2025 (planned)
v2.0.1: August 18, 2025 (planned)</div></dd></dl></li><li><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Most participants complete data collection in a single session. Subset of participants require multiple sessions to complete protocol, resulting in multiple sessions per participant for some individuals.

Session Structure: Single or multi-session per participant</div></dd></dl></li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Preprocessing Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Raw audio preprocessing pipeline standardizes all recordings to monaural (single-channel) format and...</td></tr><tr><td>Spectrogram extraction using short-time Fast Fourier Transform (FFT) with 25ms window size, 10ms hop...</td></tr><tr><td>Mel-frequency cepstral coefficient (MFCC) extraction with 60 coefficients computed from spectrograms...</td></tr><tr><td>Acoustic feature extraction using OpenSMILE (Speech and Music Interpretation by Large-space Extracti...</td></tr><tr><td>Phonetic and prosodic feature computation using Parselmouth (Python interface to Praat) for fundamen...</td></tr><tr><td>Automated speech transcription using OpenAI's Whisper Large model for accurate transcription of voic...</td></tr><tr><td>REDCap data export and conversion using open-source b2aiprep library (v0.21.0) for standardized extr...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Cleaning Strategies
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>HIPAA Safe Harbor de-identification method applied systematically to remove all 18 categories of ide...</td></tr><tr><td>Raw audio waveforms excluded from public releases v1.0 and v1.1 to protect participant privacy and p...</td></tr><tr><td>Free speech transcripts removed from all public releases to prevent disclosure of potentially identi...</td></tr><tr><td>Data quality control procedures including acoustic quality validation, outlier detection, completene...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Maintainers
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Long-term dataset stewardship provided through PhysioNet infrastructure maintained by MIT Laboratory...</td></tr><tr><td>Data access governance managed by University of South Florida as Provider Institution through Data A...</td></tr><tr><td>Bridge2AI-Voice Consortium led by University of South Florida maintains dataset curation, quality co...</td></tr><tr><td>Technical support and documentation maintained through project website (docs.b2ai-voice.org), GitHub...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Regulatory Restrictions
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Collaborators at other research organizations and other research teams at the same organization must apply independently for data access and sign separate Data Transfer and Use Agreement (DTUA) with Provider before accessing data. No sub-licensing or derivative access permissions.


No disclosure, release, sale, rent, lease, loan, or granting access to data to any third party except authorized persons without prior written consent of Provider. Data must be retained under recipient control at all times.


Provider may unilaterally amend DTUA if Federal sponsor requires revision. If recipient objects to amendment, DTUA immediately terminates and recipient must immediately return or destroy all data.</div></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üöÄ</span>
                    <div>
                        <h2 class="section-title">Uses</h2>
                        <p class="section-description">What (other) tasks could the dataset be used for?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Tasks
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th><th>Name</th><th>Response</th></tr></thead><tbody><tr><td>Task type: AI/ML model development

Enable AI/ML research for disease screening, diagnosis, and trea...</td><td></td><td>Task type: AI/ML model development

Enable AI/ML research for disease screening, diagnosis, and trea...</td></tr><tr><td>Task type: Biomarker discovery

Discovery and validation of novel acoustic biomarkers associated wit...</td><td></td><td>Task type: Biomarker discovery

Discovery and validation of novel acoustic biomarkers associated wit...</td></tr><tr><td>Task type: Clinical application

Development of clinical decision support tools integrating voice bi...</td><td></td><td>Task type: Clinical application

Development of clinical decision support tools integrating voice bi...</td></tr><tr><td>Task type: Multi-modal integration

Multi-modal biomarker research integrating voice with EHR, radio...</td><td></td><td>Task type: Multi-modal integration

Multi-modal biomarker research integrating voice with EHR, radio...</td></tr><tr><td>To build a multi-modal, multi-institutional, large scale, diverse and ethically sourced human voice ...</td><td>Data Acquisition Module</td><td>Aim 1: Data Acquisition Module
To build a multi-modal, multi-institutional, large scale, diverse and...</td></tr><tr><td>To introduce the field of acoustic biomarkers by developing new standards of acoustic and voice data...</td><td>Standard Module</td><td>Aim 2: Standard Module
To introduce the field of acoustic biomarkers by developing new standards of ...</td></tr><tr><td>To develop a software and cloud infrastructure for automated voice data collection through a smartph...</td><td>Tool Development and Optimization</td><td>Aim 3: Tool Development and Optimization
To develop a software and cloud infrastructure for automate...</td></tr><tr><td>To integrate existing scholarship, tools, and guidance with development of new standard and normativ...</td><td>Ethics Module</td><td>Aim 4: Ethics Module
To integrate existing scholarship, tools, and guidance with development of new ...</td></tr><tr><td>To build bridges between the medical voice research world, the acoustic engineers, and the AI/ML wor...</td><td>Teaming Module</td><td>Aim 5: Teaming Module
To build bridges between the medical voice research world, the acoustic engine...</td></tr><tr><td>To develop a unique curriculum on voice biomarkers of health and the development, validation, and im...</td><td>Skills and Workforce Development Module</td><td>Aim 6: Skills and Workforce Development Module
To develop a unique curriculum on voice biomarkers of...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Existing Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Dataset publicly released through PhysioNet and Health Data Nexus for voice AI research community ac...</td></tr><tr><td>Publication: Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., P...</td></tr><tr><td>Publication: Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., ...</td></tr><tr><td>Publication: Sigaras, A., Zisimopoulos, P., Tang, J., Bevers, I., Gallois, H., Bernier, A., Bensouss...</td></tr><tr><td>Publication: Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A.,...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Future Use Impacts
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Voice biomarker discovery for disease screening and diagnosis may enable earlier detection, non-inva...</td></tr><tr><td>Multi-modal AI model development integrating voice with EHR, genomics, and imaging data may provide ...</td></tr><tr><td>Federated learning applications may enable privacy-preserving collaborative research across institut...</td></tr><tr><td>Commercial voice AI applications (e.g., smartphone-based screening) may increase accessibility but r...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Intended Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Description</th></tr></thead><tbody><tr><td>Use case: AI/ML model development

Development and validation of AI/ML models for voice-based diseas...</td></tr><tr><td>Use case: Biomarker discovery

Discovery and validation of novel acoustic biomarkers associated with...</td></tr><tr><td>Use case: Clinical decision support

Development of clinical decision support tools integrating voic...</td></tr><tr><td>Use case: Multi-modal data integration

Multi-modal biomarker research integrating voice with EHR, r...</td></tr><tr><td>Use case: Federated learning research

Federated learning applications for privacy-preserving collab...</td></tr><tr><td>Use case: Standards development

Development of standards, best practices, and quality measures for ...</td></tr><tr><td>Use case: Workforce development

Education and training of interdisciplinary researchers in voice bi...</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Discouraged Uses
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Role</th><th>Name</th><th>ORCID</th><th>Affiliation</th></tr></thead><tbody><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr><tr><td>Contributor</td><td></td><td>-</td><td>-</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License And Use Terms
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Bridge2AI Voice Registered Access License with Data Transfer and Use Agreement (DTUA) required for all data access. Registered users must sign DTUA and obtain approval from Data Access Compliance Office (DACO) before accessing files. Recipients must establish administrative, technical, and physical safeguards to protect Personally Identifiable Information (PII) per OMB M-07-16 and ensure only authorized persons access data. Data provided "AS IS" without warranties of any kind. Recipients assume all liability for use, storage, disclosure, or disposal. No unauthorized disclosure to third parties; collaborators must apply independently. Attribution required citing both dataset DOI and PhysioNet platform. Commercial use allowed under DTUA terms. Recipients may retain derivative works with proper attribution and may publish results (open-access encouraged). Two-year use period from DTUA start date upon completion of project, expiration of ethics approval, or termination, whichever occurs first; renewable with Provider approval. One archival copy allowed for records retention compliance. Provider Institution (University of South Florida) may unilaterally amend if Federal sponsor requires; recipient may object resulting in immediate termination. Certificate of Confidentiality protections apply and must be asserted against compulsory legal demands. DTUA approved for use through August 31, 2025.


Citation requirements:
description: Primary dataset citation required for all publications, presentations, and other uses of data. Should cite specific version used (via version DOI) for reproducibility.

citation_type: Dataset citation
format: Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155

description: PhysioNet platform citation required as standard acknowledgment of infrastructure supporting data distribution.

citation_type: Platform citation
format: Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215‚Äìe220. RRID:SCR_007345.

description: Recipient agrees to recognize contribution of Provider as source of data in all written, visual, or oral public disclosures of research using data, as appropriate in accordance with scholarly standards.

citation_type: Attribution requirement
policy: Provider recognition required in all public disclosures
description: Recipients encouraged to make results publicly available in open-access journals or pre-print servers where possible to maximize scientific dissemination and reproducibility.

citation_type: Publication encouragement
policy: Open-access publication encouraged
License Name: Bridge2AI Voice Registered Access License
Agreement Required: Data Transfer and Use Agreement (DTUA)
Approval Authority: Data Access Compliance Office (DACO)
Provider Institution: University of South Florida Board of Trustees
Effective Through: August 31, 2025

Key Terms:
- Registered access with DACO approval required
- Data classified as Personally Identifiable Information (PII, OMB M-07-16)
- Administrative, technical, physical safeguards required
- Certificate of Confidentiality protections (must assert against legal demands)
- Data provided "AS IS" without warranties
- Recipients assume liability for use
- No unauthorized third-party disclosure
- Collaborators apply independently
- Attribution requirements (dataset DOI + PhysioNet)
- Commercial use allowed
- Open-access publication encouraged
- Two-year use period (renewable)
- Archival copy allowed for records retention
- Provider may amend if Federal sponsor requires
- Termination results in data destruction (certification required)</div></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üì§</span>
                    <div>
                        <h2 class="section-title">Distribution</h2>
                        <p class="section-description">How will the dataset be distributed?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            License
                            
                        </label>
                        <div class="item-value">Bridge2AI Voice Registered Access License</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Version Access
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">All versions available through PhysioNet with version-specific DOIs for citability and reproducibility. Latest version DOI always points to most recent release. Users can access specific versions for replication or access latest version for most current data.

Version Doi 1 0: https://doi.org/10.57764/qb6h-em84
Version Doi 1 1: https://doi.org/10.13026/249v-w155
Latest Doi: https://doi.org/10.13026/37yb-1t42
Current Latest Version: 2.0.1</div></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîÑ</span>
                    <div>
                        <h2 class="section-title">Maintenance</h2>
                        <p class="section-description">How will the dataset be maintained?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Version
                            
                        </label>
                        <div class="item-value">1.1</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Updates
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Description</dt><dd><div class="long-description">Update frequency and schedule:
Ongoing data collection with periodic versioned releases. Major releases planned approximately every 6-12 months during active project period (2022-2026). Future releases will include additional participants from adult cohorts, pediatric cohort data (currently not included), and potentially raw audio waveforms with enhanced privacy protections. Post-project maintenance will continue through PhysioNet infrastructure with updates as needed for corrections, documentation, and community contributions.



Version 1.0:
Initial release of Bridge2AI-Voice dataset with 12,523 recordings from 306 participants across five clinical sites. Included spectrograms, acoustic features, phenotype data, and static features.


Version 1.1:
Added Mel-frequency cepstral coefficients (MFCCs) with 60 coefficients per recording, providing additional perceptually-relevant acoustic features commonly used in speech and voice analysis.


Version 2.0.0:
Planned future release with additional participants, enhanced features, and expanded cohorts. Details to be announced.


Version 2.0.1:
Planned maintenance release with bug fixes, documentation updates, and minor enhancements. Currently the latest version available.</div></dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-12-15 23:11:22 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>