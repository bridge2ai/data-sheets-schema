
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>physionet b2ai-voice 1.1 d4d - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>physionet b2ai-voice 1.1 d4d</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üéØ</span>
                    <div>
                        <h2 class="section-title">Motivation</h2>
                        <p class="section-description">Why was the dataset created?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Motivation
                            
                        </label>
                        <div class="item-value">Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice representations to demographic, clinical, and questionnaire data.</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Funding And Acknowledgements
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Funding</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Agency</dt><dd>National Institutes of Health</dd><dt>Award Number</dt><dd>3OT2OD032720-01S1</dd><dt>Project Title</dt><dd>Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database to understand disease like never before</dd></dl></li></ul></dd><dt>Acknowledgements</dt><dd>We acknowledge the contribution of study participants and the NIH for continued support of the project.</dd><dt>Platform Support</dt><dd>National Institute of Biomedical Imaging and Bioengineering under NIH grant number R01EB030362 supported PhysioNet infrastructure.</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Dataset Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Dataset Acronym
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Project
                            
                        </label>
                        <div class="item-value">VOICE</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Publisher
                            
                        </label>
                        <div class="item-value">PhysioNet</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Release Date
                            
                        </label>
                        <div class="item-value">2025-01-17</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Rrid
                            
                        </label>
                        <div class="item-value">SCR_007345</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Project Website
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Topics
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice</li><li>bridge2ai</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Authors
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Name</th></tr></thead><tbody><tr><td>Alistair Johnson</td></tr><tr><td>Jean-Christophe B√©lisle-Pipon</td></tr><tr><td>David Dorr</td></tr><tr><td>Satrajit Ghosh</td></tr><tr><td>Philip Payne</td></tr><tr><td>Maria Powell</td></tr><tr><td>Anais Rameau</td></tr><tr><td>Vardit Ravitsky</td></tr><tr><td>Alexandros Sigaras</td></tr><tr><td>Olivier Elemento</td></tr><tr><td>Yael Bensoussan</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Corresponding Author
                            
                        </label>
                        <div class="item-value">Not publicly listed; contact information requires login.</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Abstract
                            
                        </label>
                        <div class="item-value"><div class="long-description">The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. Recent advances in artificial intelligence have provided techniques to extract previously unknown prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. Here we present Bridge2AI-Voice, a comprehensive collection of data derived from voice recordings with corresponding clinical information. Bridge2AI-Voice v1.0, the initial release, provides 12,523 recordings for 306 participants collected across five sites in North America. Participants were selected based on known conditions which manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and respiratory disorders. The initial release contains data considered low risk, including derivations such as spectrograms but not the original voice recordings. Detailed demographic, clinical, and validated questionnaire data are also made available.</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Composition
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Overview</dt><dd>Derived audio representations and associated phenotype data from adult participants recruited at specialty clinics.</dd><dt>Population</dt><dd><dl class='nested-dict'><dt>Cohort Scope</dt><dd>Adult cohort only as of v1.1</dd><dt>Recruitment Region</dt><dd>Five sites in North America</dd><dt>Participants</dt><dd>306</dd><dt>Recordings</dt><dd>12,523</dd></dl></dd><dt>Condition Groups</dt><dd><ul class='formatted-list'><li>Voice disorders</li><li>Neurological and neurodegenerative disorders</li><li>Mood and psychiatric disorders</li><li>Respiratory disorders</li><li>Pediatric voice and speech disorders (planned; not included in v1.1)</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Data Characteristics
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Modalities</dt><dd><ul class='formatted-list'><li>Spectrograms derived from audio</li><li>Mel-frequency cepstral coefficients</li><li>Acoustic feature sets (openSMILE)</li><li>Phonetic and prosodic features (Parselmouth and Praat)</li><li>Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)</li><li>Phenotype and questionnaire data</li></ul></dd><dt>Data Formats</dt><dd><ul class='formatted-list'><li>Parquet</li><li>TSV</li><li>JSON</li></ul></dd><dt>Identifiers In Files</dt><dd><ul class='formatted-list'><li>participant_id</li><li>session_id</li><li>task_name</li></ul></dd><dt>Sampling And Dimensions</dt><dd>Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional to recording length.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Collection Process
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Setting</dt><dd>Specialty clinics and institutions</dd><dt>Participant Selection</dt><dd>Screened for inclusion and exclusion criteria within five predetermined groups.</dd><dt>Consent</dt><dd>Participants provided consent for data collection and sharing of de-identified research data.</dd><dt>Procedure</dt><dd>Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice, disease specific information, and voice tasks such as sustained vowel phonation.</dd><dt>Data Capture</dt><dd>Custom tablet application used for collection; headset used when possible.</dd><dt>Sessions</dt><dd>Most participants completed one session; a subset required multiple sessions.</dd><dt>Data Export And Merge</dt><dd>Exported from REDCap and converted using an open source library.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Preprocessing And Derived Data
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Raw Audio Processing</dt><dd>Converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter.</dd><dt>Spectrograms</dt><dd>Short-time FFT with 25 ms window, 10 ms hop, 512-point FFT; stored in power representation.</dd><dt>Mfcc</dt><dd>60 coefficients computed from spectrograms.</dd><dt>Acoustic Features</dt><dd>Extracted using openSMILE capturing temporal dynamics and acoustic characteristics.</dd><dt>Phonetic Prosodic Features</dt><dd>Computed using Parselmouth and Praat; includes measures of fundamental frequency, formants, and voice quality.</dd><dt>Transcription</dt><dd>Generated using OpenAI Whisper Large; transcripts of free speech audio were removed prior to release.</dd><dt>Open Source Code</dt><dd>b2aiprep library used to preprocess waveforms and merge phenotype data.</dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Files
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Version Notice</dt><dd>Files for version 1.1 are no longer available; the latest version of this project is 2.0.1.</dd><dt>Listing</dt><dd><table class="data-table"><thead><tr><th>Description</th><th>Path</th><th>Type</th></tr></thead><tbody><tr><td>Dense time-frequency representations derived from voice waveforms; includes participant_id, session_...</td><td>spectrograms.parquet</td><td>Parquet</td></tr><tr><td>Mel-frequency cepstral coefficients derived from spectrograms; arrays of size 60 x N per recording.</td><td>mfcc.parquet</td><td>Parquet</td></tr><tr><td>One row per participant; demographics, acoustic confounders, and responses to validated questionnair...</td><td>phenotype.tsv</td><td>TSV</td></tr><tr><td>Data dictionary for phenotype.tsv with one sentence descriptions per column.</td><td>phenotype.json</td><td>JSON</td></tr><tr><td>One row per audio recording; features derived using openSMILE, Praat, parselmouth, and torchaudio.</td><td>static_features.tsv</td><td>TSV</td></tr><tr><td>Data dictionary for static_features.tsv with feature descriptions.</td><td>static_features.json</td><td>JSON</td></tr></tbody></table></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Limitations
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>Adult cohort only in v1.1; pediatric data not included.</li><li>No raw audio is released in v1.1; analyses are limited to derived representations.</li><li>Participants were selected based on conditions known to manifest in voice, which may affect generalizability.</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Software And Tools
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Preprocessing Code</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>b2aiprep</dd><dt>URL</dt><dd><a href="https://github.com/sensein/b2aiprep" target="_blank">https://github.com/sensein/b2aiprep</a></dd><dt>Description</dt><dd>Open source library used to preprocess raw audio and merge phenotype data.</dd></dl></dd><dt>Referenced Tools</dt><dd><ul class='formatted-list'><li>openSMILE</li><li>Praat</li><li>Parselmouth</li><li>torchaudio</li><li>OpenAI Whisper Large</li><li>librosa (example usage for visualization)</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Variables And Fields
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Common Identifiers</dt><dd><ul class='formatted-list'><li>participant_id</li><li>session_id</li><li>task_name</li></ul></dd><dt>Spectrograms</dt><dd><dl class='nested-dict'><dt>Dimensions</dt><dd>513 x N</dd><dt>Description</dt><dd>Power spectrogram per recording.</dd></dl></dd><dt>Mfcc</dt><dd><dl class='nested-dict'><dt>Dimensions</dt><dd>60 x N</dd><dt>Description</dt><dd>Mel-frequency cepstral coefficients derived from spectrograms.</dd></dl></dd><dt>Phenotype</dt><dd><dl class='nested-dict'><dt>Granularity</dt><dd>One row per participant</dd><dt>Contents</dt><dd>Demographics, acoustic confounders, validated questionnaire responses.</dd></dl></dd><dt>Static Features</dt><dd><dl class='nested-dict'><dt>Granularity</dt><dd>One row per recording</dd><dt>Contents</dt><dd>Acoustic, phonetic, and prosodic features from multiple toolkits.</dd></dl></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Release Notes
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Date</th><th>Notes</th><th>Version</th></tr></thead><tbody><tr><td>2025-01-17</td><td>This release added Mel-frequency cepstral coefficients.</td><td>1.1</td></tr><tr><td>2024</td><td>Initial release of the dataset.</td><td>1.0</td></tr></tbody></table></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Citations
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Dataset Citation</dt><dd><div class="long-description">Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155</div></dd><dt>Platform Citation</dt><dd><div class="long-description">Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215‚Äìe220. RRID:SCR_007345.</div></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            References
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li><div class="long-description">Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D., Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926</div></li><li><div class="long-description">Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D., Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P., Tang, J., Sigaras, A., Elemento, O., Dorr, D., ‚Ä¶ Bridge2AI-Voice. (2024). Bridge2AI Voice REDCap (v3.20.0). Zenodo. https://doi.org/10.5281/zenodo.14148755</div></li><li><div class="long-description">Florian Eyben, Martin W√∂llmer, Bj√∂rn Schuller: openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010.</div></li><li>Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.</li><li>Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics. 2018 Nov 1;71:1-5.</li><li><div class="long-description">Hwang, J., Hira, M., Chen, C., Zhang, X., Ni, Z., Sun, G., Ma, P., Huang, R., Pratap, V., Zhang, Y., Kumar, A., Yu, C.-Y., Zhu, C., Liu, C., Kahn, J., Ravanelli, M., Sun, P., Watanabe, S., Shi, Y., Tao, T., Scheibler, R., Cornell, S., Kim, S., & Petridis, S. (2023). TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch. arXiv preprint arXiv:2310.17864</div></li><li><div class="long-description">Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-B√©lair, V, & Shi, Y. (2021). TorchAudio: Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018.</div></li><li>Bevers, I., Ghosh, S., Johnson, A., Brito, R., Bedrick, S., Catania, F., & Ng, E. (2017). My Research Software (Version 0.21.0) [Computer software]. https://github.com/sensein/b2aiprep</li><li><div class="long-description">Johnson, A., B√©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84</div></li></ul></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üöÄ</span>
                    <div>
                        <h2 class="section-title">Uses</h2>
                        <p class="section-description">What (other) tasks could the dataset be used for?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Intended Uses
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Primary</dt><dd>Artificial intelligence and clinical research on voice as a biomarker of health.</dd><dt>Examples</dt><dd><ul class='formatted-list'><li>Development and benchmarking of models to associate voice-derived features with health conditions.</li><li>Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data.</li></ul></dd><dt>Usage Notes</dt><dd>Data are provided as derived representations without raw audio to reduce re-identification risk.</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üì§</span>
                    <div>
                        <h2 class="section-title">Distribution</h2>
                        <p class="section-description">How will the dataset be distributed?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            DOI
                            
                        </label>
                        <div class="item-value">10.13026/249v-w155</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            DOI URL
                            
                        </label>
                        <div class="item-value"><a href="https://doi.org/10.13026/249v-w155" target="_blank">https://doi.org/10.13026/249v-w155</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Latest Version DOI
                            
                        </label>
                        <div class="item-value">10.13026/37yb-1t42</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Access And Licensing
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Platform</dt><dd>PhysioNet</dd><dt>Access Policy</dt><dd>Restricted Access</dd><dt>Access Conditions</dt><dd>Only registered users who sign the specified data use agreement can access the files.</dd><dt>License</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Data Use Agreement</dt><dd>Bridge2AI Voice Registered Access Agreement</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîÑ</span>
                    <div>
                        <h2 class="section-title">Maintenance</h2>
                        <p class="section-description">How will the dataset be maintained?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Dataset Version
                            
                        </label>
                        <div class="item-value">1.1</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Versions Available On Platform
                            
                        </label>
                        <div class="item-value"><table class="data-table"><thead><tr><th>Date</th><th>Version</th></tr></thead><tbody><tr><td>2025-01-17</td><td>1.1</td></tr><tr><td>2025-04-16</td><td>2.0.0</td></tr><tr><td>2025-08-18</td><td>2.0.1</td></tr></tbody></table></div>
                    </div>
                    
                </div>
            </div>
            
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üë•</span>
                    <div>
                        <h2 class="section-title">Human Subjects</h2>
                        <p class="section-description">Does the dataset relate to people?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Deidentification And Privacy
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>Approach</dt><dd>HIPAA Safe Harbor</dd><dt>Actions</dt><dd><ul class='formatted-list'><li>Removal of HIPAA Safe Harbor identifiers.</li><li>Removal of state and province; retention of country of data collection.</li><li>Removal of transcripts of free speech audio.</li><li>Omission of raw audio waveforms in v1.1; only spectrograms and other derived features are released.</li></ul></dd><dt>Examples Of Identifiers Removed</dt><dd><ul class='formatted-list'><li>Names</li><li>Geographic locators</li><li>Dates at finer than year resolution</li><li>Phone and fax numbers</li><li>Email addresses</li><li>IP addresses</li><li>Social Security Numbers</li><li>Medical record numbers</li><li>Health plan beneficiary numbers</li><li>Device identifiers</li><li>License numbers</li><li>Account numbers</li><li>Vehicle identifiers</li><li>Website URLs</li><li>Full face photos</li><li>Biometric identifiers</li><li>Any unique identifiers</li></ul></dd></dl></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Ethics
                            
                        </label>
                        <div class="item-value"><dl class='nested-dict'><dt>IRB Approval</dt><dd>Data collection and sharing approved by the University of South Florida Institutional Review Board.</dd><dt>Ethical Position</dt><dd>Dataset is ethically sourced with privacy protections; derived data released for low risk.</dd><dt>Conflicts Of Interest</dt><dd>None to declare.</dd></dl></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-11-08 22:21:58 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>