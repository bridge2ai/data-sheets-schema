# D4D Metadata for Bridge2AI-Voice Dataset
# Sources:
#   - https://physionet.org/content/b2ai-voice/1.1/
#   - https://healthdatanexus.ai/content/b2ai-voice/1.0/
#   - https://docs.b2ai-voice.org/
# Column: VOICE
# Generated: 2025-11-07

id: "b2ai-voice-v1.1"
dataset_name: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information"
dataset_acronym: "Bridge2AI-Voice"
name: "Bridge2AI-Voice"
project: "VOICE"
publisher: "PhysioNet"
dataset_version: "1.1"
release_date: "2025-01-17"
doi: "10.13026/249v-w155"
doi_url: "https://doi.org/10.13026/249v-w155"
latest_version_doi: "10.13026/37yb-1t42"
rrid: "SCR_007345"
project_website: "https://docs.b2ai-voice.org"

topics:
  - "voice"
  - "biomarker"
  - "health"
  - "bridge2ai"
  - "artificial intelligence"
  - "neurological disorders"
  - "voice disorders"
  - "mood disorders"
  - "respiratory disorders"

authors:
  - name: "Alistair Johnson"
  - name: "Jean-Christophe Bélisle-Pipon"
  - name: "David Dorr"
  - name: "Satrajit Ghosh"
  - name: "Philip Payne"
  - name: "Maria Powell"
  - name: "Anais Rameau"
  - name: "Vardit Ravitsky"
  - name: "Alexandros Sigaras"
  - name: "Olivier Elemento"
  - name: "Yael Bensoussan"

corresponding_author: "Contact through DACO@b2ai-voice.org for data access questions"

abstract: |
  The human voice contains complex acoustic markers which have been linked to important health conditions
  including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising
  characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility.
  Recent advances in artificial intelligence have provided techniques to extract previously unknown
  prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project
  seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence
  and support critical insights into the use of voice as a biomarker of health.

  Here we present Bridge2AI-Voice v1.1, a comprehensive collection of data derived from voice recordings
  with corresponding clinical information. The dataset provides 12,523 recordings for 306 participants
  collected across five sites in North America. Participants were selected based on known conditions which
  manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and
  respiratory disorders. This release contains data considered low risk, including derivations such as
  spectrograms and MFCC but not the original voice recordings. Detailed demographic, clinical, and validated
  questionnaire data are also made available.

motivation: |
  Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice
  representations to demographic, clinical, and questionnaire data. The project addresses the critical need
  for comprehensive, ethically-collected voice datasets that can advance artificial intelligence research
  while maintaining the highest standards of participant privacy and data protection. By focusing on multiple
  disease cohorts and using standardized collection protocols, the dataset fills a significant gap in
  available resources for voice-based health biomarker research.

composition:
  overview: |
    Derived audio representations and associated phenotype data from adult participants recruited at specialty
    clinics across five North American sites.

  population:
    cohort_scope: "Adult cohort only as of v1.1"
    recruitment_region: "Five sites in North America"
    participants: 306
    recordings: 12523

  condition_groups:
    - "Voice disorders (laryngeal pathologies)"
    - "Neurological and neurodegenerative disorders"
    - "Mood and psychiatric disorders"
    - "Respiratory disorders"
    - "Pediatric voice and speech disorders (planned; not included in v1.1)"

  missing_data: |
    Raw audio waveforms are excluded from v1.1 release for privacy protection. Free speech transcripts
    are removed for participant privacy. Pediatric cohort data is limited in v1.1. HIPAA Safe Harbor
    identifiers have been removed, including state/province data (country of collection retained).

data_characteristics:
  modalities:
    - "Spectrograms derived from audio (513×N dimensions)"
    - "Mel-frequency cepstral coefficients (60×N dimensions)"
    - "Acoustic feature sets extracted using OpenSMILE"
    - "Phonetic and prosodic features from Parselmouth and Praat"
    - "Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)"
    - "Phenotype and questionnaire data"

  data_formats:
    - "Parquet"
    - "TSV"
    - "JSON"

  identifiers_in_files:
    - "participant_id"
    - "session_id"
    - "task_name"

  sampling_and_dimensions: |
    Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional
    to recording length.

collection_process:
  setting: "Specialty clinics and institutions across five North American sites"

  participant_selection: |
    Participants were screened for inclusion and exclusion criteria within five predetermined clinical
    condition groups: voice disorders, neurological/neurodegenerative disorders, mood/psychiatric disorders,
    respiratory disorders, and pediatric conditions.

  consent: |
    Participants provided informed consent for data collection and sharing of de-identified research data.
    Data collection and sharing was approved by the University of South Florida Institutional Review Board
    and submitted to the University of Toronto Research Ethics Board.

  procedure: |
    Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice,
    disease-specific information, and voice tasks such as sustained vowel phonation. The protocol was
    designed through a team science approach involving clinical expertise, bioethics, standards, and
    diversity/equity/inclusion considerations.

  data_capture: |
    Custom tablet application used for collection with headset when possible. Tasks included sustained
    phonation of vowel sounds and free speech recordings.

  sessions: |
    Most participants completed one session; a subset required multiple sessions to complete all data
    collection procedures.

  data_export_and_merge: |
    Data exported from REDCap and converted using the b2aiprep open source library available at
    https://github.com/sensein/b2aiprep.

preprocessing_and_derived_data:
  raw_audio_processing: |
    Raw audio converted to monaural (mono) and resampled to 16 kHz with a Butterworth anti-aliasing filter
    to prevent frequency artifacts.

  spectrograms: |
    Short-time Fourier Transform (FFT) applied with 25 ms window, 10 ms hop length, and 512-point FFT.
    Spectrograms stored in power representation format (513×N dimensions where N varies by recording length).

  mfcc: |
    60 Mel-frequency cepstral coefficients computed from spectrograms, providing compact acoustic feature
    representations (60×N dimensions).

  acoustic_features: |
    Comprehensive acoustic features extracted using the OpenSMILE toolkit, capturing temporal dynamics and
    acoustic characteristics across multiple time scales.

  phonetic_prosodic_features: |
    Phonetic and prosodic features computed using Parselmouth (Python interface to Praat) and Praat directly.
    Features include measurements of fundamental frequency (F0), formants (F1, F2, F3), and voice quality
    measures.

  transcription: |
    Automated transcriptions generated using OpenAI's Whisper Large model for speech-to-text conversion.
    Transcripts of free speech audio were removed prior to release to protect participant privacy and reduce
    re-identification risk.

  open_source_code: |
    The b2aiprep library (https://github.com/sensein/b2aiprep) was used to preprocess waveforms and merge
    phenotype data. All preprocessing code is available as open source.

deidentification_and_privacy:
  approach: "HIPAA Safe Harbor"

  actions:
    - "Removal of all 18 HIPAA Safe Harbor identifier categories"
    - "Removal of state and province; retention of country of data collection only"
    - "Removal of transcripts of free speech audio to prevent content-based re-identification"
    - "Omission of raw audio waveforms in v1.1; only spectrograms and derived features are released"
    - "Removal of dates at finer than year resolution"

  examples_of_identifiers_removed:
    - "Names"
    - "Geographic locators (state/province level)"
    - "Dates at finer than year resolution"
    - "Phone and fax numbers"
    - "Email addresses"
    - "IP addresses"
    - "Social Security Numbers"
    - "Medical record numbers"
    - "Health plan beneficiary numbers"
    - "Device identifiers and serial numbers"
    - "License numbers"
    - "Account numbers"
    - "Vehicle identifiers"
    - "Website URLs"
    - "Full face photographs"
    - "Biometric identifiers"
    - "Any other unique identifying numbers or codes"

files:
  version_notice: |
    Files for version 1.1 are no longer the latest; the project has been updated to version 2.0.1 as of
    August 2025. The files listed below describe v1.1 contents.

  listing:
    - path: "spectrograms.parquet"
      type: "Parquet"
      description: |
        Dense time-frequency representations derived from voice waveforms. Columns include participant_id,
        session_id, task_name, and spectrogram arrays of size 513 x N.

    - path: "mfcc.parquet"
      type: "Parquet"
      description: |
        Mel-frequency cepstral coefficients derived from spectrograms. Arrays of size 60 x N per recording,
        providing compact acoustic representations.

    - path: "phenotype.tsv"
      type: "TSV"
      description: |
        One row per participant containing demographics, acoustic confounders, and responses to validated
        questionnaires. See phenotype.json for data dictionary.

    - path: "phenotype.json"
      type: "JSON"
      description: |
        Data dictionary for phenotype.tsv providing one-sentence descriptions for each column and variable
        definitions.

    - path: "static_features.tsv"
      type: "TSV"
      description: |
        One row per audio recording containing features derived using OpenSMILE, Praat, Parselmouth, and
        TorchAudio toolkits.

    - path: "static_features.json"
      type: "JSON"
      description: |
        Data dictionary for static_features.tsv with detailed feature descriptions and extraction methods.

intended_uses:
  primary: |
    Artificial intelligence and clinical research on voice as a biomarker of health. Suitable for developing
    and benchmarking machine learning models that associate voice-derived features with health conditions.

  examples:
    - "Development and benchmarking of AI models to predict health conditions from voice features"
    - "Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data"
    - "Research on voice-based biomarkers for neurological, psychiatric, and respiratory conditions"
    - "Multi-modal analysis combining voice features with clinical and demographic data"
    - "Development of standardized protocols for voice biomarker research"

  usage_notes: |
    Data are provided as derived representations (spectrograms, MFCC, extracted features) without raw audio
    to reduce re-identification risk. Researchers should be aware that analyses are limited to these derived
    representations in v1.1.

limitations:
  - "Adult cohort only in v1.1; pediatric data not included in this version"
  - "No raw audio waveforms released in v1.1; analyses limited to derived representations"
  - "Participants selected based on conditions known to manifest in voice, which may affect generalizability to healthy populations"
  - "Data collected at specialty clinics may not be representative of community-based populations"
  - "Free speech transcripts removed for privacy, limiting certain types of linguistic analysis"
  - "Geographic data limited to country-level to comply with HIPAA Safe Harbor de-identification"

access_and_licensing:
  platform: "PhysioNet"
  access_policy: "Restricted Access"

  access_conditions: |
    Only registered users who sign the specified Data Use Agreement can access the files. Users must:
    1. Create a PhysioNet account
    2. Complete required training (TCPS 2: CORE 2022)
    3. Sign the Bridge2AI Voice Registered Access Agreement
    4. Agree to use data only for research purposes

  license: "Bridge2AI Voice Registered Access License"
  data_use_agreement: "Bridge2AI Voice Registered Access Agreement"

  raw_audio_access: |
    Raw audio waveforms are not included in the public PhysioNet release. Researchers requiring raw audio
    for specific approved purposes may submit controlled access requests to DACO@b2ai-voice.org.

ethics:
  irb_approval: |
    Data collection and sharing approved by the University of South Florida Institutional Review Board (IRB).
    Protocol also submitted to the University of Toronto Research Ethics Board.

  ethical_position: |
    Dataset is ethically sourced with extensive privacy protections. Derived data released as low-risk
    category. The project emphasizes demographic diversity, ethical data collection practices, and transparent
    documentation of methods and limitations.

  participant_consent: |
    All participants provided informed consent for data collection and sharing of de-identified research data.
    Consent process included explanation of data use, privacy protections, and participant rights.

  conflicts_of_interest: "None to declare"

funding_and_acknowledgements:
  funding:
    - agency: "National Institutes of Health (NIH)"
      award_number: "3OT2OD032720-01S1"
      project_title: |
        Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database
        to understand disease like never before

  acknowledgements: |
    We acknowledge the essential contribution of study participants who generously donated their time and
    data to advance health research. We thank the NIH for continued support of the Bridge2AI program and
    the multidisciplinary team of clinicians, researchers, bioethicists, and community stakeholders who
    contributed to protocol development.

  platform_support: |
    PhysioNet infrastructure supported by the National Institute of Biomedical Imaging and Bioengineering
    (NIBIB) under NIH grant number R01EB030362.

software_and_tools:
  preprocessing_code:
    name: "b2aiprep"
    url: "https://github.com/sensein/b2aiprep"
    description: |
      Open source Python library used to preprocess raw audio waveforms and merge phenotype data from REDCap exports.

  referenced_tools:
    - name: "OpenSMILE"
      description: "Munich versatile and fast open-source audio feature extractor"
    - name: "Praat"
      description: "Phonetic analysis software for speech and voice analysis"
    - name: "Parselmouth"
      description: "Python interface to Praat for programmatic phonetic analysis"
    - name: "TorchAudio"
      description: "PyTorch library for audio processing and feature extraction"
    - name: "OpenAI Whisper Large"
      description: "State-of-the-art speech recognition model for transcription generation"
    - name: "librosa"
      description: "Python library for audio analysis (referenced in usage examples)"

variables_and_fields:
  common_identifiers:
    - "participant_id - Unique de-identified participant identifier"
    - "session_id - Unique session identifier"
    - "task_name - Voice task type (e.g., sustained vowel, free speech)"

  spectrograms:
    dimensions: "513 x N"
    description: |
      Power spectrogram per recording. Each element represents energy in a specific frequency bin at a
      specific time point. N varies based on recording length.

  mfcc:
    dimensions: "60 x N"
    description: |
      Mel-frequency cepstral coefficients derived from spectrograms. Provides compact representation of
      spectral envelope, commonly used in speech and audio processing.

  phenotype:
    granularity: "One row per participant"
    contents: |
      Demographics (age, gender, ethnicity), acoustic confounders (hearing status, language background),
      validated questionnaire responses (health-related quality of life, symptom severity).

  static_features:
    granularity: "One row per recording"
    contents: |
      Acoustic features: Energy, pitch, spectral characteristics, voice quality measures
      Phonetic features: Formant frequencies, fundamental frequency statistics
      Prosodic features: Speaking rate, pause patterns, intonation measures
      Extracted using OpenSMILE, Praat, Parselmouth, and TorchAudio toolkits.

release_notes:
  - version: "1.1"
    date: "2025-01-17"
    notes: |
      This release added Mel-frequency cepstral coefficients (MFCC) to the dataset, providing additional
      derived acoustic representations for analysis.

  - version: "1.0"
    date: "2024"
    notes: |
      Initial public release of the Bridge2AI-Voice dataset with spectrograms, acoustic features, phenotype
      data, and validated questionnaire responses for 306 adult participants.

versions_available_on_platform:
  - version: "1.1"
    date: "2025-01-17"
    doi: "10.13026/249v-w155"
  - version: "2.0.0"
    date: "2025-04-16"
    notes: "Major update with additional participants and features"
  - version: "2.0.1"
    date: "2025-08-18"
    notes: "Latest version - bug fixes and minor improvements"
    doi: "10.13026/37yb-1t42"

citations:
  dataset_citation: |
    Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V.,
    Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse
    voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345.
    https://doi.org/10.13026/249v-w155

  platform_citation: |
    Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., Mietus, J. E.,
    Moody, G. B., Peng, C. K., & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet:
    Components of a new research resource for complex physiologic signals. Circulation [Online].
    101 (23), pp. e215–e220. RRID:SCR_007345.

references:
  - citation: |
      Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M.,
      Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J.,
      Awan, S., Bolser, D., Bensoussan, Y. (2024). Developing Multi-Disorder Voice Protocols: A team
      science approach involving clinical expertise, bioethics, standards, and DEI. Proc. Interspeech
      2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926

  - citation: |
      Bensoussan, Y., Ghosh, S. S., Rameau, A., et al. (2024). Bridge2AI Voice REDCap (v3.20.0).
      Zenodo. https://doi.org/10.5281/zenodo.14148755

  - citation: |
      Florian Eyben, Martin Wöllmer, Björn Schuller: openSMILE - The Munich Versatile and Fast
      Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy,
      ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010.

  - citation: |
      Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.

  - citation: |
      Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat.
      Journal of Phonetics. 2018 Nov 1;71:1-5.
