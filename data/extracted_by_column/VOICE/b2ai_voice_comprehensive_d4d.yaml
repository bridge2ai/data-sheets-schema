# D4D Metadata for Bridge2AI-Voice Dataset
# Sources:
#   - https://physionet.org/content/b2ai-voice/1.1/
#   - https://healthdatanexus.ai/content/b2ai-voice/1.0/
#   - Bridge2AI Data Transfer and Use Agreement (Google Drive PDF)
#   - Bridge2AI Voice IRB Protocol v14 (Google Doc)
# Column: VOICE
# Generated: 2025-11-07
# Updated: 2025-11-07 with DUA and IRB details

id: "b2ai-voice-v1.1"
dataset_name: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information"
dataset_acronym: "Bridge2AI-Voice"
name: "Bridge2AI-Voice"
project: "VOICE"
publisher: "PhysioNet"
dataset_version: "1.1"
release_date: "2025-01-17"
doi: "10.13026/249v-w155"
doi_url: "https://doi.org/10.13026/249v-w155"
latest_version_doi: "10.13026/37yb-1t42"
rrid: "SCR_007345"
project_website: "https://docs.b2ai-voice.org"

topics:
  - "voice"
  - "biomarker"
  - "health"
  - "bridge2ai"
  - "artificial intelligence"
  - "neurological disorders"
  - "voice disorders"
  - "mood disorders"
  - "respiratory disorders"

authors:
  - name: "Alistair Johnson"
  - name: "Jean-Christophe Bélisle-Pipon"
  - name: "David Dorr"
  - name: "Satrajit Ghosh"
  - name: "Philip Payne"
  - name: "Maria Powell"
  - name: "Anais Rameau"
  - name: "Vardit Ravitsky"
  - name: "Alexandros Sigaras"
  - name: "Olivier Elemento"
  - name: "Yael Bensoussan"

corresponding_author: "Contact through DACO@b2ai-voice.org for data access questions"

abstract: |
  The human voice contains complex acoustic markers which have been linked to important health conditions
  including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising
  characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility.
  Recent advances in artificial intelligence have provided techniques to extract previously unknown
  prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project
  seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence
  and support critical insights into the use of voice as a biomarker of health.

  Here we present Bridge2AI-Voice v1.1, a comprehensive collection of data derived from voice recordings
  with corresponding clinical information. The dataset provides 12,523 recordings for 306 participants
  collected across five sites in North America. Participants were selected based on known conditions which
  manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and
  respiratory disorders. This release contains data considered low risk, including derivations such as
  spectrograms and MFCC but not the original voice recordings. Detailed demographic, clinical, and validated
  questionnaire data are also made available.

motivation: |
  Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice
  representations to demographic, clinical, and questionnaire data. The project addresses the critical need
  for comprehensive, ethically-collected voice datasets that can advance artificial intelligence research
  while maintaining the highest standards of participant privacy and data protection. By focusing on multiple
  disease cohorts and using standardized collection protocols, the dataset fills a significant gap in
  available resources for voice-based health biomarker research.

composition:
  overview: |
    Derived audio representations and associated phenotype data from adult participants recruited at specialty
    clinics across five North American sites.

  population:
    cohort_scope: "Adult cohort only as of v1.1"
    recruitment_region: "Five sites in North America"
    participants: 306
    recordings: 12523

  condition_groups:
    - "Voice disorders (laryngeal pathologies)"
    - "Neurological and neurodegenerative disorders"
    - "Mood and psychiatric disorders"
    - "Respiratory disorders"
    - "Pediatric voice and speech disorders (planned; not included in v1.1)"

  missing_data: |
    Raw audio waveforms are excluded from v1.1 release for privacy protection. Free speech transcripts
    are removed for participant privacy. Pediatric cohort data is limited in v1.1. HIPAA Safe Harbor
    identifiers have been removed, including state/province data (country of collection retained).

data_characteristics:
  modalities:
    - "Spectrograms derived from audio (513×N dimensions)"
    - "Mel-frequency cepstral coefficients (60×N dimensions)"
    - "Acoustic feature sets extracted using OpenSMILE"
    - "Phonetic and prosodic features from Parselmouth and Praat"
    - "Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)"
    - "Phenotype and questionnaire data"

  data_formats:
    - "Parquet"
    - "TSV"
    - "JSON"

  identifiers_in_files:
    - "participant_id"
    - "session_id"
    - "task_name"

  sampling_and_dimensions: |
    Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional
    to recording length.

collection_process:
  setting: "Specialty clinics and institutions across five North American sites"

  sites:
    us_institutions:
      - "University of South Florida (USF) - IRB of record for Single IRB"
      - "Weill Cornell Medicine (WCM)"
      - "Vanderbilt University Medical Center (VUMC)"
      - "Massachusetts Institute of Technology (MIT)"
      - "Massachusetts Eye and Ear Institute (MEEI)"
      - "Emory University (EU)"
    canadian_institutions:
      - "University of Toronto (UofT) - separate REB approval"
      - "Hospital for Sick Children (HSC) - separate REB approval"
      - "Mount Sinai Hospital (MSH) - separate REB approval"

  participant_selection: |
    Participants were screened for inclusion and exclusion criteria within five predetermined clinical
    condition groups: voice disorders, neurological/neurodegenerative disorders, mood/psychiatric disorders,
    respiratory disorders, and pediatric conditions.

  inclusion_criteria:
    treatment_population:
      - "Diagnosed or treated for voice, respiratory, pediatric, mood, or neurological disorders affecting voice, speech, cough, or breath"
      - "Age ≥18 years (pediatric exceptions at designated sites)"
      - "English or Spanish language proficiency"
      - "Smartphone access for remote data collection participants"
      - "Consent to voice/speech sample contribution to open-source database"
    control_population:
      - "Same as treatment population except NOT diagnosed/treated for specified conditions"

  exclusion_criteria:
    - "Non-English/Spanish speakers"
    - "Recent surgical intervention significantly altering disease symptoms"
    - "Refusal of data de-identification for database upload"

  recruitment_methods:
    in_clinic:
      - "Clinic-based screening from High Volume Expert Clinics (HVEC) and Community Outreach Clinics (COC)"
      - "EHR review for eligibility prior to appointments using EPIC at USF"
      - "Research staff available during clinic visits for interested participants"
    digital:
      - "Social media platforms: LinkedIn, X, Facebook, Instagram, YouTube"
      - "Bridge2AI-Voice website: www.b2ai-voice.org"
      - "Bridge2AI website: www.bridge2ai.org"
      - "FlowTrials online platform"
      - "QR-coded flyers linking to REDCap screening surveys"
      - "Institutional websites"
    recruitment_app:
      - "Bridge2AI Enrollment Web app (HIPAA-compliant, browser-based, no download required)"

  consent: |
    Participants provided informed consent for data collection and sharing of de-identified research data.
    Multiple consent modalities were permitted: signed paper consent, electronic consent via REDCap,
    video consent within app (participants record themselves reading a consent statement), or verbal consent
    with waiver of documentation for remote participation. Research assistants conducted consent discussions
    lasting approximately 30 minutes. Electronic consent versions stored in REDCap for minimum 5-year retention
    post-study. For longitudinal data collection, ongoing electronic consent was required before each subsequent
    session to ensure continued agreement.

  consent_special_populations:
    pediatric: |
      Only enrolled at pediatric participating institutions (not USF). Parents/guardians provided signed consent;
      children provided assent when developmentally appropriate. Children under 7 were not asked to provide assent.

  irb_approval_structure:
    single_irb: |
      All US-based institutions operate under a Single IRB arrangement with USF serving as the IRB of record.
      Once protocol approved at USF, each participating US institution submitted review based on the Single IRB at USF.
    canadian_reb: |
      Canadian institutions (UofT, HSC, MSH) do not follow the Single IRB process and applied for separate
      Research Ethics Board (REB) approval to comply with Canadian regulations.

  procedure: |
    Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice,
    disease-specific information, and voice tasks such as sustained vowel phonation. The protocol was
    designed through a team science approach involving clinical expertise, bioethics, standards, and
    diversity/equity/inclusion considerations.

  data_types_collected:
    acoustic_data:
      - "Prolonged vowel sounds"
      - "Free and spontaneous speech"
      - "Rainbow Passage (standardized text containing all English phonemes)"
      - "Breathing, coughing, and snoring sounds"
      - "Disease-specific vocalizations"
    clinical_data:
      - "Demographics (age, sex, gender, race, ethnicity, language)"
      - "Disease diagnosis, severity, symptoms, and management"
    imaging_retrospective_only:
      - "Chest X-rays (respiratory disorders)"
      - "Brain CT/MRI (neurological disorders)"
      - "Video-laryngoscopy (voice disorders)"
      - "NOTE: NO ADDITIONAL IMAGING PERFORMED"
    genomic_data:
      - "Whole genome sequencing (Alzheimer's cohort only)"
      - "Collected exclusively at UofT and MSH under separate protocol/REB"
    validated_questionnaires:
      - "Voice-Handicap Index-10 (VHI-10) for voice disorders"
      - "Disease-specific instruments integrated into mobile app"

  collection_settings:
    in_clinic:
      - "High Volume Expert Clinics during regular follow-up visits"
      - "Morsani location (primary USF site)"
      - "USF Health Byrd Institute, Park Clinic, Davis Medical Building"
      - "VUMC Shade Tree Clinic and Academy Children's Clinic"
    remote:
      - "Bridge2AI Voice Web app (browser-based)"
      - "Bridge2AI Voice iOS app (for iOS devices)"
      - "Participant's personal smartphone at home"

  longitudinal_data:
    conditions: "Alzheimer's, Parkinson's, and similar progressive/degenerative diseases"
    in_clinic_collection: "During regular follow-up visits (no additional visits required)"
    home_based_collection: "Via smartphone app at 1-6 month intervals"
    maximum_duration: "4 years"
    withdrawal: "Participants may withdraw anytime"

  data_capture: |
    Custom tablet application used for collection with headset when possible. Tasks included sustained
    phonation of vowel sounds and free speech recordings. Bridge2AI Voice Web app and iOS app are both
    HIPAA-compliant with automatic data deletion after page closure (no persistent local storage).

  sessions: |
    Most participants completed one session; a subset required multiple sessions to complete all data
    collection procedures.

  data_export_and_merge: |
    Data exported from REDCap and converted using the b2aiprep open source library available at
    https://github.com/sensein/b2aiprep.

preprocessing_and_derived_data:
  raw_audio_processing: |
    Raw audio converted to monaural (mono) and resampled to 16 kHz with a Butterworth anti-aliasing filter
    to prevent frequency artifacts.

  spectrograms: |
    Short-time Fourier Transform (FFT) applied with 25 ms window, 10 ms hop length, and 512-point FFT.
    Spectrograms stored in power representation format (513×N dimensions where N varies by recording length).

  mfcc: |
    60 Mel-frequency cepstral coefficients computed from spectrograms, providing compact acoustic feature
    representations (60×N dimensions).

  acoustic_features: |
    Comprehensive acoustic features extracted using the OpenSMILE toolkit, capturing temporal dynamics and
    acoustic characteristics across multiple time scales.

  phonetic_prosodic_features: |
    Phonetic and prosodic features computed using Parselmouth (Python interface to Praat) and Praat directly.
    Features include measurements of fundamental frequency (F0), formants (F1, F2, F3), and voice quality
    measures.

  transcription: |
    Automated transcriptions generated using OpenAI's Whisper Large model for speech-to-text conversion.
    Transcripts of free speech audio were removed prior to release to protect participant privacy and reduce
    re-identification risk.

  open_source_code: |
    The b2aiprep library (https://github.com/sensein/b2aiprep) was used to preprocess waveforms and merge
    phenotype data. All preprocessing code is available as open source.

deidentification_and_privacy:
  approach: "HIPAA Safe Harbor"

  data_classification: |
    Data is Personally Identifiable Information (PII) as defined in OMB Memorandum M-07-16, and not
    covered under HIPAA, FERPA, or similar laws governing personal information. Data is protected under
    a Certificate of Confidentiality (CoC), which must be asserted against compulsory legal demands such
    as court orders and subpoenas for identifying information or characteristics of research participants.

  actions:
    - "Removal of all 18 HIPAA Safe Harbor identifier categories"
    - "Removal of state and province; retention of country of data collection only"
    - "Removal of transcripts of free speech audio to prevent content-based re-identification"
    - "Omission of raw audio waveforms in v1.1; only spectrograms and derived features are released"
    - "Removal of dates at finer than year resolution"

  examples_of_identifiers_removed:
    - "Names"
    - "Geographic locators (state/province level)"
    - "Dates at finer than year resolution"
    - "Phone and fax numbers"
    - "Email addresses"
    - "IP addresses"
    - "Social Security Numbers"
    - "Medical record numbers"
    - "Health plan beneficiary numbers"
    - "Device identifiers and serial numbers"
    - "License numbers"
    - "Account numbers"
    - "Vehicle identifiers"
    - "Website URLs"
    - "Full face photographs"
    - "Biometric identifiers"
    - "Any other unique identifying numbers or codes"

  security_requirements: |
    Recipients must store data with security controls adequate to protect Personally Identifiable Information,
    including administrative, physical, and technical safeguards that covered entities and business associates
    must put in place to secure individuals' electronic protected health information. Only authorized persons
    may have access to the data at all times.

files:
  version_notice: |
    Files for version 1.1 are no longer the latest; the project has been updated to version 2.0.1 as of
    August 2025. The files listed below describe v1.1 contents.

  listing:
    - path: "spectrograms.parquet"
      type: "Parquet"
      description: |
        Dense time-frequency representations derived from voice waveforms. Columns include participant_id,
        session_id, task_name, and spectrogram arrays of size 513 x N.

    - path: "mfcc.parquet"
      type: "Parquet"
      description: |
        Mel-frequency cepstral coefficients derived from spectrograms. Arrays of size 60 x N per recording,
        providing compact acoustic representations.

    - path: "phenotype.tsv"
      type: "TSV"
      description: |
        One row per participant containing demographics, acoustic confounders, and responses to validated
        questionnaires. See phenotype.json for data dictionary.

    - path: "phenotype.json"
      type: "JSON"
      description: |
        Data dictionary for phenotype.tsv providing one-sentence descriptions for each column and variable
        definitions.

    - path: "static_features.tsv"
      type: "TSV"
      description: |
        One row per audio recording containing features derived using OpenSMILE, Praat, Parselmouth, and
        TorchAudio toolkits.

    - path: "static_features.json"
      type: "JSON"
      description: |
        Data dictionary for static_features.tsv with detailed feature descriptions and extraction methods.

intended_uses:
  primary: |
    Artificial intelligence and clinical research on voice as a biomarker of health. Suitable for developing
    and benchmarking machine learning models that associate voice-derived features with health conditions.

  examples:
    - "Development and benchmarking of AI models to predict health conditions from voice features"
    - "Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data"
    - "Research on voice-based biomarkers for neurological, psychiatric, and respiratory conditions"
    - "Multi-modal analysis combining voice features with clinical and demographic data"
    - "Development of standardized protocols for voice biomarker research"

  usage_notes: |
    Data are provided as derived representations (spectrograms, MFCC, extracted features) without raw audio
    to reduce re-identification risk. Researchers should be aware that analyses are limited to these derived
    representations in v1.1.

limitations:
  - "Adult cohort only in v1.1; pediatric data not included in this version"
  - "No raw audio waveforms released in v1.1; analyses limited to derived representations"
  - "Participants selected based on conditions known to manifest in voice, which may affect generalizability to healthy populations"
  - "Data collected at specialty clinics may not be representative of community-based populations"
  - "Free speech transcripts removed for privacy, limiting certain types of linguistic analysis"
  - "Geographic data limited to country-level to comply with HIPAA Safe Harbor de-identification"

access_and_licensing:
  platform: "PhysioNet"
  access_policy: "Restricted Access"

  access_conditions: |
    Only registered users who sign the specified Data Use Agreement can access the files. Users must:
    1. Create a PhysioNet account
    2. Complete required training (TCPS 2: CORE 2022)
    3. Sign the Bridge2AI Voice Registered Access Agreement
    4. Agree to use data only for research purposes
    5. Have IRB approval for their research project (may be required)
    6. Submit application to Data Access Compliance Office (DACO) for review and approval

  license: "Bridge2AI Voice Registered Access License"
  data_use_agreement: "Bridge2AI Voice Registered Access Agreement"

  data_use_agreement_provider: "University of South Florida Board of Trustees"

  dua_key_terms:
    agreement_term: "Two years after start date, upon completion of project, upon expiration of applicable ethics approval, or termination by Provider, whichever occurs first"

    permitted_uses: |
      Data shall be used solely to conduct the approved research project and only by Recipient Scientist and
      approved Recipient Personnel (faculty, employees, fellows, students, agents) with a need to use the data.
      Collaborators at other research organizations and other research teams at the same organization must apply
      independently for access and sign separate DUA before accessing data.

    restrictions: |
      - No disclosure, release, sale, rent, lease, loan, or access to third parties without prior written consent
      - No use of intellectual property protection or database rights to prevent or limit access to data or research conclusions
      - No contact with data subjects without IRB approval, Provider approval, and informed consent
      - No use or further disclosure except as permitted by DUA or required by law
      - Must establish administrative, technical, and physical safeguards to prevent unauthorized access
      - Must store data with security controls adequate to protect Personally Identifiable Information
      - Must ensure only Authorized Persons have access to data

    sharing_policy: |
      Recipient is encouraged to make research results publicly available in open-access journals or
      pre-print servers where possible. Results must recognize the contribution of Provider as source of data
      in all public disclosures.

    data_disposition: |
      Upon termination or expiration, data shall be destroyed according to Provider instructions.
      Recipient shall submit written certification of data destruction within 30 days after termination
      or expiration. Recipient may retain one copy to comply with records retention requirements under law,
      regulation, or institutional policy, and for research integrity and verification purposes.

    legal_protections: |
      - Data is Personally Identifiable Information (PII) as defined in OMB Memorandum M-07-16
      - Not covered under HIPAA, FERPA, or similar laws
      - Protected under a Certificate of Confidentiality, which must be asserted against compulsory legal
        demands (court orders, subpoenas) for identifying information or characteristics of research participants
      - If required by law or legal process to disclose, Recipient must notify Provider prior to disclosure
        (to extent allowed by law) and disclose least possible amount necessary

    breach_notification: |
      Recipient must notify Provider of any unauthorized use or disclosure not provided for by DUA within
      5 business days of discovery. Recipient shall take steps to minimize impact, cooperate with Provider
      to investigate/correct/mitigate, and support Provider's obligations to make notifications under state law.

    warranties_and_liability: |
      Data provided "AS IS" with no warranties of merchantability, fitness for particular purpose, or
      non-infringement. Recipient assumes all liability for damages arising from use, storage, disclosure,
      or disposal of data. Provider not liable for any loss, claim, or demand. No indemnification provided
      by either party.

    physionet_compliance: |
      Recipient agrees to adhere to specific requirements of PhysioNet.org managed by MIT Laboratory for
      Computational Physiology and supported by NIBIB under NIH grant R01EB030362.

    amendment_rights: |
      Provider may unilaterally amend DUA if federal sponsor requires revision. If Recipient objects to
      amendment, DUA terminates immediately and Recipient must return or destroy all data.

  raw_audio_access: |
    Raw audio waveforms are not included in the public PhysioNet release. Researchers requiring raw audio
    for specific approved purposes may submit controlled access requests to DACO@b2ai-voice.org.

ethics:
  protocol_title: "Bridge2AI Voice Data Acquisition"

  protocol_version: "V14 (July 11, 2025, clean version dated September 4, 2025)"

  principal_investigator: "Yael Bensoussan, MD MSc, FRCSC (USF Department of Otolaryngology)"

  study_design: "Prospective cohort study"

  target_sample: "30,000 participants across 9 participating institutions"

  irb_approval: |
    Data collection and sharing approved by the University of South Florida Institutional Review Board (IRB),
    which serves as the IRB of record for all US-based institutions under a Single IRB arrangement. Canadian
    institutions (University of Toronto, Hospital for Sick Children, Mount Sinai Hospital) obtained separate
    Research Ethics Board (REB) approval to comply with Canadian regulations.

  irb_structure:
    single_irb_lead: "University of South Florida (USF) - IRB of record for US institutions"
    us_institutions_under_single_irb:
      - "Weill Cornell Medicine (WCM)"
      - "Vanderbilt University Medical Center (VUMC)"
      - "Massachusetts Institute of Technology (MIT)"
      - "Massachusetts Eye and Ear Institute (MEEI)"
      - "Emory University (EU)"
    canadian_separate_reb:
      - "University of Toronto (UofT)"
      - "Hospital for Sick Children (HSC)"
      - "Mount Sinai Hospital (MSH)"

  ethical_position: |
    Dataset is ethically sourced with extensive privacy protections. Derived data released as low-risk
    category. The project emphasizes demographic diversity, ethical data collection practices, and transparent
    documentation of methods and limitations. The protocol underwent 14 revisions (January 2023 - July 2025)
    with major updates including electronic consent options, Spanish-language speaker inclusion, and additional
    recruitment platforms.

  participant_consent: |
    All participants provided informed consent for data collection and sharing of de-identified research data.
    Multiple consent modalities permitted: signed paper consent, electronic consent via REDCap, video consent
    within app, or verbal consent with documentation waiver for remote participation. Research assistants
    conducted consent discussions lasting approximately 30 minutes, allowing adequate time for questions.
    Participants made explicitly aware that participation is optional and voluntary. For longitudinal studies,
    ongoing electronic consent required before each subsequent data collection to ensure continued agreement.
    When study changes occur, participants receive notification and re-consent before additional data collection.

  consent_retention: |
    Electronic consent versions stored in REDCap for minimum 5-year retention post-study. Paper copies scanned
    and uploaded electronically.

  participant_rights:
    withdrawal: |
      If participants withdraw after completing voice data collection, their data is retained. If withdrawal
      occurs during/before completion, data is excluded. For longitudinal studies, completed data from withdrawn
      participants remains in the database since the purpose is to provide a platform, not analyze during
      the study.
    satisfaction_survey: |
      Participants withdrawing or declining participation may optionally complete a satisfaction survey (no PHI
      collected) to provide feedback on their decision.
    future_contact: |
      Participants may consent to future contact for extended longitudinal data collection beyond the 4-year
      study period. Contact information (email, phone) stored locally at each institution only.

  vulnerable_populations:
    pediatric_protections:
      - "Enrollment restricted to designated pediatric sites only (not USF)"
      - "Parental consent required (one parent sufficient for minimal-risk research)"
      - "Assent documentation required for children ≥7 years (developmentally appropriate)"
      - "Children under 7 not asked to provide assent"
      - "Checklist HRP-416 completed for subjects under 21"
      - "No compensation for pediatric participants"
    diversity_and_inclusion:
      - "Plan for Enhancing Diverse Perspectives (PEDP) ensures recruitment from socioeconomically disadvantaged populations"
      - "Community Outreach Clinics prioritize underserved/underrepresented groups"
      - "Spanish-language speaker recruitment added in V11 (February 2025)"
      - "Medical record review for underrepresented diagnostic categories"

  risks_and_mitigation:
    identified_risks:
      primary_risk: "Personal information being mistakenly released"
      physical_risk: "Minimal - voice collection is safe and non-invasive"
      psychological_risk: "Questionnaires addressing mood disorders may trigger negative emotions in vulnerable participants"
      voice_recognition_risk: |
        Voice data cannot be completely de-identified due to unique biometric properties. Even when removing
        all HIPAA protected information, it is always possible, even if unlikely, that someone could recognize
        the participant's voice.
    mitigation_strategies:
      - "Strict confidentiality protocols and HIPAA compliance training for all personnel"
      - "Federated learning to prevent centralized data aggregation"
      - "HIPAA waivers limited to recruitment screening purposes only"
      - "Two-tiered privacy: institutional storage + de-identified cloud sharing"
      - "Data use agreements between collaborating institutions"
      - "Multi-level privacy framework with institutional, shared, and federated layers"

  potential_benefits: |
    No direct immediate benefits to participants during this study. Societal benefits include development
    of AI models for earlier disease screening, improved diagnostic accuracy, and enhanced healthcare
    accessibility for underserved populations.

  compensation:
    adults:
      - "$40 gift card for sessions <90 minutes"
      - "$80 gift card for sessions ≥90 minutes"
      - "Maximum of three compensated sessions ($120 total)"
    pediatric:
      - "No compensation for pediatric participants"

  data_retention:
    institutional_storage: "Maximum 10 years post-study; then destroyed"
    participant_withdrawal: |
      If participants withdraw before data completion, data is excluded. If withdrawal occurs after completion,
      data is retained as the purpose is to provide a platform for future research.

  privacy_framework:
    level_1_institutional:
      - "Identifiable PHI maintained locally at each institution on password-protected REDCap databases"
      - "Access restricted to authorized research personnel"
      - "Retention period: maximum 10 years post-study; then destroyed"
    level_2_deidentified_shared:
      - "De-identified voice, demographic, and clinical data hosted on NIH STRIDES Partner cloud infrastructure"
      - "Partners: Amazon, Google, Microsoft (pre-negotiated HIPAA agreements)"
      - "Data transfer agreements drafted by each institution"
    level_3_federated_learning:
      - "Algorithms run on data at each institution and model updates shared to central node"
      - "Researchers benefit from other institutions' data without sharing actual data"
      - "Prevents centralized data aggregation while enabling multi-center collaboration"

  data_security:
    - "Secure HTTPS protocol for all data transmission"
    - "Bridge2AI Voice Web app and iOS app both HIPAA-compliant"
    - "Automatic data deletion after page closure (no persistent local storage)"
    - "Individual usernames and passwords for REDCap access"
    - "Encryption and access controls on cloud servers"
    - "Security controls adequate to protect Personally Identifiable Information"
    - "Only authorized persons have access to data at all times"

  quality_control:
    year_1: "All samples reviewed for acoustic quality and machine learning readiness by 2 acoustic engineers and 3 AI data scientists"
    years_2_4: "Random 10% monthly sample audits for quality and ML preparedness"
    application_features: "Volume standardization and noise cancellation models"

  phased_implementation:
    phase_1_exploratory:
      status: "Completed November 2023"
      activities:
        - "IT/cloud infrastructure development"
        - "App development and refinement"
        - "Initial data collection: up to 180 participants"
    phase_2_pilot:
      status: "Began November 2023 (ongoing as of document date)"
      activities:
        - "Cumulative 600 participants"
        - "Main sites + HVEC expansion"
        - "Data sharing framework development"
    phase_3_expansion:
      activities:
        - "Community Outreach Clinic inclusion"
        - "Cumulative 3,000 participants"
        - "Open-source database preparation"
    phase_4_outreach:
      activities:
        - "Normal cohort partnerships"
        - "HVEC/COC expansion"
        - "Cumulative 5,000 participants"

  open_source_timeline: |
    By end of Phase 2, de-identified data becomes publicly available through an NIH hosted platform similar
    to the Clinical Genomic Database.

  special_features:
    nih_strides: |
      This initiative leverages cloud environments to streamline NIH data use by partnering with commercial
      providers (Amazon, Google, Microsoft), modernizing biomedical research through commercial cloud services.
    federated_learning: |
      Federated learning prevents institutions from sharing raw data while enabling multi-center model training.
      Centralized model updates allow distributed research collaboration with HIPAA-compliant AI analysis
      without data migration.

  conflicts_of_interest: "None to declare"

  protocol_revision_history: |
    The protocol has undergone 14 revisions (January 2023 - July 2025). Major updates include: V4 - electronic
    consent options added; V11 - Spanish-language speaker inclusion and new recruitment platform; V13 - flyer
    recruitment clarification and additional Vanderbilt sites.

funding_and_acknowledgements:
  funding:
    - agency: "National Institutes of Health (NIH)"
      award_number: "3OT2OD032720-01S1"
      project_title: |
        Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database
        to understand disease like never before

  acknowledgements: |
    We acknowledge the essential contribution of study participants who generously donated their time and
    data to advance health research. We thank the NIH for continued support of the Bridge2AI program and
    the multidisciplinary team of clinicians, researchers, bioethicists, and community stakeholders who
    contributed to protocol development.

  platform_support: |
    PhysioNet infrastructure supported by the National Institute of Biomedical Imaging and Bioengineering
    (NIBIB) under NIH grant number R01EB030362.

software_and_tools:
  preprocessing_code:
    name: "b2aiprep"
    url: "https://github.com/sensein/b2aiprep"
    description: |
      Open source Python library used to preprocess raw audio waveforms and merge phenotype data from REDCap exports.

  referenced_tools:
    - name: "OpenSMILE"
      description: "Munich versatile and fast open-source audio feature extractor"
    - name: "Praat"
      description: "Phonetic analysis software for speech and voice analysis"
    - name: "Parselmouth"
      description: "Python interface to Praat for programmatic phonetic analysis"
    - name: "TorchAudio"
      description: "PyTorch library for audio processing and feature extraction"
    - name: "OpenAI Whisper Large"
      description: "State-of-the-art speech recognition model for transcription generation"
    - name: "librosa"
      description: "Python library for audio analysis (referenced in usage examples)"

variables_and_fields:
  common_identifiers:
    - "participant_id - Unique de-identified participant identifier"
    - "session_id - Unique session identifier"
    - "task_name - Voice task type (e.g., sustained vowel, free speech)"

  spectrograms:
    dimensions: "513 x N"
    description: |
      Power spectrogram per recording. Each element represents energy in a specific frequency bin at a
      specific time point. N varies based on recording length.

  mfcc:
    dimensions: "60 x N"
    description: |
      Mel-frequency cepstral coefficients derived from spectrograms. Provides compact representation of
      spectral envelope, commonly used in speech and audio processing.

  phenotype:
    granularity: "One row per participant"
    contents: |
      Demographics (age, gender, ethnicity), acoustic confounders (hearing status, language background),
      validated questionnaire responses (health-related quality of life, symptom severity).

  static_features:
    granularity: "One row per recording"
    contents: |
      Acoustic features: Energy, pitch, spectral characteristics, voice quality measures
      Phonetic features: Formant frequencies, fundamental frequency statistics
      Prosodic features: Speaking rate, pause patterns, intonation measures
      Extracted using OpenSMILE, Praat, Parselmouth, and TorchAudio toolkits.

release_notes:
  - version: "1.1"
    date: "2025-01-17"
    notes: |
      This release added Mel-frequency cepstral coefficients (MFCC) to the dataset, providing additional
      derived acoustic representations for analysis.

  - version: "1.0"
    date: "2024"
    notes: |
      Initial public release of the Bridge2AI-Voice dataset with spectrograms, acoustic features, phenotype
      data, and validated questionnaire responses for 306 adult participants.

versions_available_on_platform:
  - version: "1.1"
    date: "2025-01-17"
    doi: "10.13026/249v-w155"
  - version: "2.0.0"
    date: "2025-04-16"
    notes: "Major update with additional participants and features"
  - version: "2.0.1"
    date: "2025-08-18"
    notes: "Latest version - bug fixes and minor improvements"
    doi: "10.13026/37yb-1t42"

citations:
  dataset_citation: |
    Johnson, A., Bélisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V.,
    Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse
    voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345.
    https://doi.org/10.13026/249v-w155

  platform_citation: |
    Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., Mietus, J. E.,
    Moody, G. B., Peng, C. K., & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet:
    Components of a new research resource for complex physiologic signals. Circulation [Online].
    101 (23), pp. e215–e220. RRID:SCR_007345.

references:
  - citation: |
      Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M.,
      Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J.,
      Awan, S., Bolser, D., Bensoussan, Y. (2024). Developing Multi-Disorder Voice Protocols: A team
      science approach involving clinical expertise, bioethics, standards, and DEI. Proc. Interspeech
      2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926

  - citation: |
      Bensoussan, Y., Ghosh, S. S., Rameau, A., et al. (2024). Bridge2AI Voice REDCap (v3.20.0).
      Zenodo. https://doi.org/10.5281/zenodo.14148755

  - citation: |
      Florian Eyben, Martin Wöllmer, Björn Schuller: openSMILE - The Munich Versatile and Fast
      Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy,
      ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010.

  - citation: |
      Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7.

  - citation: |
      Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat.
      Journal of Phonetics. 2018 Nov 1;71:1-5.
