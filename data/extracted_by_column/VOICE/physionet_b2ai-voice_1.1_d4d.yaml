# D4D Metadata extracted from: physionet_b2ai-voice_1.1_row14.txt
# Column: VOICE
# Generated: 2025-09-16 18:34:24 (Retry)
# Status: Fixed failed extraction

dataset_name: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information"
dataset_acronym: "Bridge2AI-Voice"
project: "VOICE"
publisher: "PhysioNet"
dataset_version: "1.1"
release_date: "2025-01-17"
doi: "10.13026/249v-w155"
doi_url: "https://doi.org/10.13026/249v-w155"
latest_version_doi: "10.13026/37yb-1t42"
rrid: "SCR_007345"
project_website: "https://docs.b2ai-voice.org"
topics:
  - "voice"
  - "bridge2ai"

authors:
  - name: "Alistair Johnson"
  - name: "Jean-Christophe BÃ©lisle-Pipon"
  - name: "David Dorr"
  - name: "Satrajit Ghosh"
  - name: "Philip Payne"
  - name: "Maria Powell"
  - name: "Anais Rameau"
  - name: "Vardit Ravitsky"
  - name: "Alexandros Sigaras"
  - name: "Olivier Elemento"
  - name: "Yael Bensoussan"

corresponding_author: "Not publicly listed; contact information requires login."

abstract: "The human voice contains complex acoustic markers which have been linked to important health conditions including dementia, mood disorders, and cancer. When viewed as a biomarker, voice is a promising characteristic to measure as it is simple to collect, cost-effective, and has broad clinical utility. Recent advances in artificial intelligence have provided techniques to extract previously unknown prognostically useful information from dense data elements such as images. The Bridge2AI-Voice project seeks to create an ethically sourced flagship dataset to enable future research in artificial intelligence and support critical insights into the use of voice as a biomarker of health. Here we present Bridge2AI-Voice, a comprehensive collection of data derived from voice recordings with corresponding clinical information. Bridge2AI-Voice v1.0, the initial release, provides 12,523 recordings for 306 participants collected across five sites in North America. Participants were selected based on known conditions which manifest within the voice waveform including voice disorders, neurological disorders, mood disorders, and respiratory disorders. The initial release contains data considered low risk, including derivations such as spectrograms but not the original voice recordings. Detailed demographic, clinical, and validated questionnaire data are also made available."

motivation: "Enable ethically sourced, large-scale research on voice as a biomarker of health by linking derived voice representations to demographic, clinical, and questionnaire data."

composition:
  overview: "Derived audio representations and associated phenotype data from adult participants recruited at specialty clinics."
  population:
    cohort_scope: "Adult cohort only as of v1.1"
    recruitment_region: "Five sites in North America"
    participants: 306
    recordings: 12523
  condition_groups:
    - "Voice disorders"
    - "Neurological and neurodegenerative disorders"
    - "Mood and psychiatric disorders"
    - "Respiratory disorders"
    - "Pediatric voice and speech disorders (planned; not included in v1.1)"

data_characteristics:
  modalities:
    - "Spectrograms derived from audio"
    - "Mel-frequency cepstral coefficients"
    - "Acoustic feature sets (openSMILE)"
    - "Phonetic and prosodic features (Parselmouth and Praat)"
    - "Transcriptions generated by OpenAI Whisper Large (free speech transcripts removed)"
    - "Phenotype and questionnaire data"
  data_formats:
    - "Parquet"
    - "TSV"
    - "JSON"
  identifiers_in_files:
    - "participant_id"
    - "session_id"
    - "task_name"
  sampling_and_dimensions: "Audio resampled to 16 kHz; spectrograms are 513 x N; MFCC arrays are 60 x N, where N is proportional to recording length."

collection_process:
  setting: "Specialty clinics and institutions"
  participant_selection: "Screened for inclusion and exclusion criteria within five predetermined groups."
  consent: "Participants provided consent for data collection and sharing of de-identified research data."
  procedure: "Standardized protocol collecting demographics, health questionnaires, targeted confounders for voice, disease specific information, and voice tasks such as sustained vowel phonation."
  data_capture: "Custom tablet application used for collection; headset used when possible."
  sessions: "Most participants completed one session; a subset required multiple sessions."
  data_export_and_merge: "Exported from REDCap and converted using an open source library."

preprocessing_and_derived_data:
  raw_audio_processing: "Converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter."
  spectrograms: "Short-time FFT with 25 ms window, 10 ms hop, 512-point FFT; stored in power representation."
  mfcc: "60 coefficients computed from spectrograms."
  acoustic_features: "Extracted using openSMILE capturing temporal dynamics and acoustic characteristics."
  phonetic_prosodic_features: "Computed using Parselmouth and Praat; includes measures of fundamental frequency, formants, and voice quality."
  transcription: "Generated using OpenAI Whisper Large; transcripts of free speech audio were removed prior to release."
  open_source_code: "b2aiprep library used to preprocess waveforms and merge phenotype data."

deidentification_and_privacy:
  approach: "HIPAA Safe Harbor"
  actions:
    - "Removal of HIPAA Safe Harbor identifiers."
    - "Removal of state and province; retention of country of data collection."
    - "Removal of transcripts of free speech audio."
    - "Omission of raw audio waveforms in v1.1; only spectrograms and other derived features are released."
  examples_of_identifiers_removed:
    - "Names"
    - "Geographic locators"
    - "Dates at finer than year resolution"
    - "Phone and fax numbers"
    - "Email addresses"
    - "IP addresses"
    - "Social Security Numbers"
    - "Medical record numbers"
    - "Health plan beneficiary numbers"
    - "Device identifiers"
    - "License numbers"
    - "Account numbers"
    - "Vehicle identifiers"
    - "Website URLs"
    - "Full face photos"
    - "Biometric identifiers"
    - "Any unique identifiers"

files:
  version_notice: "Files for version 1.1 are no longer available; the latest version of this project is 2.0.1."
  listing:
    - path: "spectrograms.parquet"
      type: "Parquet"
      description: "Dense time-frequency representations derived from voice waveforms; includes participant_id, session_id, task_name, spectrogram of size 513 x N."
    - path: "mfcc.parquet"
      type: "Parquet"
      description: "Mel-frequency cepstral coefficients derived from spectrograms; arrays of size 60 x N per recording."
    - path: "phenotype.tsv"
      type: "TSV"
      description: "One row per participant; demographics, acoustic confounders, and responses to validated questionnaires."
    - path: "phenotype.json"
      type: "JSON"
      description: "Data dictionary for phenotype.tsv with one sentence descriptions per column."
    - path: "static_features.tsv"
      type: "TSV"
      description: "One row per audio recording; features derived using openSMILE, Praat, parselmouth, and torchaudio."
    - path: "static_features.json"
      type: "JSON"
      description: "Data dictionary for static_features.tsv with feature descriptions."

intended_uses:
  primary: "Artificial intelligence and clinical research on voice as a biomarker of health."
  examples:
    - "Development and benchmarking of models to associate voice-derived features with health conditions."
    - "Exploration of acoustic, phonetic, and prosodic correlates of disease using de-identified derived data."
  usage_notes: "Data are provided as derived representations without raw audio to reduce re-identification risk."

limitations:
  - "Adult cohort only in v1.1; pediatric data not included."
  - "No raw audio is released in v1.1; analyses are limited to derived representations."
  - "Participants were selected based on conditions known to manifest in voice, which may affect generalizability."

access_and_licensing:
  platform: "PhysioNet"
  access_policy: "Restricted Access"
  access_conditions: "Only registered users who sign the specified data use agreement can access the files."
  license: "Bridge2AI Voice Registered Access License"
  data_use_agreement: "Bridge2AI Voice Registered Access Agreement"

ethics:
  irb_approval: "Data collection and sharing approved by the University of South Florida Institutional Review Board."
  ethical_position: "Dataset is ethically sourced with privacy protections; derived data released for low risk."
  conflicts_of_interest: "None to declare."

funding_and_acknowledgements:
  funding:
    - agency: "National Institutes of Health"
      award_number: "3OT2OD032720-01S1"
      project_title: "Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioaccoustic database to understand disease like never before"
  acknowledgements: "We acknowledge the contribution of study participants and the NIH for continued support of the project."
  platform_support: "National Institute of Biomedical Imaging and Bioengineering under NIH grant number R01EB030362 supported PhysioNet infrastructure."

software_and_tools:
  preprocessing_code:
    name: "b2aiprep"
    url: "https://github.com/sensein/b2aiprep"
    description: "Open source library used to preprocess raw audio and merge phenotype data."
  referenced_tools:
    - "openSMILE"
    - "Praat"
    - "Parselmouth"
    - "torchaudio"
    - "OpenAI Whisper Large"
    - "librosa (example usage for visualization)"

variables_and_fields:
  common_identifiers:
    - "participant_id"
    - "session_id"
    - "task_name"
  spectrograms:
    dimensions: "513 x N"
    description: "Power spectrogram per recording."
  mfcc:
    dimensions: "60 x N"
    description: "Mel-frequency cepstral coefficients derived from spectrograms."
  phenotype:
    granularity: "One row per participant"
    contents: "Demographics, acoustic confounders, validated questionnaire responses."
  static_features:
    granularity: "One row per recording"
    contents: "Acoustic, phonetic, and prosodic features from multiple toolkits."

release_notes:
  - version: "1.1"
    date: "2025-01-17"
    notes: "This release added Mel-frequency cepstral coefficients."
  - version: "1.0"
    date: "2024"
    notes: "Initial release of the dataset."

versions_available_on_platform:
  - version: "1.1"
    date: "2025-01-17"
  - version: "2.0.0"
    date: "2025-04-16"
  - version: "2.0.1"
    date: "2025-08-18"

citations:
  dataset_citation: "Johnson, A., BÃ©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2025). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.1). PhysioNet. RRID:SCR_007345. https://doi.org/10.13026/249v-w155"
  platform_citation: "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215âe220. RRID:SCR_007345."

references:
  - "Rameau, A., Ghosh, S., Sigaras, A., Elemento, O., Belisle-Pipon, J.-C., Ravitsky, V., Powell, M., Johnson, A., Dorr, D., Payne, P., Boyer, M., Watts, S., Bahr, R., Rudzicz, F., Lerner-Ellis, J., Awan, S., Bolser, D., Bensoussan, Y. (2024) Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.. Proc. Interspeech 2024, 1445-1449, doi: 10.21437/Interspeech.2024-1926"
  - "Bensoussan, Y., Ghosh, S. S., Rameau, A., Boyer, M., Bahr, R., Watts, S., Rudzicz, F., Bolser, D., Lerner-Ellis, J., Awan, S., Powell, M. E., Belisle-Pipon, J.-C., Ravitsky, V., Johnson, A., Zisimopoulos, P., Tang, J., Sigaras, A., Elemento, O., Dorr, D., â¦ Bridge2AI-Voice. (2024). Bridge2AI Voice REDCap (v3.20.0). Zenodo. https://doi.org/10.5281/zenodo.14148755"
  - "Florian Eyben, Martin WÃ¶llmer, BjÃ¶rn Schuller: openSMILE - The Munich Versatile and Fast Open-Source Audio Feature Extractor, Proc. ACM Multimedia (MM), ACM, Florence, Italy, ISBN 978-1-60558-933-6, pp. 1459-1462, 25.-29.10.2010."
  - "Boersma P, Van Heuven V. Speak and unSpeak with PRAAT. Glot International. 2001 Nov;5(9/10):341-7."
  - "Jadoul Y, Thompson B, De Boer B. Introducing parselmouth: A python interface to praat. Journal of Phonetics. 2018 Nov 1;71:1-5."
  - "Hwang, J., Hira, M., Chen, C., Zhang, X., Ni, Z., Sun, G., Ma, P., Huang, R., Pratap, V., Zhang, Y., Kumar, A., Yu, C.-Y., Zhu, C., Liu, C., Kahn, J., Ravanelli, M., Sun, P., Watanabe, S., Shi, Y., Tao, T., Scheibler, R., Cornell, S., Kim, S., & Petridis, S. (2023). TorchAudio 2.1: Advancing speech recognition, self-supervised learning, and audio processing components for PyTorch. arXiv preprint arXiv:2310.17864"
  - "Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov, A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D., Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Mahadeokar, J., Hwang, J., Chen, J., Goldsborough, P., Roy, P., Narenthiran, S., Watanabe, S., Chintala, S., Quenneville-BÃ©lair, V, & Shi, Y. (2021). TorchAudio: Building Blocks for Audio and Speech Processing. arXiv preprint arXiv:2110.15018."
  - "Bevers, I., Ghosh, S., Johnson, A., Brito, R., Bedrick, S., Catania, F., & Ng, E. (2017). My Research Software (Version 0.21.0) [Computer software]. https://github.com/sensein/b2aiprep"
  - "Johnson, A., BÃ©lisle-Pipon, J., Dorr, D., Ghosh, S., Payne, P., Powell, M., Rameau, A., Ravitsky, V., Sigaras, A., Elemento, O., & Bensoussan, Y. (2024). Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (version 1.0). Health Data Nexus. https://doi.org/10.57764/qb6h-em84"