=== YAML Fixing Applied ===
id: "doi:10.57764/qb6h-em84"
name: Bridge2AI-Voice v1.0
title: "Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information"
description: >
  Bridge2AI-Voice v1.0 is the initial release of an ethically sourced, multi-institutional,
  diverse voice dataset linked to health information to enable AI research on voice as a biomarker
  of health. The release provides 12,523 derived recordings from 306 adult participants across five
  North American sites, including spectrograms and acoustic/phonetic/prosodic features, along with
  detailed demographic, clinical, and validated questionnaire data. To reduce risk, original raw
  audio waveforms and free-speech transcripts are not included in v1.0; only de-identified derived
  data are distributed.
language: English
issued: 2024-11-27
created_on: 2024-11-27
last_updated_on: 2024-11-27
version: "1.0"
doi: "doi:10.57764/qb6h-em84"
page: "https://docs.b2ai-voice.org"
keywords:
  - voice
  - bridge2ai
  - audio
  - spectrograms
  - phenotype
license: Bridge2AI Voice Registered Access License
created_by:
  - Alistair Johnson
  - Jean-Christophe Bélisle-Pipon
  - David Dorr
  - Satrajit Ghosh
  - Philip Payne
  - Maria Powell
  - Anaïs Rameau
  - Vardit Ravitsky
  - Alexandros Sigaras
  - Olivier Elemento
  - Yael Bensoussan
purposes:
  - name: Purpose
    response: >
      Create an ethically sourced flagship dataset to enable AI research using voice as a
      biomarker of health and to support clinical insights across multiple disease domains.
tasks:
  - name: Task
    response: >
      Development and evaluation of AI methods for detection, screening, monitoring, and
      characterization of health conditions using voice and speech-derived features.
addressing_gaps:
  - name: AddressingGap
    response: >
      Address the lack of large, diverse, multi-institutional voice datasets linked to health
      information with standardized collection protocols to advance clinically useful voice AI.
creators:
  - name: Bridge2AI-Voice Project Team
funders:
  - name: NIH Bridge2AI Program
    grantor:
      id: NIH
      name: National Institutes of Health (NIH)
    grant:
      name: "Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioacoustic database to understand disease like never before"
      grant_number: 3OT2OD032720-01S1
instances:
  - name: Instance
    representation: Voice recordings and associated clinical/phenotype data
    instance_type: >
      Multiple related instance types: (1) per-recording derived data (spectrograms and static
      acoustic/phonetic/prosodic features), (2) per-participant phenotype/questionnaire records.
    data_type: >
      Derived features from raw audio (spectrograms, OpenSMILE features, Parselmouth/Praat measures)
      and tabular phenotype/questionnaire data; raw audio not included in v1.0.
    counts: 12523
    label: No explicit target labels provided; derived features and phenotype data only.
    sampling_strategies:
      - name: SamplingStrategy
        is_sample:
          - yes; cohort-based sample of clinic-presenting patients
        is_random:
          - no; condition-based inclusion
        source_data:
          - Patients at specialty clinics across five North American sites
        is_representative:
          - not stated
        strategies:
          - Prospective recruitment into five predetermined disease cohorts with inclusion/exclusion screening
    missing_information:
      - name: MissingInfo
        missing:
          - Raw audio waveforms
          - Free speech transcripts
        why_missing:
          - Omitted in v1.0 for privacy and risk reduction; only low-risk derived data released
sampling_strategies:
  - name: SamplingStrategy
    is_sample:
      - yes; recruited cohort sample
    is_random:
      - no
    source_data:
      - Specialty clinic patients selected into predefined cohorts (respiratory, voice, neurological, mood, pediatric)
    why_not_representative:
      - Condition-based enrollment rather than population sampling
collection_mechanisms:
  - name: CollectionMechanism
    description:
      - Standardized multi-site protocol using a custom tablet application
      - Headset microphone used when possible
      - Screening for inclusion/exclusion prior to visit
      - Data exported from REDCap and converted using an open-source library (b2aiprep)
data_collectors:
  - name: DataCollector
    description:
      - Project investigators at specialty clinics across five North American sites; participants provided informed consent
ethical_reviews:
  - name: EthicalReview
    description:
      - Data collection and sharing approved by the University of South Florida Institutional Review Board
      - Submitted for review to the University of Toronto Research Ethics Board
acquisition_methods:
  - name: InstanceAcquisition
    description:
      - Voice tasks (e.g., sustained vowel) recorded via standardized protocol; demographic and clinical questionnaires collected
    was_directly_observed: yes (audio recordings)
    was_reported_by_subjects: yes (questionnaire responses)
    was_inferred_derived: yes (acoustic/phonetic/prosodic features; spectrograms; ASR transcriptions)
    was_validated_verified: >
      Standardized multi-site data collection protocol; preprocessing code released (b2aiprep) to ensure reproducibility
preprocessing_strategies:
  - name: PreprocessingStrategy
    description:
      - Raw audio converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter
      - Spectrograms via STFT (25 ms window, 10 ms hop, 512-point FFT)
      - Acoustic features via OpenSMILE
      - Phonetic/prosodic measures via Parselmouth and Praat
      - Transcriptions generated using OpenAI Whisper Large
    used_software:
      - name: OpenSMILE
        url: "https://audeering.github.io/opensmile/"
      - name: Parselmouth
        url: "https://parselmouth.readthedocs.io/"
      - name: Praat
        url: "https://www.fon.hum.uva.nl/praat/"
      - name: torchaudio
        url: "https://pytorch.org/audio/"
      - name: OpenAI Whisper Large
        url: "https://github.com/openai/whisper"
      - name: b2aiprep
        url: "https://github.com/sensein/b2aiprep"
labeling_strategies:
  - name: LabelingStrategy
    description:
      - Automatic ASR transcriptions generated using Whisper Large; free speech transcripts excluded from release
raw_sources:
  - name: RawData
    description:
      - Raw audio waveforms were not distributed in v1.0; future releases aim to include voice data with additional safeguards
external_resources:
  - name: ExternalResource
    external_resources:
      - Documentation website: https://docs.b2ai-voice.org
      - REDCap export tooling (Zenodo): https://doi.org/10.5281/zenodo.14148755
      - Preprocessing library (b2aiprep): https://github.com/sensein/b2aiprep
    archival:
      - Versioned DOIs provided (v1.0: https://doi.org/10.57764/qb6h-em84; latest: https://doi.org/10.57764/3sg0-7440)
confidential_elements:
  - name: Confidentiality
    description:
      - Underlying clinical information pertains to individuals; de-identified data and registered access mitigate confidentiality risks
content_warnings:
  - name: ContentWarning
    warnings:
      - None noted
subpopulations:
  - name: Subpopulation
    identification:
      - Adult cohort (v1.0 release contains adults only)
      - Disease cohorts: voice disorders; neurological/neurodegenerative; mood/psychiatric; respiratory; pediatric (planned)
    distribution:
      - Counts by cohort not provided in this release note
sensitive_elements:
  - name: SensitiveElement
    description:
      - Health-related data including demographic, clinical, and validated questionnaire responses
is_deidentified:
  - name: Deidentification
    description:
      - HIPAA Safe Harbor identifiers removed
      - State/province removed; country of data collection retained
      - Free speech transcripts removed
      - Original audio waveforms omitted; only derived spectrograms/features released
distribution_formats:
  - name: DistributionFormat
    description:
      - spectrograms.parquet (Parquet): time-frequency representations and identifiers (participant_id, session_id, task_name)
  - name: DistributionFormat
    description:
      - static_features.tsv (TSV) and static_features.json (dictionary): per-recording acoustic/phonetic/prosodic features and metadata
  - name: DistributionFormat
    description:
      - phenotype.tsv (TSV) and phenotype.json (dictionary): per-participant demographics, confounders, and validated questionnaire responses
distribution_dates:
  - name: DistributionDate
    description:
      - Initial public release on 2024-11-27 (v1.0)
license_and_use_terms:
  name: LicenseAndUseTerms
  description:
    - Access is restricted to credentialed users who sign the Bridge2AI Voice Registered Access Agreement (DUA)
    - License: Bridge2AI Voice Registered Access License
    - Required training: TCPS 2: CORE 2022
    - Files hosted under registered/credentialed access via Health Data Nexus
updates:
  name: UpdatePlan
  description:
    - Future releases aim to include voice audio with additional security precautions; v1.0 is the first release
version_access:
  name: VersionAccess
  description:
    - Versioned DOIs provided (v1.0 DOI: https://doi.org/10.57764/qb6h-em84; latest version DOI: https://doi.org/10.57764/3sg0-7440)
subsets:
  - id: bridge2ai-voice-v1-0-adult
    name: Adult cohort v1.0
    title: Adult cohort subset
    description: Adult participants only; pediatric cohort not included in v1.0
    is_data_split: "no"
    is_subpopulation: "yes"