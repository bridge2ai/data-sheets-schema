
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="datasheet-common.css">
    <title>physionet b2ai-voice 1.1 d4d - Datasheet for Dataset</title>
</head>
<body>
    <div class="header">
        <h1>physionet b2ai-voice 1.1 d4d</h1>
        <p class="subtitle">Datasheet for Dataset - Human Readable Format</p>
    </div>
    
    
        
            
            <div class="section">
                <div class="section-header">
                    <span class="section-icon">üîç</span>
                    <div>
                        <h2 class="section-title">Collection Process</h2>
                        <p class="section-description">How was the data acquired?</p>
                    </div>
                </div>
                <div class="section-content">
                    
                    <div class="data-item">
                        
                        <label class="item-label required-field">
                            ID
                            <span class="required-indicator" title="Required field">*</span>
                        </label>
                        <div class="item-value">bridge2ai-voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Name
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Title
                            
                        </label>
                        <div class="item-value">Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Description
                            
                        </label>
                        <div class="item-value"><div class="long-description">Bridge2AI-Voice is a comprehensive, ethically-sourced collection of data derived from voice recordings linked to clinical information, intended to enable artificial intelligence research into voice as a biomarker of health. Version 1.1 provides 12,523 recordings for 306 participants collected across five sites in North America, focusing on cohorts with conditions known to manifest in the voice (voice disorders, neurological disorders, mood disorders, and respiratory disorders). This release contains low-risk derived data (e.g., spectrograms, MFCCs, static acoustic/phonetic/prosodic features) and associated phenotype data; raw audio is not included and is available only via controlled access to protect participant privacy.</div></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Language
                            
                        </label>
                        <div class="item-value">English</div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Page
                            
                        </label>
                        <div class="item-value"><a href="https://docs.b2ai-voice.org" target="_blank">https://docs.b2ai-voice.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Publisher
                            
                        </label>
                        <div class="item-value"><a href="https://physionet.org" target="_blank">https://physionet.org</a></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Keywords
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>voice</li><li>speech</li><li>biomarker</li><li>health</li><li>Bridge2AI</li><li>spectrograms</li><li>MFCC</li><li>acoustic features</li><li>phenotypes</li><li>clinical data</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Created By
                            
                        </label>
                        <div class="item-value"><ul class='formatted-list'><li>Alistair Johnson</li><li>Jean-Christophe B√©lisle-Pipon</li><li>David Dorr</li><li>Satrajit Ghosh</li><li>Philip Payne</li><li>Maria Powell</li><li>Anais Rameau</li><li>Vardit Ravitsky</li><li>Alexandros Sigaras</li><li>Olivier Elemento</li><li>Yael Bensoussan</li></ul></div>
                    </div>
                    
                    <div class="data-item">
                        
                        <label class="item-label optional-field">
                            Resources
                            
                        </label>
                        <div class="item-value"><ol class='formatted-list'><li><dl class='nested-dict'><dt>ID</dt><dd>bridge2ai-voice-v1.1</dd><dt>Name</dt><dd>Bridge2AI-Voice v1.1</dd><dt>Title</dt><dd>Bridge2AI-Voice: An ethically-sourced, diverse voice dataset linked to health information (v1.1)</dd><dt>Description</dt><dd><div class="long-description">Version 1.1 of Bridge2AI-Voice includes derived voice data (spectrograms, MFCCs, static acoustic/phonetic/prosodic features) and linked phenotype data for an adult cohort. It excludes raw audio waveforms. Files listed for this version include: spectrograms.parquet, mfcc.parquet, static_features.tsv (+ JSON data dictionary), phenotype.tsv (+ JSON data dictionary). Access is restricted to registered users under a data use agreement. Raw audio may be requested via controlled access.</div></dd><dt>Language</dt><dd>English</dd><dt>Page</dt><dd><a href="https://doi.org/10.13026/249v-w155" target="_blank">https://doi.org/10.13026/249v-w155</a></dd><dt>DOI</dt><dd><a href="https://doi.org/10.13026/249v-w155" target="_blank">doi:10.13026/249v-w155</a></dd><dt>Issued</dt><dd>2025-01-17</dd><dt>Version</dt><dd>1.1</dd><dt>License</dt><dd>Bridge2AI Voice Registered Access License</dd><dt>Keywords</dt><dd><ul class='formatted-list'><li>voice</li><li>speech</li><li>biomarker</li><li>health</li><li>Bridge2AI</li><li>spectrograms</li><li>MFCC</li><li>acoustic features</li><li>phenotypes</li><li>clinical data</li></ul></dd><dt>Is Tabular</dt><dd>mixed (Parquet, TSV, JSON)</dd><dt>Purposes</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Purpose</dd><dt>Response</dt><dd>Create an ethically-sourced flagship dataset to enable AI research on voice as a biomarker of health and support critical clinical insights.</dd></dl></li></ul></dd><dt>Tasks</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Task</dd><dt>Response</dt><dd>Voice-based biomarker discovery, disease classification/screening, and analysis of acoustic/phonetic/prosodic features linked to health.</dd></dl></li></ul></dd><dt>Addressing Gaps</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>AddressingGap</dd><dt>Response</dt><dd><div class="long-description">Provide a large, multi-institutional, diverse voice dataset linked to health information with standardized collection protocols and explicit ethical oversight, addressing limitations of prior small and demographically limited datasets.</div></dd></dl></li></ul></dd><dt>Funders</dt><dd><table class="data-table"><thead><tr><th>Grantor</th><th>Grant Name</th><th>Grant Number</th></tr></thead><tbody><tr><td>National Institutes of Health (NIH)</td><td>Bridge2AI: Voice as a Biomarker of Health - Building an ethically sourced, bioacoustic database to understand disease like never before</td><td>3OT2OD032720-01S1</td></tr><tr><td>National Institute of Biomedical Imaging and Bioengineering (NIBIB), NIH</td><td>PhysioNet infrastructure support</td><td>R01EB030362</td></tr></tbody></table></dd><dt>Instances</dt><dd><table class="data-table"><thead><tr><th>Counts</th><th>Data Type</th><th>Instance Type</th><th>Label</th><th>Name</th><th>Representation</th><th>Sampling Strategies</th></tr></thead><tbody><tr><td>12523</td><td>Derived features from audio (spectrograms: 513xN; MFCC: 60xN; static acoustic/phonetic/prosodic feat...</td><td>Multiple instance types: recordings (sessions/tasks) with derived data and per-participant phenotype...</td><td>No explicit supervised label provided; clinical/phenotype variables available</td><td>Voice-derived data instances</td><td>Derived voice data per recording session (e.g., spectrograms, MFCCs, static features)</td><td>{'name': 'Cohort sampling strategy', 'strategies': ['Targeted enrollment of patients in five predetermined clinical cohorts (respiratory, voice, neurological, mood, pediatric; adult cohort available in v1.1)'], 'is_sample': ['yes (targeted clinical cohorts)'], 'is_random': [False], 'source_data': ['Patients presenting at specialty clinics across five sites in North America'], 'is_representative': ['No (targeted cohorts; not representative of general population)'], 'why_not_representative': ['Enrollment focused on diseases with recognized vocal manifestations and unmet needs, to fuel disease-relevant voice AI research']}</td></tr><tr><td>306</td><td>Tabular phenotype data (phenotype.tsv with data dictionary phenotype.json)</td><td>Participants with demographics, questionnaires, and clinical phenotype data</td><td></td><td>Participants</td><td>Study participants (adult cohort in v1.1)</td><td></td></tr></tbody></table></dd><dt>External Resources</dt><dd><ol class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Raw audio access</dd><dt>External Resources</dt><dd><ul class='formatted-list'><li>Original raw audio available only via controlled access request (contact: DACO@b2ai-voice.org)</li></ul></dd><dt>Future Guarantees</dt><dd><ul class='formatted-list'><li>Controlled access to protect participant privacy</li></ul></dd><dt>Archival</dt><dd><ul class='formatted-list'><li>Derived datasets (spectrograms, MFCCs, features, phenotypes) are archived with DOI at PhysioNet</li></ul></dd></dl></li></ol></dd><dt>Confidential Elements</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Confidentiality</dd><dt>Description</dt><dd><ul class='formatted-list'><li>No raw audio recordings released in v1.1; dataset contains only derived features and de-identified phenotype data to reduce risk</li></ul></dd></dl></li></ul></dd><dt>Content Warnings</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Content warnings</dd><dt>Warnings</dt><dd><ul class='formatted-list'><li>None noted</li></ul></dd></dl></li></ul></dd><dt>Subpopulations</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Adult cohort</dd><dt>Identification</dt><dd><ul class='formatted-list'><li>Adult participants (v1.1 includes adult cohort only)</li></ul></dd><dt>Distribution</dt><dd><ul class='formatted-list'><li>306 participants across five North American sites; disease-focused cohorts (respiratory, voice, neurological, mood; pediatric planned)</li></ul></dd></dl></li></ul></dd><dt>Sensitive Elements</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Sensitive elements</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Clinical/health-related phenotype data; identifiers removed per HIPAA Safe Harbor and additional de-identification</li></ul></dd></dl></li></ul></dd><dt>Acquisition Methods</dt><dd><ol class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Instance acquisition</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Standardized, protocol-driven data collection using a custom tablet application; headset used when possible; demographic and clinical questionnaires; targeted tasks including sustained vowel phonation</li></ul></dd><dt>Was Directly Observed</dt><dd>yes (audio tasks recorded)</dd><dt>Was Reported By Subjects</dt><dd>yes (questionnaires and targeted confounders)</dd><dt>Was Inferred Derived</dt><dd>yes (features, spectrograms, MFCCs, prosodic/phonetic metrics, transcriptions)</dd><dt>Was Validated Verified</dt><dd>Standardized multi-site protocol; preprocessing with defined parameters</dd></dl></li></ol></dd><dt>Collection Mechanisms</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Collection mechanisms</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Custom application on a tablet; headset microphone when possible; REDCap-based data export and conversion using an open-source library</li></ul></dd></dl></li></ul></dd><dt>Data Collectors</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Data collectors</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Project investigators at specialty clinics across five North American sites screened and enrolled patients, obtained consent, and conducted standardized data collection sessions</li></ul></dd></dl></li></ul></dd><dt>Ethical Reviews</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>IRB approval</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Data collection and sharing approved by the University of South Florida Institutional Review Board</li></ul></dd></dl></li></ul></dd><dt>Preprocessing Strategies</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Audio preprocessing and feature extraction</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Raw audio converted to mono and resampled to 16 kHz with a Butterworth anti-aliasing filter</li><li>Spectrograms via STFT (25 ms window, 10 ms hop, 512-point FFT; 513 x N)</li><li>60 MFCCs derived from spectrograms (60 x N)</li><li>Acoustic features via openSMILE capturing temporal dynamics and acoustic characteristics</li><li>Phonetic and prosodic features via Parselmouth and Praat (e.g., F0, formants, voice quality)</li><li>Transcriptions generated using OpenAI Whisper Large (used for derivations; free-speech transcripts removed)</li></ul></dd><dt>Used Software</dt><dd><table class="data-table"><thead><tr><th>ID</th><th>Name</th><th>URL</th></tr></thead><tbody><tr><td>openSMILE</td><td>openSMILE</td><td>https://audeering.github.io/opensmile/</td></tr><tr><td>praat</td><td>Praat</td><td>https://www.fon.hum.uva.nl/praat/</td></tr><tr><td>parselmouth</td><td>Parselmouth</td><td>https://parselmouth.readthedocs.io/</td></tr><tr><td>torchaudio</td><td>Torchaudio</td><td>https://pytorch.org/audio</td></tr><tr><td>whisper-large</td><td>OpenAI Whisper Large</td><td>https://github.com/openai/whisper</td></tr></tbody></table></dd></dl></li></ul></dd><dt>Cleaning Strategies</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>De-identification and content removal</dd><dt>Description</dt><dd><ul class='formatted-list'><li>HIPAA Safe Harbor identifiers removed (e.g., names, finer-than-year dates, contact numbers, IPs, MRNs, device IDs, URLs, images, etc.); state/province removed; country retained</li><li>Transcripts of free speech audio removed</li><li>Audio waveforms omitted from release; only derived data (spectrograms and other features) provided in v1.1</li></ul></dd></dl></li></ul></dd><dt>Raw Sources</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Raw audio data</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Original audio waveforms exist but are not distributed in v1.1; may be requested via controlled access by contacting DACO@b2ai-voice.org</li></ul></dd></dl></li></ul></dd><dt>Future Use Impacts</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Considerations for future use</dd><dt>Description</dt><dd><ul class='formatted-list'><li><div class="long-description">Use of derived features only (no raw audio) reduces re-identification risk but may limit some modeling approaches; cohort-targeted sampling may impact generalizability; users should consider potential bias and fairness implications</div></li></ul></dd></dl></li></ul></dd><dt>Discouraged Uses</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Discouraged uses</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Uses that attempt to re-identify individuals or reconstruct content beyond the scope of the derived features; any uses outside the Data Use Agreement and Registered Access License</li></ul></dd></dl></li></ul></dd><dt>Distribution Formats</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Formats</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Parquet (spectrograms.parquet, mfcc.parquet)</li><li>TSV (phenotype.tsv, static_features.tsv)</li><li>JSON (phenotype.json, static_features.json)</li></ul></dd></dl></li></ul></dd><dt>Distribution Dates</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Initial distribution (v1.1)</dd><dt>Description</dt><dd><ul class='formatted-list'><li>2025-01-17</li></ul></dd></dl></li></ul></dd><dt>License And Use Terms</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>License and terms</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Bridge2AI Voice Registered Access License</li><li>Bridge2AI Voice Registered Access Agreement (Data Use Agreement)</li></ul></dd></dl></dd><dt>Ip Restrictions</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>IP restrictions</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Not specified; derived datasets distributed under Registered Access License and DUA on PhysioNet</li></ul></dd></dl></dd><dt>Regulatory Restrictions</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>Export controls and regulatory restrictions</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Not specified</li></ul></dd></dl></dd><dt>Maintainers</dt><dd><ul class='formatted-list'><li><dl class='nested-dict'><dt>Name</dt><dd>Maintainers</dd><dt>Description</dt><dd><ul class='formatted-list'><li>MIT Laboratory for Computational Physiology (PhysioNet)</li><li>For controlled access to raw audio: DACO@b2ai-voice.org (Bridge2AI Voice Data Access)</li></ul></dd></dl></li></ul></dd><dt>Updates</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>Versioning and updates</dd><dt>Description</dt><dd><ul class='formatted-list'><li>v1.0: initial release (derived data; 2024, cited in Health Data Nexus)</li><li>v1.1: added MFCCs (2025-01-17)</li><li>v2.0.0: released 2025-04-16 (latest superseded)</li><li>v2.0.1: released 2025-08-18 (latest as of listing)</li></ul></dd></dl></dd><dt>Version Access</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>Access to prior versions</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Files for v1.1 are no longer available on PhysioNet; the DOI remains for citation. Users are directed to the latest version (v2.0.1).</li></ul></dd></dl></dd><dt>Extension Mechanism</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>Extension and reproducibility</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Preprocessing/merging code released as the open-source b2aiprep library to facilitate reproducibility and extension of data processing pipelines</li></ul></dd></dl></dd><dt>Is Deidentified</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>De-identification status</dd><dt>Description</dt><dd><ul class='formatted-list'><li>Yes. HIPAA Safe Harbor identifiers removed; state/province removed; free-speech transcripts removed; only derived features released in v1.1</li></ul></dd></dl></dd><dt>Third Party Sharing</dt><dd><dl class='nested-dict'><dt>Name</dt><dd>Distribution to third parties</dd><dt>Description</dt><dd>Yes, to registered users under the Registered Access License and Data Use Agreement via PhysioNet</dd></dl></dd><dt>Was Derived From</dt><dd>Original raw voice recordings collected under a standardized protocol; v1.1 consists of derived representations and features (no raw audio)</dd></dl></li></ol></div>
                    </div>
                    
                </div>
            </div>
            
        
    
    
    <div class="timestamp">
        Generated on 2025-11-08 22:21:59 using Bridge2AI Data Sheets Schema
    </div>
</body>
</html>